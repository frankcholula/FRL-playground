{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc11682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# flow_matching\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import Solver, ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# To avoide meshgrid warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b92ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f145c448c90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7640966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "\n",
    "# TODO: need to resolve temporal locality problem maybe with a CNN later.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, time_dim: int = 1, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        sz = x.size()\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        t = t.reshape(-1, self.time_dim).float()\n",
    "\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        h = torch.cat([x, t], dim=1)\n",
    "        output = self.main(h)\n",
    "\n",
    "        return output.reshape(*sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3416b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9751e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minari dataset\n",
    "import minari\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "env = minari_dataset.recover_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325af361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| Epoch     10 | 1.12 s/epoch | Loss  1.06362 \n",
      "| Epoch     20 | 1.06 s/epoch | Loss  1.05980 \n",
      "| Epoch     30 | 0.76 s/epoch | Loss  1.05979 \n",
      "| Epoch     40 | 0.89 s/epoch | Loss  1.06008 \n",
      "| Epoch     50 | 1.09 s/epoch | Loss  1.05964 \n",
      "| Epoch     60 | 1.08 s/epoch | Loss  1.05887 \n",
      "| Epoch     70 | 0.77 s/epoch | Loss  1.05756 \n",
      "| Epoch     80 | 1.08 s/epoch | Loss  1.05734 \n",
      "| Epoch     90 | 1.06 s/epoch | Loss  1.05930 \n",
      "| Epoch    100 | 1.06 s/epoch | Loss  1.05805 \n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "horizon = 1000 # need to adjust this\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "input_dim = (obs_dim + action_dim) * horizon\n",
    "\n",
    "# Training params\n",
    "lr = 0.001\n",
    "num_epochs = 100\n",
    "print_every = 10\n",
    "hidden_dim = 1024\n",
    "\n",
    "vf = MLP(input_dim=input_dim, time_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "optim = torch.optim.Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # 1. Zero gradients for this batch\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 2. Prepare data\n",
    "        observations = batch[\"observations\"][:, :-1]\n",
    "        expert_actions = batch[\"actions\"]\n",
    "        x_1 = torch.cat([observations, expert_actions], dim=-1)\n",
    "        x_1 = x_1.reshape(x_1.shape[0], -1).to(device)\n",
    "        x_0 = torch.randn_like(x_1).to(device)\n",
    "        t = torch.rand(x_1.shape[0]).to(device)\n",
    "\n",
    "        # 3. Forward pass and Loss\n",
    "        path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
    "        predicted_velocity = vf(path_sample.x_t, path_sample.t)\n",
    "        loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
    "\n",
    "        # 4. Backward pass and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"| Epoch {epoch+1:6d} | {elapsed:.2f} s/epoch | Loss {avg_epoch_loss:8.5f} \")\n",
    "        start_time = time.time()\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668e0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sampling from trained model...\n",
    "\n",
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b22867d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step size for ode solver\n",
    "step_size = 0.05\n",
    "\n",
    "batch_size = 1  # batch size\n",
    "T = torch.linspace(0,1,10)  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "x_init = torch.randn((batch_size, input_dim), dtype=torch.float32, device=device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)  # create an ODESolver class\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)  # sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66c6c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trajectory = sol.reshape(horizon, -1)\n",
    "final_trajectory.shape\n",
    "observations = final_trajectory[:, :obs_dim]\n",
    "actions = final_trajectory[:, obs_dim:obs_dim + action_dim]\n",
    "# print(observations.shape, actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4042d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [-0.59474504 -2.3656845 ], Reward: 2.0804712989958616\n",
      "Action: [-1.6046611  1.4039768], Reward: 0.529247597147845\n",
      "Action: [-0.30896065  0.40414703], Reward: 1.2011792847636968\n",
      "Action: [-1.4175168  0.1911168], Reward: 1.1590071000121895\n",
      "Action: [-0.3444995  1.2422355], Reward: 0.23162445040557714\n",
      "Action: [ 0.8719092  -0.16229032], Reward: -1.1592154582360479\n",
      "Action: [0.90314084 0.65312976], Reward: -2.524799849197235\n",
      "Action: [ 0.5631698 -0.9630722], Reward: -2.927212301543244\n",
      "Action: [ 1.014588   -0.01312742], Reward: -1.8393398567428279\n",
      "Action: [0.93166876 1.2738359 ], Reward: -2.716529957730306\n",
      "Action: [2.109867   0.09654108], Reward: -1.7539641506018484\n",
      "Action: [-2.3889139  0.9981402], Reward: -0.24904855373091095\n",
      "Action: [-0.4826049   0.11079448], Reward: 0.4263253605946318\n",
      "Action: [-0.30478856  1.472222  ], Reward: -0.5918381787485874\n",
      "Action: [-0.44305983 -1.0094665 ], Reward: 0.861314760187754\n",
      "Action: [0.5981448 0.865999 ], Reward: -2.45467567226086\n",
      "Action: [-0.5309649 -1.7332036], Reward: 1.147586088996975\n",
      "Action: [1.9908663  0.57395196], Reward: -1.1996481522996465\n",
      "Action: [ 0.1698991 -0.0761921], Reward: -2.631858120942013\n",
      "Action: [-0.04871259 -1.3056962 ], Reward: 0.8756854630828184\n",
      "Action: [-0.9577911   0.60665506], Reward: -0.6303592292787948\n",
      "Action: [1.3412906  0.45997176], Reward: -0.9538308485264964\n",
      "Action: [0.2770562  0.90470004], Reward: -3.3218535031999146\n",
      "Action: [-0.364021   1.2659976], Reward: -1.7480001830462573\n",
      "Action: [-0.8127733   0.28449142], Reward: -0.8566971533006722\n",
      "Action: [0.4142499 1.0537795], Reward: -3.421814196072141\n",
      "Action: [ 0.20110403 -0.7364965 ], Reward: -1.1135851443365696\n",
      "Action: [-0.537443    0.67326444], Reward: -1.9542325548257986\n",
      "Action: [ 0.7292987  -0.49747896], Reward: -3.3144339841000887\n",
      "Action: [ 1.0933212  -0.12074453], Reward: -2.2501702132573937\n",
      "Action: [-0.95991033  1.2531176 ], Reward: -2.6887968961156035\n",
      "Action: [0.23541921 1.3380672 ], Reward: -5.255299966269006\n",
      "Action: [ 0.52893263 -0.72087544], Reward: -3.2316990667275403\n",
      "Action: [0.7429961  0.40257224], Reward: -4.218360025898181\n",
      "Action: [ 0.45178995 -1.7671987 ], Reward: -0.8345331999266467\n",
      "Action: [0.15743957 1.3233099 ], Reward: -4.9065469521632386\n",
      "Action: [-1.0675703 -0.4474927], Reward: -1.7877990807709239\n",
      "Action: [-0.9007794   0.34364715], Reward: -1.9186268367033108\n",
      "Action: [-0.741978    0.67837936], Reward: -2.7906580805181567\n",
      "Action: [0.6561444 0.50246  ], Reward: -5.205696209035421\n",
      "Action: [0.49113232 0.2797345 ], Reward: -3.452209175546477\n",
      "Action: [-1.4198672  -0.11700829], Reward: -2.434553711939344\n",
      "Action: [-0.6973227  -0.53666145], Reward: -2.0290754260645825\n",
      "Action: [-0.10656843 -0.3823138 ], Reward: -2.541494988650925\n",
      "Action: [ 0.6633025  -0.43280217], Reward: -6.186984149805909\n",
      "Action: [-0.32472637  0.774929  ], Reward: -3.289829148555798\n",
      "Action: [-0.03287338 -0.28870288], Reward: -2.7366172140131653\n",
      "Action: [-0.7952646  -0.21702448], Reward: -2.8117722705039796\n",
      "Action: [ 2.2865365 -0.0984932], Reward: -5.488861870473829\n",
      "Action: [-2.2887862  -0.97917724], Reward: -1.9348181513963731\n",
      "Action: [-0.27004483  1.2529523 ], Reward: -3.945766514333029\n",
      "Action: [-0.9643221 -1.627599 ], Reward: -1.987993804049977\n",
      "Action: [-1.6181418  1.6683184], Reward: -4.018630154844714\n",
      "Action: [0.08987443 1.4492507 ], Reward: -4.9283734861354835\n",
      "Action: [-0.16202384  0.03229222], Reward: -3.301468112017801\n",
      "Action: [ 0.0857707  -0.06362072], Reward: -4.082095403441644\n",
      "Action: [ 0.25725204 -0.50935477], Reward: -4.1666843026443985\n",
      "Action: [0.409474   0.61455494], Reward: -6.309493439336886\n",
      "Action: [1.5541613  0.74555624], Reward: -7.230077403952807\n",
      "Action: [ 1.4251568 -1.4247463], Reward: -5.030049690025139\n",
      "Action: [-0.42737988  1.5223497 ], Reward: -4.167118599478015\n",
      "Action: [-1.7230546  -0.59447795], Reward: -2.661796695952023\n",
      "Action: [-1.0192364  2.2382717], Reward: -4.204186155194548\n",
      "Action: [ 1.1590211 -0.4003851], Reward: -9.487359846254027\n",
      "Action: [1.1815822 0.5174269], Reward: -7.667992399933555\n",
      "Action: [ 1.7437586 -0.3601011], Reward: -7.504225103859471\n",
      "Action: [0.25575265 0.06309423], Reward: -5.694390999404584\n",
      "Action: [-0.28823328 -0.52530265], Reward: -3.068187874895982\n",
      "Action: [0.17955138 0.47594312], Reward: -5.387881021024202\n",
      "Action: [-0.28832236 -1.0621403 ], Reward: -2.4941505128642505\n",
      "Action: [-1.4785383  1.7391893], Reward: -3.9911259711620617\n",
      "Action: [-1.1665053  0.974634 ], Reward: -4.3129589244237305\n",
      "Action: [ 0.26170453 -1.400499  ], Reward: -5.528856298306396\n",
      "Action: [0.01356583 1.0848023 ], Reward: -5.595285118135023\n",
      "Action: [-0.4071122   0.48214686], Reward: -3.557535787483914\n",
      "Action: [ 1.8378286  -0.21838443], Reward: -11.416404533859055\n",
      "Action: [1.8423456  0.71376926], Reward: -9.529053627879529\n",
      "Action: [-0.21184191  0.28385913], Reward: -3.5375780909680543\n",
      "Action: [-0.06573839 -1.3883272 ], Reward: -2.999683323042602\n",
      "Action: [1.2014477  0.38204738], Reward: -6.857753884486374\n",
      "Action: [ 1.4472454 -1.6884159], Reward: -9.240416162595993\n",
      "Action: [-0.75341207  1.1279963 ], Reward: -3.8656090710283864\n",
      "Action: [ 0.9254718 -1.5564356], Reward: -11.592670047656888\n",
      "Action: [-0.28731972  0.45605955], Reward: -3.0730155326315867\n",
      "Action: [1.5484601  0.07499121], Reward: -11.216969813642937\n",
      "Action: [-0.87583727 -1.6843407 ], Reward: -2.4313368670381394\n",
      "Action: [-1.3687193 -0.3664716], Reward: -2.745017831883274\n",
      "Action: [-0.70799476  0.5573358 ], Reward: -3.043008937176355\n",
      "Action: [ 0.5602868  -0.15678866], Reward: -6.254036266989351\n",
      "Action: [0.5348063 1.7509265], Reward: -6.410555579402187\n",
      "Action: [-0.536185   1.1004034], Reward: -3.6644848981475477\n",
      "Action: [-1.5149372  -0.53057474], Reward: -3.213077020456169\n",
      "Action: [ 1.1680704  -0.60316324], Reward: -11.531749503307356\n",
      "Action: [ 0.8145318  -0.24630693], Reward: -9.033738730640255\n",
      "Action: [-0.66663057  0.00181796], Reward: -3.2496073585318186\n",
      "Action: [ 2.5358162 -1.1848665], Reward: -7.253249727439384\n",
      "Action: [ 0.812238   -0.58304185], Reward: -6.724387461425494\n",
      "Action: [-0.4340971 -1.1425997], Reward: -2.857994469727514\n",
      "Action: [-0.18828343  1.0208238 ], Reward: -3.136753925731368\n",
      "Action: [ 0.34641808 -2.6447916 ], Reward: -8.348153849642204\n",
      "Action: [-0.6935842 -2.4279213], Reward: -2.849604946143272\n",
      "Action: [-1.6973644  1.3708405], Reward: -3.188921585063581\n",
      "Action: [-0.37237027  0.4146776 ], Reward: -3.266803197040872\n",
      "Action: [-1.3986448   0.22164688], Reward: -3.3908101367412655\n",
      "Action: [-0.3110792  1.2646451], Reward: -3.688125177564784\n",
      "Action: [ 0.9113347  -0.15118803], Reward: -7.232898607414995\n",
      "Action: [0.95288026 0.65244156], Reward: -11.380618750954763\n",
      "Action: [ 0.6000908 -0.9725351], Reward: -7.712191016229626\n",
      "Action: [ 1.0574521  -0.01133474], Reward: -13.390598996783798\n",
      "Action: [0.95811015 1.2663502 ], Reward: -13.102231792084199\n",
      "Action: [2.1494308  0.09837355], Reward: -10.78149929335341\n",
      "Action: [-2.349491   1.0079681], Reward: -5.189408422665138\n",
      "Action: [-0.43859166  0.11259348], Reward: -5.633228707745047\n",
      "Action: [-0.24839258  1.4804002 ], Reward: -5.820066349431754\n",
      "Action: [-0.41832435 -1.0078329 ], Reward: -6.410658440984462\n",
      "Action: [0.630368   0.87078846], Reward: -100\n"
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "total_rew = 0\n",
    "for i in range(horizon):\n",
    "    action = actions[i].cpu().numpy()\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    total_rew += rew\n",
    "    print(f\"Action: {action}, Reward: {rew}\")\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
