{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc11682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# flow_matching\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import Solver, ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# To avoide meshgrid warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b92ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10eae7590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7640966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "\n",
    "# TODO: need to resolve temporal locality problem maybe with a CNN later.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, time_dim: int = 1, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        sz = x.size()\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        t = t.reshape(-1, self.time_dim).float()\n",
    "\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        h = torch.cat([x, t], dim=1)\n",
    "        output = self.main(h)\n",
    "\n",
    "        return output.reshape(*sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3416b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    observations = [torch.as_tensor(x.observations) for x in batch]\n",
    "    actions = [torch.as_tensor(x.actions) for x in batch]\n",
    "    rewards = [torch.as_tensor(x.rewards) for x in batch]\n",
    "    terminations = [torch.as_tensor(x.terminations) for x in batch]\n",
    "    truncations = [torch.as_tensor(x.truncations) for x in batch]\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            observations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            actions,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            rewards,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            terminations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            truncations,\n",
    "            batch_first=True\n",
    "        )\n",
    "    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9751e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minari dataset\n",
    "import minari\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-deterministic-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "env = minari_dataset.recover_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "325af361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| Epoch     10 | 0.43 s/epoch | Loss  1.03202 \n",
      "| Epoch     20 | 0.43 s/epoch | Loss  1.02859 \n",
      "| Epoch     30 | 0.39 s/epoch | Loss  1.02167 \n",
      "| Epoch     40 | 0.39 s/epoch | Loss  1.01973 \n",
      "| Epoch     50 | 0.39 s/epoch | Loss  1.01940 \n",
      "| Epoch     60 | 0.39 s/epoch | Loss  1.01963 \n",
      "| Epoch     70 | 0.42 s/epoch | Loss  1.01684 \n",
      "| Epoch     80 | 0.41 s/epoch | Loss  1.01512 \n",
      "| Epoch     90 | 0.40 s/epoch | Loss  1.01169 \n",
      "| Epoch    100 | 0.40 s/epoch | Loss  1.01212 \n",
      "| Epoch    110 | 0.41 s/epoch | Loss  1.00878 \n",
      "| Epoch    120 | 0.42 s/epoch | Loss  1.00858 \n",
      "| Epoch    130 | 0.39 s/epoch | Loss  1.00506 \n",
      "| Epoch    140 | 0.40 s/epoch | Loss  1.00555 \n",
      "| Epoch    150 | 0.39 s/epoch | Loss  1.00294 \n",
      "| Epoch    160 | 0.39 s/epoch | Loss  1.00307 \n",
      "| Epoch    170 | 0.40 s/epoch | Loss  1.00188 \n",
      "| Epoch    180 | 0.39 s/epoch | Loss  1.00036 \n",
      "| Epoch    190 | 0.40 s/epoch | Loss  1.00065 \n",
      "| Epoch    200 | 0.39 s/epoch | Loss  0.99817 \n",
      "| Epoch    210 | 0.40 s/epoch | Loss  0.99790 \n",
      "| Epoch    220 | 0.39 s/epoch | Loss  0.99741 \n",
      "| Epoch    230 | 0.39 s/epoch | Loss  0.99661 \n",
      "| Epoch    240 | 0.40 s/epoch | Loss  0.99600 \n",
      "| Epoch    250 | 0.39 s/epoch | Loss  0.99644 \n",
      "| Epoch    260 | 0.39 s/epoch | Loss  0.99372 \n",
      "| Epoch    270 | 0.40 s/epoch | Loss  0.99311 \n",
      "| Epoch    280 | 0.39 s/epoch | Loss  0.99191 \n",
      "| Epoch    290 | 0.41 s/epoch | Loss  0.99412 \n",
      "| Epoch    300 | 0.40 s/epoch | Loss  0.99216 \n",
      "| Epoch    310 | 0.41 s/epoch | Loss  0.99280 \n",
      "| Epoch    320 | 0.41 s/epoch | Loss  0.99178 \n",
      "| Epoch    330 | 0.41 s/epoch | Loss  0.99049 \n",
      "| Epoch    340 | 0.41 s/epoch | Loss  0.99121 \n",
      "| Epoch    350 | 0.41 s/epoch | Loss  0.99012 \n",
      "| Epoch    360 | 0.40 s/epoch | Loss  0.98973 \n",
      "| Epoch    370 | 0.42 s/epoch | Loss  0.99038 \n",
      "| Epoch    380 | 0.41 s/epoch | Loss  0.98990 \n",
      "| Epoch    390 | 0.41 s/epoch | Loss  0.98922 \n",
      "| Epoch    400 | 0.41 s/epoch | Loss  0.98825 \n",
      "| Epoch    410 | 0.42 s/epoch | Loss  0.98717 \n",
      "| Epoch    420 | 0.42 s/epoch | Loss  0.98836 \n",
      "| Epoch    430 | 0.41 s/epoch | Loss  0.98711 \n",
      "| Epoch    440 | 0.41 s/epoch | Loss  0.98843 \n",
      "| Epoch    450 | 0.41 s/epoch | Loss  0.98666 \n",
      "| Epoch    460 | 0.40 s/epoch | Loss  0.98771 \n",
      "| Epoch    470 | 0.41 s/epoch | Loss  0.98543 \n",
      "| Epoch    480 | 0.40 s/epoch | Loss  0.98423 \n",
      "| Epoch    490 | 0.40 s/epoch | Loss  0.98644 \n",
      "| Epoch    500 | 0.42 s/epoch | Loss  0.98415 \n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "horizon = 250\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "input_dim = (obs_dim + action_dim) * horizon\n",
    "\n",
    "# Training params\n",
    "lr = 0.001\n",
    "num_epochs = 500\n",
    "print_every = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "vf = MLP(input_dim=input_dim, time_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "optim = torch.optim.Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # 1. Zero gradients for this batch\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # 2. Prepare data\n",
    "        observations = batch[\"observations\"][:, :-1][:, :horizon]\n",
    "        expert_actions = batch[\"actions\"][:, :horizon]\n",
    "        expert_actions = batch[\"actions\"][:, :horizon]\n",
    "        x_1 = torch.cat([observations, expert_actions], dim=-1)\n",
    "        x_1 = x_1.reshape(x_1.shape[0], -1).to(device)\n",
    "        x_0 = torch.randn_like(x_1).to(device)\n",
    "        t = torch.rand(x_1.shape[0]).to(device)\n",
    "\n",
    "        # 3. Forward pass and Loss\n",
    "        path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
    "        predicted_velocity = vf(path_sample.x_t, path_sample.t)\n",
    "        loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
    "\n",
    "        # 4. Backward pass and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"| Epoch {epoch+1:6d} | {elapsed:.2f} s/epoch | Loss {avg_epoch_loss:8.5f} \")\n",
    "        start_time = time.time()\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "668e0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sampling from trained model...\n",
    "\n",
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b22867d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step size for ode solver\n",
    "step_size = 0.05\n",
    "\n",
    "batch_size = 1  # batch size\n",
    "T = torch.linspace(0,1,10)  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "x_init = torch.randn((batch_size, input_dim), dtype=torch.float32, device=device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)  # create an ODESolver class\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)  # sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66c6c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 8]) torch.Size([250, 2])\n"
     ]
    }
   ],
   "source": [
    "final_trajectory = sol.reshape(horizon, -1)\n",
    "final_trajectory.shape\n",
    "observations = final_trajectory[:, :obs_dim]\n",
    "actions = final_trajectory[:, obs_dim:obs_dim + action_dim]\n",
    "print(observations.shape, actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4042d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [0.2158604  0.60467863], Reward: -1.4394063672600463\n",
      "Action: [ 0.551068  -1.0015647], Reward: -2.0564348103179486\n",
      "Action: [-1.0130321  0.3193489], Reward: -0.02581130638174045\n",
      "Action: [1.1032864 1.2012154], Reward: -4.341134774900683\n",
      "Action: [-1.760066  -1.6100212], Reward: 0.6875222002680641\n",
      "Action: [-1.0467013  -0.92995286], Reward: 0.814113899677318\n",
      "Action: [ 0.5016967 -1.0278525], Reward: -1.6822322741013511\n",
      "Action: [0.95547706 0.44048885], Reward: -1.2967076224971605\n",
      "Action: [ 0.17151117 -0.7747519 ], Reward: -0.15411553887180957\n",
      "Action: [-0.25561076 -0.12994733], Reward: 0.4426983293079729\n",
      "Action: [0.30285662 0.21694663], Reward: -0.9423567858843171\n",
      "Action: [-0.33751944 -1.7288631 ], Reward: 1.247951803727717\n",
      "Action: [0.29053512 0.15206575], Reward: -1.1299692777025485\n",
      "Action: [0.40577304 0.86196953], Reward: -2.2788200959333222\n",
      "Action: [ 0.39176193 -0.48698473], Reward: -1.1072202172536663\n",
      "Action: [0.57743746 0.15333663], Reward: -0.820941460335041\n",
      "Action: [-0.18786347  0.87494206], Reward: -0.5167608445888027\n",
      "Action: [-0.85850984 -1.6312749 ], Reward: 1.2600813670215996\n",
      "Action: [0.47011316 1.5660057 ], Reward: -1.9993070818271905\n",
      "Action: [ 1.1293612  -0.29095724], Reward: -1.7190547713654099\n",
      "Action: [-0.6329743  1.4854088], Reward: -1.112446911779556\n",
      "Action: [1.7067323 1.166383 ], Reward: -3.244941716618898\n",
      "Action: [-0.45017156  1.0675466 ], Reward: -1.2529541980459544\n",
      "Action: [ 0.3474515  -0.21141697], Reward: -3.114595774151155\n",
      "Action: [ 0.9422482  -0.27091354], Reward: -2.1342870175170505\n",
      "Action: [0.11208741 0.57793796], Reward: -3.805828493361969\n",
      "Action: [ 0.50541425 -0.969371  ], Reward: -1.2976169736548206\n",
      "Action: [-1.0183178   0.31552294], Reward: -0.4271705589086139\n",
      "Action: [1.1381869 1.2062671], Reward: -4.600516245559151\n",
      "Action: [-1.713329  -1.6203316], Reward: 0.5154852126535456\n",
      "Action: [-1.0091276 -0.9001275], Reward: 0.5990720166031883\n",
      "Action: [ 0.5210944 -1.0276169], Reward: -3.1590197510598204\n",
      "Action: [1.0103276 0.4105993], Reward: -4.7803246406750075\n",
      "Action: [ 0.19648193 -0.78718144], Reward: 0.5696466430686404\n",
      "Action: [-0.19613203 -0.11533333], Reward: 0.14272476566858927\n",
      "Action: [0.3426382  0.21384381], Reward: -1.229041260861146\n",
      "Action: [-0.32143262 -1.7137997 ], Reward: 1.0204923428800339\n",
      "Action: [0.34016678 0.15641195], Reward: -1.2987166572632418\n",
      "Action: [0.42021123 0.89926636], Reward: -3.1095110066414566\n",
      "Action: [ 0.38477597 -0.5070815 ], Reward: -0.30311565692791076\n",
      "Action: [0.5924982  0.14365233], Reward: -2.328269781587687\n",
      "Action: [-0.13585153  0.9075658 ], Reward: -0.9251738477667323\n",
      "Action: [-0.84007835 -1.6367303 ], Reward: 0.7594108954289436\n",
      "Action: [0.4994146 1.562116 ], Reward: -3.348677912654209\n",
      "Action: [ 1.144819 -0.328293], Reward: -3.4444801193301826\n",
      "Action: [-0.63767415  1.4798828 ], Reward: -1.3543688732161445\n",
      "Action: [1.7471507 1.1512059], Reward: -7.182365406090826\n",
      "Action: [-0.45770052  1.0684186 ], Reward: -1.8491924015811196\n",
      "Action: [ 0.3133936  -0.19126178], Reward: -2.3790508508104837\n",
      "Action: [ 0.8549979  -0.23527063], Reward: -3.2573585063577752\n",
      "Action: [0.02033266 0.55395216], Reward: -2.880073028915342\n",
      "Action: [ 0.4861831 -0.9406202], Reward: -3.995096646239724\n",
      "Action: [-0.9978817   0.30221215], Reward: -0.8472159229975773\n",
      "Action: [1.1821609 1.1986923], Reward: -5.211333830066694\n",
      "Action: [-1.6701912 -1.6385298], Reward: 0.0888745214624305\n",
      "Action: [-0.97298616 -0.87323165], Reward: 0.10606594672430675\n",
      "Action: [ 0.54326993 -1.0321143 ], Reward: -0.6695712341948297\n",
      "Action: [1.0641501 0.3764976], Reward: -4.7614706762786225\n",
      "Action: [ 0.22381458 -0.8033976 ], Reward: -2.447490568184867\n",
      "Action: [-0.1330779  -0.10008924], Reward: -0.3546177183628174\n",
      "Action: [0.37913638 0.2141596 ], Reward: -2.396989255220069\n",
      "Action: [-0.3033209 -1.6930512], Reward: 0.5457393681541884\n",
      "Action: [0.39331454 0.17656583], Reward: -2.657061406953153\n",
      "Action: [0.43057027 0.9576348 ], Reward: -2.9712889313287\n",
      "Action: [ 0.3788921  -0.50661457], Reward: -4.015103184521317\n",
      "Action: [0.60527325 0.15457019], Reward: -2.65903120930775\n",
      "Action: [-0.08355284  0.9507766 ], Reward: -1.3464897225213417\n",
      "Action: [-0.82412165 -1.6254834 ], Reward: 0.4400693640825193\n",
      "Action: [0.5239459 1.5667714], Reward: -4.641676371192857\n",
      "Action: [ 1.1646577  -0.35126793], Reward: -5.89548312559923\n",
      "Action: [-0.6415182  1.4835666], Reward: -1.764819918001847\n",
      "Action: [1.7780101 1.1436163], Reward: -2.843070423002261\n",
      "Action: [-0.48282623  1.0789663 ], Reward: -2.2001894666296082\n",
      "Action: [ 0.24399763 -0.16329965], Reward: -3.3488221251591765\n",
      "Action: [ 0.7430579 -0.1940053], Reward: -4.65466975492294\n",
      "Action: [-0.06111369  0.5385156 ], Reward: -2.0491321798617173\n",
      "Action: [ 0.48721364 -0.9143967 ], Reward: -4.21077453249083\n",
      "Action: [-0.957465    0.28133243], Reward: -1.335666369458238\n",
      "Action: [1.2311158 1.180541 ], Reward: -6.76431301519189\n",
      "Action: [-1.6314683 -1.666995 ], Reward: -0.6860478996553343\n",
      "Action: [-0.937661  -0.8523855], Reward: -0.5312705831836797\n",
      "Action: [ 0.5675708 -1.045177 ], Reward: -2.0399271931683574\n",
      "Action: [1.1169463  0.33463696], Reward: -5.724896435037169\n",
      "Action: [ 0.253445  -0.8270661], Reward: -3.894298567409337\n",
      "Action: [-0.06559198 -0.08825017], Reward: -0.8906701597666711\n",
      "Action: [0.41320354 0.21302514], Reward: -3.619496049299664\n",
      "Action: [-0.28368512 -1.6717404 ], Reward: 0.05536331493974103\n",
      "Action: [0.4523034  0.20709173], Reward: -2.7629620589028834\n",
      "Action: [0.43840516 1.0317484 ], Reward: -2.5259119090970943\n",
      "Action: [ 0.3742711  -0.48916784], Reward: -2.5301044807663517\n",
      "Action: [0.6152245  0.18130986], Reward: -4.772887091588586\n",
      "Action: [-0.0307015  1.003432 ], Reward: -2.2676568893243982\n",
      "Action: [-0.81031483 -1.598742  ], Reward: -0.2627279835585046\n",
      "Action: [0.5453547 1.5798234], Reward: -7.204084763489987\n",
      "Action: [ 1.1868018 -0.3607792], Reward: -5.38880532509778\n",
      "Action: [-0.6454292  1.4944093], Reward: -2.4202346538522463\n",
      "Action: [1.8009272 1.1416458], Reward: -5.556152435259564\n",
      "Action: [-0.5205982  1.098204 ], Reward: -3.0550893113227446\n",
      "Action: [ 0.1482837  -0.13038832], Reward: -4.943695725809334\n",
      "Action: [ 0.6138774 -0.1485891], Reward: -5.742826335926566\n",
      "Action: [-0.13771944  0.53379124], Reward: -2.6643265698193477\n",
      "Action: [ 0.5003606  -0.88848984], Reward: -5.9415459435786415\n",
      "Action: [-0.90558505  0.25438324], Reward: -2.0735536276580433\n",
      "Action: [1.2805576 1.1545479], Reward: -8.919306001390582\n",
      "Action: [-1.5974987 -1.7071433], Reward: -1.5415040758997793\n",
      "Action: [-0.9033103 -0.8395491], Reward: -1.4790324310500955\n",
      "Action: [ 0.59405535 -1.0675778 ], Reward: -2.776459508043608\n",
      "Action: [1.1691053  0.28217226], Reward: -9.252927556528949\n",
      "Action: [ 0.2853478 -0.8598957], Reward: -3.0390704328679976\n",
      "Action: [ 0.00705318 -0.08305368], Reward: -100\n"
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "total_rew = 0\n",
    "for i in range(horizon):\n",
    "    action = actions[i].cpu().numpy()\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    total_rew += rew\n",
    "    print(f\"Action: {action}, Reward: {rew}\")\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "import pygame\n",
    "pygame.display.quit()\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea2174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
