{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc11682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# flow_matching\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import Solver, ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# To avoide meshgrid warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b92ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10eae7590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7640966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "\n",
    "# TODO: need to resolve temporal locality problem maybe with a CNN later.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, time_dim: int = 1, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        sz = x.size()\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        t = t.reshape(-1, self.time_dim).float()\n",
    "\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        h = torch.cat([x, t], dim=1)\n",
    "        output = self.main(h)\n",
    "\n",
    "        return output.reshape(*sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3416b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    observations = [torch.as_tensor(x.observations) for x in batch]\n",
    "    actions = [torch.as_tensor(x.actions) for x in batch]\n",
    "    rewards = [torch.as_tensor(x.rewards) for x in batch]\n",
    "    terminations = [torch.as_tensor(x.terminations) for x in batch]\n",
    "    truncations = [torch.as_tensor(x.truncations) for x in batch]\n",
    "    episode_lengths = torch.tensor([len(x.actions) for x in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            observations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            actions,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            rewards,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            terminations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            truncations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"episode_lengths\": episode_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f6286b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectory_chunks(batch, horizon):\n",
    "    \"\"\"\n",
    "    Processes a padded batch to create fixed-size trajectory chunks.\n",
    "    \"\"\"\n",
    "    batch_size = batch['observations'].shape[0]\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get the data for one episode and its true length\n",
    "        obs = batch['observations'][i]      # Shape: (max_len, 8)\n",
    "        act = batch['actions'][i]          # Shape: (max_len-1, 2)\n",
    "        length = batch['episode_lengths'][i]       # Scalar, e.g., 495\n",
    "\n",
    "        # A single episode can produce multiple chunks\n",
    "        # We slide a window of size 'horizon' over the valid part of the episode\n",
    "        for start_idx in range(length - horizon + 1):\n",
    "            end_idx = start_idx + horizon\n",
    "\n",
    "            # Slice the observation and action sequences to get a chunk\n",
    "            obs_chunk = obs[start_idx:end_idx] # Shape: (horizon, 8)\n",
    "            act_chunk = act[start_idx:end_idx] # Shape: (horizon, 2)\n",
    "            \n",
    "            # Combine them into a single (horizon, 10) tensor\n",
    "            chunk = torch.cat([obs_chunk, act_chunk], dim=-1)\n",
    "\n",
    "            # Flatten the chunk to the final 1000-D vector and add to our list\n",
    "            all_chunks.append(chunk.flatten())\n",
    "\n",
    "    if not all_chunks:\n",
    "        return None\n",
    "\n",
    "    return torch.stack(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9751e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48515, 1000])\n",
      "torch.Size([47538, 1000])\n",
      "torch.Size([47346, 1000])\n",
      "torch.Size([42836, 1000])\n"
     ]
    }
   ],
   "source": [
    "# load minari dataset\n",
    "import minari\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-deterministic-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "for batch in dataloader:\n",
    "    processed_chunks = create_trajectory_chunks(batch, 100)\n",
    "    print(processed_chunks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325af361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| Epoch     10 | 4.65 s/epoch | Loss  1.03000 \n",
      "| Epoch     20 | 4.65 s/epoch | Loss  1.00726 \n",
      "| Epoch     30 | 4.52 s/epoch | Loss  0.98533 \n",
      "| Epoch     40 | 4.82 s/epoch | Loss  0.97128 \n",
      "| Epoch     50 | 4.70 s/epoch | Loss  0.96085 \n",
      "| Epoch     60 | 4.64 s/epoch | Loss  0.95351 \n",
      "| Epoch     70 | 4.59 s/epoch | Loss  0.94628 \n",
      "| Epoch     80 | 4.62 s/epoch | Loss  0.94132 \n",
      "| Epoch     90 | 4.88 s/epoch | Loss  0.93563 \n",
      "| Epoch    100 | 5.05 s/epoch | Loss  0.93182 \n",
      "| Epoch    110 | 4.83 s/epoch | Loss  0.92846 \n",
      "| Epoch    120 | 4.81 s/epoch | Loss  0.92483 \n",
      "| Epoch    130 | 4.75 s/epoch | Loss  0.92265 \n",
      "| Epoch    140 | 4.81 s/epoch | Loss  0.92069 \n",
      "| Epoch    150 | 4.80 s/epoch | Loss  0.91799 \n",
      "| Epoch    160 | 4.95 s/epoch | Loss  0.91605 \n",
      "| Epoch    170 | 4.70 s/epoch | Loss  0.91469 \n",
      "| Epoch    180 | 4.62 s/epoch | Loss  0.91167 \n",
      "| Epoch    190 | 4.66 s/epoch | Loss  0.90975 \n",
      "| Epoch    200 | 4.61 s/epoch | Loss  0.90810 \n",
      "| Epoch    210 | 4.61 s/epoch | Loss  0.90767 \n",
      "| Epoch    220 | 4.74 s/epoch | Loss  0.90599 \n",
      "| Epoch    230 | 4.69 s/epoch | Loss  0.90544 \n",
      "| Epoch    240 | 4.79 s/epoch | Loss  0.90296 \n",
      "| Epoch    250 | 4.70 s/epoch | Loss  0.90186 \n",
      "| Epoch    260 | 4.58 s/epoch | Loss  0.90200 \n",
      "| Epoch    270 | 4.69 s/epoch | Loss  0.90134 \n",
      "| Epoch    280 | 4.74 s/epoch | Loss  0.89999 \n",
      "| Epoch    290 | 4.80 s/epoch | Loss  0.89906 \n",
      "| Epoch    300 | 4.69 s/epoch | Loss  0.89904 \n",
      "| Epoch    310 | 4.64 s/epoch | Loss  0.89752 \n",
      "| Epoch    320 | 4.64 s/epoch | Loss  0.89705 \n",
      "| Epoch    330 | 4.75 s/epoch | Loss  0.89522 \n",
      "| Epoch    340 | 4.61 s/epoch | Loss  0.89433 \n",
      "| Epoch    350 | 4.67 s/epoch | Loss  0.89478 \n",
      "| Epoch    360 | 4.68 s/epoch | Loss  0.89399 \n",
      "| Epoch    370 | 4.62 s/epoch | Loss  0.89261 \n",
      "| Epoch    380 | 4.71 s/epoch | Loss  0.89245 \n",
      "| Epoch    390 | 4.85 s/epoch | Loss  0.89113 \n",
      "| Epoch    400 | 4.64 s/epoch | Loss  0.89037 \n",
      "| Epoch    410 | 4.69 s/epoch | Loss  0.89108 \n",
      "| Epoch    420 | 4.61 s/epoch | Loss  0.88974 \n",
      "| Epoch    430 | 4.64 s/epoch | Loss  0.88915 \n",
      "| Epoch    440 | 4.72 s/epoch | Loss  0.88842 \n",
      "| Epoch    450 | 4.63 s/epoch | Loss  0.88748 \n",
      "| Epoch    460 | 4.69 s/epoch | Loss  0.88775 \n",
      "| Epoch    470 | 4.68 s/epoch | Loss  0.88681 \n",
      "| Epoch    480 | 4.69 s/epoch | Loss  0.88651 \n",
      "| Epoch    490 | 4.69 s/epoch | Loss  0.88741 \n",
      "| Epoch    500 | 4.84 s/epoch | Loss  0.88628 \n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment()\n",
    "horizon = 100\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "input_dim = (obs_dim + action_dim) * horizon\n",
    "\n",
    "# Training params\n",
    "lr = 0.001\n",
    "num_epochs = 500\n",
    "print_every = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "vf = MLP(input_dim=input_dim, time_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "optim = torch.optim.Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x_1 = create_trajectory_chunks(batch, horizon)\n",
    "        if x_1 is None:\n",
    "            continue\n",
    "        x_1 = x_1.to(device)\n",
    "        x_0 = torch.randn_like(x_1).to(device)\n",
    "        t = torch.rand(x_1.shape[0]).to(device)\n",
    "\n",
    "        # 3. Forward pass and Loss\n",
    "        path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
    "        predicted_velocity = vf(path_sample.x_t, path_sample.t)\n",
    "        loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
    "\n",
    "        # 4. Backward pass and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"| Epoch {epoch+1:6d} | {elapsed:.2f} s/epoch | Loss {avg_epoch_loss:8.5f} \")\n",
    "        start_time = time.time()\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "668e0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sampling from trained model...\n",
    "\n",
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b22867d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1000])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step size for ode solver\n",
    "step_size = 0.05\n",
    "\n",
    "batch_size = 1  # batch size\n",
    "T = torch.linspace(0,1,10)  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "x_init = torch.randn((batch_size, input_dim), dtype=torch.float32, device=device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)  # create an ODESolver class\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)\n",
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2d576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory():\n",
    "    sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)\n",
    "    final_trajectory = sol[-1]\n",
    "    reshaped_trajectory = final_trajectory.reshape(horizon, obs_dim + action_dim)\n",
    "    print(reshaped_trajectory.shape)\n",
    "    observations = reshaped_trajectory[:, :obs_dim]\n",
    "    actions = reshaped_trajectory[:, obs_dim:obs_dim + action_dim]\n",
    "    print(observations.shape, actions.shape)\n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "66c6c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100, 8]) torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "observations, actions = generate_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4042d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Action: [-1.0113046   0.09929121], Reward: 0.22038934106339525\n",
      "Action: [-0.30197778 -1.6979274 ], Reward: 0.9631495138452533\n",
      "Action: [ 1.6620966 -0.7585534], Reward: -1.4074033861991337\n",
      "Action: [0.43808994 1.5055774 ], Reward: -2.2912380142502786\n",
      "Action: [-1.7186     0.5056048], Reward: -0.3415482071275153\n",
      "Action: [ 2.6137555  -0.49187252], Reward: -4.167473103899954\n",
      "Action: [ 1.2885908  -0.13608862], Reward: -2.4754375993725146\n",
      "Action: [-0.8797167  1.875761 ], Reward: -0.9152571326638952\n",
      "Action: [-1.955152  -1.1314248], Reward: 1.0919989343134955\n",
      "Action: [-0.7718348 -0.4404358], Reward: 0.160859930516267\n",
      "Action: [1.1052104  0.37521407], Reward: -2.944002022035863\n",
      "Action: [0.31785864 1.2918353 ], Reward: -2.3368015759477077\n",
      "Action: [ 0.07928108 -1.3897114 ], Reward: -1.1875503234348161\n",
      "Action: [-0.820808 -2.848034], Reward: 1.2992528323366355\n",
      "Action: [-1.6282293   0.76094323], Reward: -0.5903354172017725\n",
      "Action: [-0.6522331 -0.7577608], Reward: 0.8966462321723009\n",
      "Action: [-0.21356429  0.6381553 ], Reward: -0.3970989018108537\n",
      "Action: [-0.56283027 -1.5041193 ], Reward: 1.0640367796306418\n",
      "Action: [-0.15713327  0.31773886], Reward: 0.16795068257169987\n",
      "Action: [ 1.3147845  -0.66343683], Reward: -1.118937461596047\n",
      "Action: [1.2060959 0.9927293], Reward: -2.509452736243038\n",
      "Action: [-0.3098464   0.20747583], Reward: 0.14969055955157273\n",
      "Action: [ 0.7172957 -2.2193308], Reward: -0.0951568205732303\n",
      "Action: [ 2.3928738 -1.3826584], Reward: -2.4686936092904985\n",
      "Action: [0.683539   0.14613274], Reward: -0.9329337749205593\n",
      "Action: [1.5299888 0.6133435], Reward: -3.5275913963353207\n",
      "Action: [-1.7882675 -1.1656657], Reward: 1.6401671026677331\n",
      "Action: [0.5004374  0.07018933], Reward: -0.2818799435890071\n",
      "Action: [ 0.34240136 -1.2339512 ], Reward: 0.8049774354554142\n",
      "Action: [0.2025177 0.5591168], Reward: -1.5067939290712582\n",
      "Action: [ 0.784643  -0.4675987], Reward: -1.506227153528465\n",
      "Action: [-0.5024271 -0.5108107], Reward: 1.3998198999765816\n",
      "Action: [-0.13000596  0.6210685 ], Reward: 0.08719733776752037\n",
      "Action: [1.1395856 1.3742161], Reward: -4.053101291632162\n",
      "Action: [-0.8502655  -0.71036774], Reward: 1.2293966369100793\n",
      "Action: [-0.60906523 -0.9805379 ], Reward: 1.418034789632095\n",
      "Action: [ 0.3173606  -0.01956456], Reward: -0.7357953640178266\n",
      "Action: [-1.0586684 -1.226222 ], Reward: 2.022207532485625\n",
      "Action: [-1.0875671  -0.61205405], Reward: 1.4807896809474164\n",
      "Action: [1.801491   0.62218994], Reward: -0.49318261226119375\n",
      "Action: [-0.6905711 -0.6653595], Reward: 1.671257832770499\n",
      "Action: [ 0.905992   -0.41286418], Reward: -0.1425632422089279\n",
      "Action: [ 0.87122923 -0.89833444], Reward: -0.8360394034940885\n",
      "Action: [ 1.2003686 -1.0266676], Reward: 2.371367094460352\n",
      "Action: [ 1.4443386 -2.0839608], Reward: 0.6520405616900984\n",
      "Action: [-1.3632388  0.3859823], Reward: 0.43705040364426395\n",
      "Action: [-0.30112222  1.1738799 ], Reward: -1.6466780672642176\n",
      "Action: [-0.3772626  0.3958234], Reward: -0.8128534603450532\n",
      "Action: [-0.5049284 -1.1165107], Reward: -0.15731235756123738\n",
      "Action: [ 0.32950893 -1.7160876 ], Reward: -1.6775976436927567\n",
      "Action: [1.327055  0.7764437], Reward: -2.47947237602453\n",
      "Action: [-0.5438845  1.4409561], Reward: -1.8447878905710684\n",
      "Action: [ 0.29664996 -1.0803305 ], Reward: -0.06627603313666441\n",
      "Action: [1.0012418 0.9025682], Reward: -2.99452547702542\n",
      "Action: [0.24257162 0.15408395], Reward: -0.8396661440921207\n",
      "Action: [ 0.6772189 -2.1672933], Reward: -1.849346213052461\n",
      "Action: [ 1.2737131 -0.787193 ], Reward: -0.3524672713908933\n",
      "Action: [-1.0947137  1.0954013], Reward: -2.2517812956984913\n",
      "Action: [ 1.2378786 -1.6014717], Reward: 1.3647802893773768\n",
      "Action: [0.5453658 1.5782142], Reward: -2.4086996227939252\n",
      "Action: [0.6109515 1.0017048], Reward: -2.291302149669484\n",
      "Action: [-0.12751006 -1.8922287 ], Reward: -0.12293547022747361\n",
      "Action: [ 1.3327351 -0.9775008], Reward: -1.681855642714968\n",
      "Action: [2.038622  3.2344482], Reward: -2.933217795306193\n",
      "Action: [-0.35163125  1.7828991 ], Reward: -1.477554488734653\n",
      "Action: [ 0.7074604  -0.94999486], Reward: -1.1941481535533922\n",
      "Action: [0.24243908 2.0726643 ], Reward: -1.1396507942762344\n",
      "Action: [-0.22399405 -0.18396139], Reward: -0.5476089907703567\n",
      "Action: [2.1154792 0.3576519], Reward: -1.7130648097660128\n",
      "Action: [0.5042889  0.26954097], Reward: 0.19525272151639683\n",
      "Action: [1.2889098  0.66031283], Reward: -2.3674576614669647\n",
      "Action: [0.24259216 0.73550195], Reward: -0.9044821557381113\n",
      "Action: [0.14669557 0.3701434 ], Reward: -1.0955008405446562\n",
      "Action: [-0.6888987  1.2955892], Reward: -0.7024362312921244\n",
      "Action: [1.1793462 1.2569747], Reward: -2.712577361570838\n",
      "Action: [ 0.15185179 -0.8294676 ], Reward: 0.6143808742275803\n",
      "Action: [ 2.4495518  -0.39455572], Reward: -1.6318878893465751\n",
      "Action: [-0.17150792  0.09116567], Reward: 0.2469524711100064\n",
      "Action: [-0.06531869 -0.376251  ], Reward: 0.15938997578152225\n",
      "Action: [ 1.1911031 -0.701796 ], Reward: -1.438596452899875\n",
      "Action: [-0.5676303 -1.553767 ], Reward: 0.528630956340238\n",
      "Action: [-0.7984827  0.3317486], Reward: -0.40631526432969167\n",
      "Action: [0.82686526 0.5019305 ], Reward: 0.4225784468610482\n",
      "Action: [-0.6920077 -0.511009 ], Reward: 0.13160360391712855\n",
      "Action: [-0.38453993 -1.4054028 ], Reward: 0.29028742084494863\n",
      "Action: [-0.28269708 -0.9528655 ], Reward: -0.2670638099497694\n",
      "Action: [ 1.1443949  -0.04339148], Reward: -0.5451664554234583\n",
      "Action: [-0.14959794  0.49736834], Reward: -1.4112911525801053\n",
      "Action: [-1.2799104 -1.0071243], Reward: -0.9619115699686052\n",
      "Action: [-1.3034074 -0.3505935], Reward: -2.212359866614321\n",
      "Action: [-1.6738349  1.693785 ], Reward: -3.0915381752935205\n",
      "Action: [-0.24806115 -0.14590108], Reward: -2.383544247345725\n",
      "Action: [0.10792299 2.8180485 ], Reward: 0.50378792222652\n",
      "Action: [2.1127224  0.08073722], Reward: 4.360409712160686\n",
      "Action: [ 0.12830432 -1.9988546 ], Reward: -0.21148992584472126\n",
      "Action: [0.98399025 1.1035488 ], Reward: 0.10809997062090246\n",
      "Action: [-0.30762902 -0.30438995], Reward: -2.6724763198722883\n",
      "Action: [-0.5771891  -0.27984855], Reward: -2.9357229155631615\n",
      "Action: [-2.1324177   0.42563888], Reward: -3.1087160084252616\n",
      "Action: [ 0.39308226 -0.23280036], Reward: 1.0962764738028568\n"
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment(eval_env = True, enable_wind=False, render_mode=\"human\")\n",
    "obs, _ = env.reset()\n",
    "total_rew = 0\n",
    "print(len(actions))\n",
    "for i in range(len(actions)):\n",
    "    action = actions[i].cpu().numpy()\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    total_rew += rew\n",
    "    print(f\"Action: {action}, Reward: {rew}\")\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "horizon = 100\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-deterministic-v1\")\n",
    "env = minari_dataset.recover_environment(eval_env=True, render_mode=\"human\")\n",
    "\n",
    "# --- Random Agent Evaluation ---\n",
    "print(\"\\n--- Running Random Agent ---\")\n",
    "obs, _ = env.reset()\n",
    "total_rew_random = 0\n",
    "for _ in range(horizon):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    total_rew_random += rew\n",
    "    env.render()\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished early.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward from random agent: {total_rew_random:.2f}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
