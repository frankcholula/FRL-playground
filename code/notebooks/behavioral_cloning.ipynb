{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Behavioral cloning with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here how to perform behavioral cloning on a Minari dataset using [PyTorch](https://pytorch.org/).\n",
    "We will start generating the dataset of the expert policy for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, which is a classic control problem.\n",
    "The objective is to balance the pole on the cart, and we receive a reward of +1 for each successful timestep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "For this tutorial you will need the [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) library, which you can install with `pip install rl_zoo3`.\n",
    "Let's then import all the required packages and set the random seed for reproducibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1158d6fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import minari\n",
    "from minari import DataCollector\n",
    "\n",
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy training\n",
    "Now we can train the expert policy using RL Baselines3 Zoo.\n",
    "We train a PPO agent on the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LunarLanderContinuous-v3 ==========\n",
      "Seed: 1897547941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsufanglu\u001b[0m (\u001b[33mfrankcholula\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frankcholula/Workspace/school/FRL-playground/wandb/run-20250707_194913-4iht0hhr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frankcholula/FRL/runs/4iht0hhr' target=\"_blank\">LunarLanderContinuous-v3__ppo__1897547941__1751888953</a></strong> to <a href='https://wandb.ai/frankcholula/FRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frankcholula/FRL' target=\"_blank\">https://wandb.ai/frankcholula/FRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frankcholula/FRL/runs/4iht0hhr' target=\"_blank\">https://wandb.ai/frankcholula/FRL/runs/4iht0hhr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hyperparameters from: /Users/frankcholula/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/rl_zoo3/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 64),\n",
      "             ('ent_coef', 0.01),\n",
      "             ('gae_lambda', 0.98),\n",
      "             ('gamma', 0.999),\n",
      "             ('n_envs', 16),\n",
      "             ('n_epochs', 4),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 16 environments\n",
      "Overwriting n_timesteps with n=1000000\n",
      "Creating test environment\n",
      "Using cpu device\n",
      "Log path: logs/ppo/LunarLanderContinuous-v3_4\n",
      "Logging to runs/LunarLanderContinuous-v3__ppo__1897547941__1751888953/LunarLanderContinuous-v3/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    fps             | 22817    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24992, episode_reward=-139.37 +/- 22.20\n",
      "Episode length: 65.40 +/- 9.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 65.4         |\n",
      "|    mean_reward          | -139         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24992        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059353397 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.000166     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+03     |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.73e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 12692    |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11114       |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004467642 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.00248    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.64e+03    |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.24e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49984, episode_reward=-122.97 +/- 57.20\n",
      "Episode length: 82.20 +/- 10.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 82.2         |\n",
      "|    mean_reward          | -123         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052393377 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.000738     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 747          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 2.19e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    fps             | 10479    |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74976, episode_reward=-68.56 +/- 38.31\n",
      "Episode length: 72.60 +/- 17.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 72.6        |\n",
      "|    mean_reward          | -68.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74976       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009187003 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.00703    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 307         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 781         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | -95.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 10131    |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | -81.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9958         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044391984 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.000592    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 306          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 581          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=99968, episode_reward=-68.40 +/- 48.90\n",
      "Episode length: 88.80 +/- 12.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 88.8         |\n",
      "|    mean_reward          | -68.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065487092 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.00221      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 196          |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 445          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -65.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 9799     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124960, episode_reward=-74.77 +/- 32.35\n",
      "Episode length: 94.20 +/- 2.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 94.2         |\n",
      "|    mean_reward          | -74.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062956344 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -0.00176     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 365          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | -48.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 9681     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 118        |\n",
      "|    ep_rew_mean          | -33.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9638       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509621 |\n",
      "|    clip_fraction        | 0.0588     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.85      |\n",
      "|    explained_variance   | -5.08e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 216        |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 440        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=149952, episode_reward=-98.37 +/- 49.37\n",
      "Episode length: 140.60 +/- 24.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 141        |\n",
      "|    mean_reward          | -98.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 149952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00515866 |\n",
      "|    clip_fraction        | 0.0403     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.83      |\n",
      "|    explained_variance   | -4.89e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 168        |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.00273   |\n",
      "|    std                  | 0.988      |\n",
      "|    value_loss           | 430        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | -21.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 9551     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174944, episode_reward=-132.47 +/- 53.10\n",
      "Episode length: 165.60 +/- 31.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 166         |\n",
      "|    mean_reward          | -132        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006302977 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 5.19e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 228         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 497         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | -22.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 9474     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9452        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005333285 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | -0.0189     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 329         |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.000809   |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 754         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=199936, episode_reward=52.42 +/- 184.14\n",
      "Episode length: 201.20 +/- 64.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 201         |\n",
      "|    mean_reward          | 52.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003936667 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | -0.00707    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 214         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.000784   |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 650         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | -1.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 9383     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224928, episode_reward=-73.48 +/- 58.47\n",
      "Episode length: 168.20 +/- 35.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 168          |\n",
      "|    mean_reward          | -73.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056592273 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -2.5e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 275          |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 665          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | -12.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 9325     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -26.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9316         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043896977 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 425          |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | 0.000821     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 779          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=249920, episode_reward=-103.68 +/- 51.10\n",
      "Episode length: 170.00 +/- 16.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 170          |\n",
      "|    mean_reward          | -104         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 249920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047015855 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 2.5e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 653          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000341    |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 928          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 265      |\n",
      "|    ep_rew_mean     | -31.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 9269     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274912, episode_reward=-106.50 +/- 20.58\n",
      "Episode length: 144.40 +/- 15.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 144          |\n",
      "|    mean_reward          | -106         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 274912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062305713 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -4.52e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 433          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 789          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 276      |\n",
      "|    ep_rew_mean     | -16.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 9228     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 307          |\n",
      "|    ep_rew_mean          | -8.56        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9219         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052695544 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 9.51e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 485          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 644          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=299904, episode_reward=-7.00 +/- 146.28\n",
      "Episode length: 215.20 +/- 114.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 215         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 299904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002993689 |\n",
      "|    clip_fraction        | 0.0141      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0124      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 428         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 386      |\n",
      "|    ep_rew_mean     | -9.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 9152     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324896, episode_reward=27.34 +/- 151.98\n",
      "Episode length: 253.40 +/- 90.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 253          |\n",
      "|    mean_reward          | 27.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038739454 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.533        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.6         |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 270          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 434      |\n",
      "|    ep_rew_mean     | 0.131    |\n",
      "| time/              |          |\n",
      "|    fps             | 9112     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 423          |\n",
      "|    ep_rew_mean          | 9.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9113         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043987734 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.705        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 310          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=349888, episode_reward=183.37 +/- 153.97\n",
      "Episode length: 274.40 +/- 58.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | 183          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 349888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043638106 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.3         |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 475      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 9072     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374880, episode_reward=110.59 +/- 129.37\n",
      "Episode length: 274.60 +/- 85.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 111         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 374880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004947194 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 91.5        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.000796   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 550      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 9035     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 576         |\n",
      "|    ep_rew_mean          | 33.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9040        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005482744 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66.2        |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=399872, episode_reward=-19.95 +/- 155.53\n",
      "Episode length: 286.00 +/- 151.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | -19.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 399872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004499326 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.000457   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 98.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 632      |\n",
      "|    ep_rew_mean     | 38.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 9005     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424864, episode_reward=-45.06 +/- 77.15\n",
      "Episode length: 240.40 +/- 85.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 240          |\n",
      "|    mean_reward          | -45.1        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 424864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030884973 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -9.76e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 43.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8982     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 728          |\n",
      "|    ep_rew_mean          | 49.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8985         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036741178 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 103          |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.000435    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=449856, episode_reward=45.87 +/- 141.34\n",
      "Episode length: 352.40 +/- 248.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | 45.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 449856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052745547 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.6         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8948     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474848, episode_reward=139.43 +/- 125.63\n",
      "Episode length: 363.80 +/- 196.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | 139          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 474848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054477304 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.4         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.000919    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 53       |\n",
      "| time/              |          |\n",
      "|    fps             | 8918     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 807          |\n",
      "|    ep_rew_mean          | 60.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8924         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055680433 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.3         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.000845    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=499840, episode_reward=182.99 +/- 128.39\n",
      "Episode length: 379.60 +/- 178.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 380         |\n",
      "|    mean_reward          | 183         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 499840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004650808 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00013    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 72.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8896     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 856          |\n",
      "|    ep_rew_mean          | 80           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8900         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033970529 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 124          |\n",
      "|    policy_gradient_loss | -0.000184    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 63.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=524832, episode_reward=149.99 +/- 73.11\n",
      "Episode length: 289.80 +/- 82.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 290         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004476536 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 128         |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 88.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 8882     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549824, episode_reward=230.74 +/- 42.15\n",
      "Episode length: 305.20 +/- 21.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | 231          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 549824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032201307 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 132          |\n",
      "|    policy_gradient_loss | -9.12e-05    |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 99.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 8863     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 875          |\n",
      "|    ep_rew_mean          | 97.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8870         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031845109 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.12         |\n",
      "|    n_updates            | 136          |\n",
      "|    policy_gradient_loss | -9.87e-05    |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 58.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=574816, episode_reward=226.84 +/- 82.63\n",
      "Episode length: 291.80 +/- 38.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 292          |\n",
      "|    mean_reward          | 227          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 574816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022415062 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000193    |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 143          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 98.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 8853     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599808, episode_reward=203.62 +/- 113.70\n",
      "Episode length: 275.80 +/- 56.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 276         |\n",
      "|    mean_reward          | 204         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 599808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004937497 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.000986   |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 98.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 8837     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 871         |\n",
      "|    ep_rew_mean          | 103         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8844        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004686108 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 148         |\n",
      "|    policy_gradient_loss | -0.000274   |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 31.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=624800, episode_reward=174.06 +/- 130.62\n",
      "Episode length: 349.40 +/- 46.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | 174          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047381558 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14         |\n",
      "|    n_updates            | 152          |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 10.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 8826     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649792, episode_reward=251.68 +/- 24.41\n",
      "Episode length: 331.80 +/- 35.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 649792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004277163 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.85        |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.000654   |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    fps             | 8810     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 960          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8818         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051876483 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 0.000379     |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 11.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=674784, episode_reward=184.43 +/- 112.09\n",
      "Episode length: 287.20 +/- 25.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 184          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 674784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041833865 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.47         |\n",
      "|    n_updates            | 164          |\n",
      "|    policy_gradient_loss | -0.000349    |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 18.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    fps             | 8805     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699776, episode_reward=256.73 +/- 30.92\n",
      "Episode length: 303.80 +/- 13.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | 257         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 699776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005421252 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 944      |\n",
      "|    ep_rew_mean     | 134      |\n",
      "| time/              |          |\n",
      "|    fps             | 8793     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 941          |\n",
      "|    ep_rew_mean          | 137          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8799         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032319021 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.7         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 31           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=724768, episode_reward=264.80 +/- 7.73\n",
      "Episode length: 316.40 +/- 6.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 724768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006763568 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.44        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | 0.000219    |\n",
      "|    std                  | 0.943       |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    fps             | 8786     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749760, episode_reward=263.33 +/- 17.17\n",
      "Episode length: 330.20 +/- 12.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 263         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 749760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003949656 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.01        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000853   |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 25.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    fps             | 8773     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 924          |\n",
      "|    ep_rew_mean          | 134          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8780         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044703013 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.1          |\n",
      "|    n_updates            | 184          |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    std                  | 0.938        |\n",
      "|    value_loss           | 26.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=774752, episode_reward=265.10 +/- 15.92\n",
      "Episode length: 324.60 +/- 14.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 265          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 774752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046494505 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.54         |\n",
      "|    n_updates            | 188          |\n",
      "|    policy_gradient_loss | 0.000868     |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 6.6          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 140      |\n",
      "| time/              |          |\n",
      "|    fps             | 8768     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799744, episode_reward=264.73 +/- 10.44\n",
      "Episode length: 331.80 +/- 10.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 799744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004828026 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.000852   |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 7.79        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 965      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    fps             | 8756     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 965          |\n",
      "|    ep_rew_mean          | 142          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8764         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046796347 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.6         |\n",
      "|    n_updates            | 196          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.926        |\n",
      "|    value_loss           | 21.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=824736, episode_reward=263.78 +/- 17.51\n",
      "Episode length: 317.00 +/- 11.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | 264          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 824736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059324317 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    std                  | 0.924        |\n",
      "|    value_loss           | 21.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    fps             | 8754     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849728, episode_reward=257.90 +/- 26.73\n",
      "Episode length: 331.80 +/- 17.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | 258          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 849728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054033455 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 204          |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    std                  | 0.919        |\n",
      "|    value_loss           | 6.69         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 8744     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 988          |\n",
      "|    ep_rew_mean          | 147          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8750         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045309905 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.37         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.000227    |\n",
      "|    std                  | 0.916        |\n",
      "|    value_loss           | 8.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=874720, episode_reward=254.83 +/- 20.56\n",
      "Episode length: 333.20 +/- 1.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 333          |\n",
      "|    mean_reward          | 255          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 874720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045548417 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.49         |\n",
      "|    n_updates            | 212          |\n",
      "|    policy_gradient_loss | 0.000697     |\n",
      "|    std                  | 0.914        |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    fps             | 8740     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899712, episode_reward=262.63 +/- 26.47\n",
      "Episode length: 325.00 +/- 17.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 263          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 899712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058511775 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.588        |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 3.94         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 8731     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 976          |\n",
      "|    ep_rew_mean          | 143          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8738         |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038825169 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 6.87e-05     |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 15.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=924704, episode_reward=272.50 +/- 8.36\n",
      "Episode length: 304.20 +/- 16.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 304          |\n",
      "|    mean_reward          | 273          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 924704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038414514 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.000952    |\n",
      "|    std                  | 0.904        |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    fps             | 8715     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949696, episode_reward=267.25 +/- 26.78\n",
      "Episode length: 291.20 +/- 8.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 291          |\n",
      "|    mean_reward          | 267          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 949696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046977648 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.889        |\n",
      "|    n_updates            | 228          |\n",
      "|    policy_gradient_loss | -0.000313    |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 5.08         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 8702     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 968          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8708         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056414334 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.79         |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | 0.000604     |\n",
      "|    std                  | 0.901        |\n",
      "|    value_loss           | 4.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=974688, episode_reward=258.68 +/- 14.88\n",
      "Episode length: 286.80 +/- 13.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 287        |\n",
      "|    mean_reward          | 259        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 974688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00437776 |\n",
      "|    clip_fraction        | 0.0324     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.63      |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.4       |\n",
      "|    n_updates            | 236        |\n",
      "|    policy_gradient_loss | 0.000791   |\n",
      "|    std                  | 0.908      |\n",
      "|    value_loss           | 16.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 8700     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 960         |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8704        |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004755489 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.07        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.000519    |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=999680, episode_reward=275.20 +/- 16.61\n",
      "Episode length: 264.40 +/- 8.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 264          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 999680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043333382 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41         |\n",
      "|    n_updates            | 244          |\n",
      "|    policy_gradient_loss | 0.000299     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 8697     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n",
      "Saving to logs/ppo/LunarLanderContinuous-v3_4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "sys.argv = [\n",
    "    \"python\", \n",
    "    \"--algo\", \"ppo\",\n",
    "    \"--env\", \"LunarLanderContinuous-v3\",\n",
    "    \"--n-timesteps\", \"1000000\",\n",
    "    \"--track\",\n",
    "    \"--wandb-project-name\", \"FRL\",\n",
    "    \"--wandb-entity\", \"frankcholula\",\n",
    "    \"--tensorboard-log\", \"runs\",\n",
    "    \"--hyperparams\",\n",
    "    \"n_envs:16\",\n",
    "    \"n_steps:1024\",\n",
    "    \"batch_size:64\",\n",
    "    \"n_epochs:4\",\n",
    "    \"gamma:0.999\",\n",
    "    \"gae_lambda:0.98\",\n",
    "    \"ent_coef:0.01\"\n",
    "]\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a new folder named `log` with the expert policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "Now let's generate the dataset using the [DataCollector](https://minari.farama.org/api/data_collector/) wrapper:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:07<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "env = DataCollector(gym.make(\"LunarLander-v3\", continuous=True))\n",
    "# path = os.path.abspath('') + '/logs/ppo/LunarLanderContinuous-v3_1/best_model.zip'\n",
    "path = os.path.abspath('')+ \"/code/models/ppo-LunarLander-v3/model.zip\"\n",
    "agent = PPO.load(path)\n",
    "\n",
    "total_episodes = 1_000\n",
    "for i in tqdm(range(total_episodes)):\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    while True:\n",
    "        action, _ = agent.predict(obs)\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = env.create_dataset(\n",
    "    dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\",\n",
    "    algorithm_name=\"ppo\",\n",
    "    code_permalink=\"https://github.com/frankcholula/FRL-playground/blob/main/code/behavioral_cloning.py\",\n",
    "    author=\"Frank Lu\",\n",
    "    author_email=\"lu.phrank@gmail.com\",\n",
    "    description=\"Behavioral cloning dataset for LunarLanderContinuous-v3 using PPO\",\n",
    "    eval_env=\"LunarLanderContinuous-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [-1. -1.], Reward: 2.080468461985818\n",
      "Action: [-0.9139328  -0.84831226], Reward: 2.0691645157514507\n",
      "Action: [-1.         -0.21528655], Reward: 1.6409476155531877\n",
      "Action: [-0.8679179 -1.       ], Reward: 1.6760357860007968\n",
      "Action: [-1.        -0.9008479], Reward: 1.645494362158638\n",
      "Action: [-1. -1.], Reward: 1.5719021784162817\n",
      "Action: [-1.  1.], Reward: -0.03407746207409332\n",
      "Action: [-1.         -0.22359085], Reward: 0.6510353826908499\n",
      "Action: [-0.30729365  1.        ], Reward: -0.2297617949425603\n",
      "Action: [-1. -1.], Reward: 1.2353997514459831\n",
      "Action: [-1.         -0.14613046], Reward: -0.06626395721497147\n",
      "Action: [-0.7597282   0.48485947], Reward: -0.3657382003323164\n",
      "Action: [-1. -1.], Reward: -0.05989879865518219\n",
      "Action: [-0.7734264  0.7703807], Reward: -1.7086247477080883\n",
      "Action: [-0.49912763 -0.16292202], Reward: -1.2862138396458818\n",
      "Action: [-0.28232288  1.        ], Reward: -2.142116784980401\n",
      "Action: [-0.50673634 -1.        ], Reward: -0.8093648874207406\n",
      "Action: [-1.         0.5135794], Reward: -2.1685111248135764\n",
      "Action: [-1.  1.], Reward: -2.4814577366277875\n",
      "Action: [-1.         -0.97159624], Reward: -1.4101762885708466\n",
      "Action: [-0.9808584  0.6550487], Reward: -2.3044963707788395\n",
      "Action: [-0.25062346 -0.25936443], Reward: -1.8926544566938617\n",
      "Action: [ 1. -1.], Reward: 2.9102819467060628\n",
      "Action: [0.6789633 1.       ], Reward: 0.2814979149425494\n",
      "Action: [0.07942379 1.        ], Reward: -0.220298752341954\n",
      "Action: [-1.         0.3589419], Reward: -1.6816975094236284\n",
      "Action: [-0.5506707  -0.53950274], Reward: -1.5749865331520778\n",
      "Action: [-0.9486017 -1.       ], Reward: -1.6414788685576684\n",
      "Action: [-1.         0.6169588], Reward: -2.212013494547948\n",
      "Action: [-1.  1.], Reward: -2.216862428296024\n",
      "Action: [ 1.         -0.56539303], Reward: 1.2641299093423481\n",
      "Action: [0.41705203 0.06296311], Reward: 0.8682507525449978\n",
      "Action: [-0.9973625 -0.8062842], Reward: -1.8028544125968506\n",
      "Action: [0.13684866 0.33165354], Reward: 1.3315777552465022\n",
      "Action: [1.         0.14562258], Reward: 0.45570872481694097\n",
      "Action: [1.         0.27358013], Reward: 3.2370519218946585\n",
      "Action: [-0.46063802  0.3428895 ], Reward: -2.1542056644791785\n",
      "Action: [-0.1011721   0.05447793], Reward: -2.1182139445345456\n",
      "Action: [-0.3017525  0.9353409], Reward: -2.1249109237008654\n",
      "Action: [-0.24749619  1.        ], Reward: -1.9117503540280996\n",
      "Action: [-0.34978384 -1.        ], Reward: -1.621322096795352\n",
      "Action: [-0.46890855  1.        ], Reward: -1.828316663807526\n",
      "Action: [0.4332307  0.69829977], Reward: 2.8928915885469064\n",
      "Action: [0.62604934 0.57756674], Reward: 0.8784572884218699\n",
      "Action: [ 0.42539304 -1.        ], Reward: 2.5826727560066005\n",
      "Action: [1.        0.8648876], Reward: 2.7226770409861287\n",
      "Action: [-1.         0.6686834], Reward: -1.4423184534942879\n",
      "Action: [0.1276986 0.9541182], Reward: 1.260844408161156\n",
      "Action: [0.9100814 0.4781895], Reward: 2.2194855210998012\n",
      "Action: [1. 1.], Reward: 1.4089960175648297\n",
      "Action: [ 0.871163   -0.66888463], Reward: 4.108973813278649\n",
      "Action: [1.         0.06772673], Reward: 2.796618875296514\n",
      "Action: [-0.45543516  1.        ], Reward: -1.17393464240041\n",
      "Action: [-0.00355989  1.        ], Reward: -0.9555460240822231\n",
      "Action: [0.74106354 0.03932649], Reward: 1.2982848957732016\n",
      "Action: [ 1.         -0.47338188], Reward: 3.2276523603019998\n",
      "Action: [ 1. -1.], Reward: 1.7243851862983046\n",
      "Action: [ 0.6860732  -0.84939295], Reward: 1.1513373361946253\n",
      "Action: [ 1. -1.], Reward: 3.7216070718601046\n",
      "Action: [ 1.         -0.25022817], Reward: 3.760080195805995\n",
      "Action: [-0.14654833 -0.51661104], Reward: -1.2812949970077683\n",
      "Action: [ 1. -1.], Reward: 4.307833803176676\n",
      "Action: [1.         0.18905042], Reward: 2.648843324120617\n",
      "Action: [ 1.         -0.47947773], Reward: 4.643782583684708\n",
      "Action: [-0.96489364  1.        ], Reward: -1.606450310609289\n",
      "Action: [0.11171311 0.3303209 ], Reward: 0.46677132627161766\n",
      "Action: [ 0.01367825 -1.        ], Reward: 0.3396297901489288\n",
      "Action: [ 1.        -0.8329414], Reward: 3.8891346279355083\n",
      "Action: [-0.0137201  -0.73523444], Reward: -2.158907473585251\n",
      "Action: [1. 1.], Reward: 0.5845787866723082\n",
      "Action: [-0.3400569   0.27624124], Reward: -1.8482907626466272\n",
      "Action: [1.        0.5695857], Reward: 3.5768010333546183\n",
      "Action: [ 1.         -0.19904567], Reward: 0.5765254458939466\n",
      "Action: [ 1.         -0.11989014], Reward: 1.3225173549643159\n",
      "Action: [ 0.66480386 -0.86293876], Reward: 1.7517379850146766\n",
      "Action: [-0.478508  1.      ], Reward: -1.7147390391170052\n",
      "Action: [ 1. -1.], Reward: 1.4581980240077268\n",
      "Action: [0.09167507 1.        ], Reward: -0.003397464086156099\n",
      "Action: [1.         0.71854025], Reward: 1.3740928106088177\n",
      "Action: [ 0.47627917 -1.        ], Reward: 0.20961328549166866\n",
      "Action: [0.516855  0.8985584], Reward: 1.2350824938963945\n",
      "Action: [-0.30118543  0.16240923], Reward: -1.682455478331491\n",
      "Action: [ 1.        -0.6283834], Reward: 4.2228690427451445\n",
      "Action: [-0.39233068  0.02292142], Reward: -1.8746131106029225\n",
      "Action: [1.         0.36424664], Reward: 3.636718316325653\n",
      "Action: [1. 1.], Reward: 1.9959675379296995\n",
      "Action: [ 1.         -0.41996205], Reward: 3.727591569524688\n",
      "Action: [ 0.4546798 -1.       ], Reward: 2.5287158699132406\n",
      "Action: [-0.10560074 -0.02679789], Reward: -1.9986194480844262\n",
      "Action: [0.5922251  0.38079953], Reward: 0.1289764756847035\n",
      "Action: [-0.9080465 -0.5774889], Reward: -2.042838913005166\n",
      "Action: [ 0.02737117 -0.16855653], Reward: -0.48979160542434097\n",
      "Action: [-0.22794342  0.945005  ], Reward: -1.507916800832083\n",
      "Action: [ 0.7770741  -0.44078368], Reward: 2.06802728236851\n",
      "Action: [0.13005126 0.7582848 ], Reward: -0.3131991534365477\n",
      "Action: [1. 1.], Reward: 1.482691112991814\n",
      "Action: [-0.96089745  0.15235221], Reward: -1.201799279907675\n",
      "Action: [ 1.         -0.16545403], Reward: 3.9730861981171133\n",
      "Action: [ 0.8973638 -0.6297213], Reward: 1.3465918714471228\n",
      "Action: [1. 1.], Reward: 4.112326800877225\n",
      "Action: [-0.4101618  -0.31131655], Reward: -1.2780073680187911\n",
      "Action: [ 1.         -0.18583965], Reward: 4.986899274316838\n",
      "Action: [-1.        -0.6014999], Reward: -1.4659614006969304\n",
      "Action: [ 0.79483986 -1.        ], Reward: 2.92353759248261\n",
      "Action: [ 0.7166437 -0.3568284], Reward: 1.148462524969159\n",
      "Action: [ 0.469243  -0.9282057], Reward: -0.42269477798564253\n",
      "Action: [ 0.26511854 -1.        ], Reward: 0.9016263950305312\n",
      "Action: [-1.         -0.34673077], Reward: -2.062242251707218\n",
      "Action: [ 0.28899547 -0.68608713], Reward: 1.9242550198372947\n",
      "Action: [-0.38416106 -0.20816213], Reward: -2.1794319352860754\n",
      "Action: [-1.          0.44030213], Reward: -2.1218657050314818\n",
      "Action: [1. 1.], Reward: 0.07372204923319545\n",
      "Action: [ 1.        -0.4092539], Reward: 3.9429267292596366\n",
      "Action: [-1. -1.], Reward: -2.3113103408168585\n",
      "Action: [-0.20126593 -1.        ], Reward: -2.3671932964242317\n",
      "Action: [ 0.04053879 -0.4488265 ], Reward: 0.22708432081423666\n",
      "Action: [1.         0.31738055], Reward: 1.4225525013485452\n",
      "Action: [-0.8832171   0.02629513], Reward: -2.191692429744961\n",
      "Action: [0.9532872 0.7296699], Reward: 1.8491794465045048\n",
      "Action: [1. 1.], Reward: 1.4199675731380506\n",
      "Action: [-1.         0.4547159], Reward: -1.848957054001275\n",
      "Action: [0.3286882  0.05096579], Reward: -0.2798343116897172\n",
      "Action: [1. 1.], Reward: 1.1590190368783464\n",
      "Action: [ 1. -1.], Reward: 1.2671296238649585\n",
      "Action: [0.49119312 1.        ], Reward: 2.939202528304885\n",
      "Action: [-0.7479601  0.8938585], Reward: -1.4453797222520257\n",
      "Action: [-0.11519337 -1.        ], Reward: -1.9279157863329874\n",
      "Action: [ 1. -1.], Reward: -0.08779218589103435\n",
      "Action: [1.        0.4371258], Reward: 0.4272567388718727\n",
      "Action: [1.         0.09219411], Reward: 3.070066959983916\n",
      "Action: [ 1.         -0.25664908], Reward: 1.4776368874945092\n",
      "Action: [0.10588437 1.        ], Reward: 1.9565441914844894\n",
      "Action: [1.        0.5545529], Reward: 0.3887583275691685\n",
      "Action: [1.         0.16611898], Reward: 1.2529006985920972\n",
      "Action: [1. 1.], Reward: 3.253879100358502\n",
      "Action: [ 0.7733227 -0.1829797], Reward: 0.4622347471132457\n",
      "Action: [ 0.14325881 -1.        ], Reward: 0.28836487329354665\n",
      "Action: [ 1.         -0.03633796], Reward: 3.43554748852618\n",
      "Action: [ 1. -1.], Reward: 2.7949428928226028\n",
      "Action: [0.998579 1.      ], Reward: 4.905423460001485\n",
      "Action: [-1.  1.], Reward: -0.6574293846728796\n",
      "Action: [ 1. -1.], Reward: 3.2617205332912804\n",
      "Action: [-0.87033427 -0.928626  ], Reward: -2.1444649876426367\n",
      "Action: [1. 1.], Reward: 2.9185367525899424\n",
      "Action: [ 0.08722258 -1.        ], Reward: 0.6007363694101042\n",
      "Action: [-1.         0.6899793], Reward: -1.1140413185423035\n",
      "Action: [-0.93021894  0.819435  ], Reward: -1.0886186647619036\n",
      "Action: [-0.15573066 -1.        ], Reward: -2.2000856219982254\n",
      "Action: [ 1. -1.], Reward: 0.3585102788275646\n",
      "Action: [0.67702514 1.        ], Reward: 0.9965318703715735\n",
      "Action: [ 1.         -0.43293208], Reward: 1.7399295591347907\n",
      "Action: [ 1.         -0.34909582], Reward: 1.8427278679938979\n",
      "Action: [ 1. -1.], Reward: 3.355628044526283\n",
      "Action: [ 0.58702433 -0.6663656 ], Reward: 0.5499445557072249\n",
      "Action: [-1.          0.58197284], Reward: -1.3175086430328815\n",
      "Action: [0.49623618 0.99580383], Reward: 2.3954322996988267\n",
      "Action: [-0.3062992  0.5076393], Reward: -1.7931074236228834\n",
      "Action: [ 1.        -0.7286521], Reward: 2.04968155808155\n",
      "Action: [1.       0.886341], Reward: 3.613497025809092\n",
      "Action: [0.21222815 0.2942387 ], Reward: -0.5525541551469303\n",
      "Action: [ 0.20922762 -0.0454294 ], Reward: -0.5201058352537842\n",
      "Action: [ 1. -1.], Reward: 0.5562679905238408\n",
      "Action: [1.         0.34963936], Reward: 1.2151429821232156\n",
      "Action: [ 0.24408582 -0.53356063], Reward: 1.4541100976106045\n",
      "Action: [ 0.8871385 -1.       ], Reward: 3.362419073803346\n",
      "Action: [-1.  1.], Reward: -1.3797244172914336\n",
      "Action: [0.8301375 1.       ], Reward: 1.8325739084598123\n",
      "Action: [-1. -1.], Reward: -2.603049944590025\n",
      "Action: [1.        0.5810313], Reward: 2.3751668790356923\n",
      "Action: [1. 1.], Reward: 2.1791758633730454\n",
      "Action: [1.        0.7113364], Reward: 3.8186587063108353\n",
      "Action: [-1.        -0.6750021], Reward: -2.364347741023504\n",
      "Action: [-0.29749152  0.19237682], Reward: -2.087137684225212\n",
      "Action: [1.         0.10676844], Reward: -0.09143488745245348\n",
      "Action: [-0.6255369  0.7155119], Reward: -1.717232561577003\n",
      "Action: [1.        0.8817998], Reward: 3.31457008745797\n",
      "Action: [ 1. -1.], Reward: 1.2233578395027944\n",
      "Action: [0.8269332  0.74375445], Reward: 2.315628280592104\n",
      "Action: [-0.19583899  0.13540658], Reward: -2.48715326766181\n",
      "Action: [-1.        -0.7994829], Reward: -2.552107971210356\n",
      "Action: [-0.41783902 -0.902935  ], Reward: -2.2394067287247212\n",
      "Action: [1.        0.8154317], Reward: 1.886491742839026\n",
      "Action: [ 0.38648582 -0.51027006], Reward: 1.1382650654080562\n",
      "Action: [ 1. -1.], Reward: 3.3000317457727832\n",
      "Action: [1. 1.], Reward: 2.069776397331465\n",
      "Action: [0.23726834 0.2359314 ], Reward: 1.1918227127605519\n",
      "Action: [ 0.46454036 -0.28603923], Reward: -0.2523061229865817\n",
      "Action: [ 0.5224631  -0.27143526], Reward: 1.529240467637497\n",
      "Action: [1.        0.2756393], Reward: 2.53192356646763\n",
      "Action: [ 0.668504  -0.6918782], Reward: -0.47908962376183695\n",
      "Action: [-0.89166325 -0.63968307], Reward: -2.5618133888361303\n",
      "Action: [-0.27472457  0.34357518], Reward: -2.6963424505135265\n",
      "Action: [-1.  1.], Reward: -2.375950357875672\n",
      "Action: [-0.31609654 -0.0500727 ], Reward: -2.356722476571967\n",
      "Action: [1. 1.], Reward: 4.352697173557798\n",
      "Action: [-1.          0.38602406], Reward: -2.2174115513972623\n",
      "Action: [ 0.2973837 -1.       ], Reward: -0.8744254724125159\n",
      "Action: [-1. -1.], Reward: -2.4862954701869016\n",
      "Action: [0.22442684 1.        ], Reward: -1.0475418426690128\n",
      "Action: [0.617575 1.      ], Reward: -0.43529987132120485\n",
      "Action: [1.         0.23481038], Reward: 0.4195028044311016\n",
      "Action: [-0.24150026 -1.        ], Reward: -2.1987654671037533\n",
      "Action: [ 1. -1.], Reward: 2.7488438078216704\n",
      "Action: [1.        0.7706372], Reward: -0.30892795495544106\n",
      "Action: [-0.01988548  1.        ], Reward: -1.9866620053330297\n",
      "Action: [ 0.39232683 -1.        ], Reward: -0.06657159763976095\n",
      "Action: [ 0.33783412 -0.17958668], Reward: -0.6810986872360829\n",
      "Action: [ 1. -1.], Reward: 3.851292876673176\n",
      "Action: [-1.         0.7684508], Reward: -2.3176341692306908\n",
      "Action: [-0.29989576  1.        ], Reward: -2.186668265947573\n",
      "Action: [ 1.         -0.60650015], Reward: 0.40001894739038196\n",
      "Action: [1. 1.], Reward: 3.338317268314816\n",
      "Action: [ 0.05339432 -0.5645867 ], Reward: 1.038365802136041\n",
      "Action: [ 1. -1.], Reward: 2.961829743340067\n",
      "Action: [1.         0.14862016], Reward: 0.8238660507156623\n",
      "Action: [ 1.      -0.58797], Reward: 3.8496130628717795\n",
      "Action: [0.12511572 0.99490047], Reward: -1.219455006070247\n",
      "Action: [1. 1.], Reward: -0.24501986365033615\n",
      "Action: [1. 1.], Reward: 3.18756286859661\n",
      "Action: [0.32683414 0.83339804], Reward: -0.41465246472807743\n",
      "Action: [0.7783656 0.7902353], Reward: 3.9567576232242607\n",
      "Action: [1. 1.], Reward: 3.6917480942917735\n",
      "Action: [0.84870094 0.539836  ], Reward: 2.791731385950809\n",
      "Action: [-0.0135397 -1.       ], Reward: -1.6979515267188698\n",
      "Action: [0.56727064 0.8409909 ], Reward: 1.8086184648400148\n",
      "Action: [ 1. -1.], Reward: 2.7916618027665145\n",
      "Action: [1.        0.2889989], Reward: 0.6882702601441106\n",
      "Action: [-0.01842763 -0.09300691], Reward: -2.515130943731558\n",
      "Action: [ 0.410596 -1.      ], Reward: -0.7608563500203412\n",
      "Action: [-0.12093017 -0.8505155 ], Reward: -2.646161374683062\n",
      "Action: [ 0.75626487 -1.        ], Reward: 1.0381173223545244\n",
      "Action: [ 1. -1.], Reward: 2.5006649814045425\n",
      "Action: [-1.  1.], Reward: -2.2998260360558036\n",
      "Action: [-0.38964286 -0.29035246], Reward: -2.383294662067186\n",
      "Action: [ 1.         -0.34977263], Reward: -0.24174007631104572\n",
      "Action: [ 1.         -0.84823227], Reward: 1.5723310392638392\n",
      "Action: [ 0.6102708 -1.       ], Reward: 0.971593491215035\n",
      "Action: [1. 1.], Reward: 2.8694347358978303\n",
      "Action: [-0.8086788  1.       ], Reward: -2.335303705295758\n",
      "Action: [ 0.5599049  -0.30444843], Reward: 1.832804457194373\n",
      "Action: [ 0.06992206 -0.8908877 ], Reward: -0.7674213583517812\n",
      "Action: [-1. -1.], Reward: -3.171237529499591\n",
      "Action: [ 1. -1.], Reward: 2.2432078097790495\n",
      "Action: [1. 1.], Reward: 3.3535561993130263\n",
      "Action: [-1.  1.], Reward: -2.558378853102797\n",
      "Action: [0.8673947 0.4133314], Reward: 0.41056409121047077\n",
      "Action: [ 1.         -0.24347204], Reward: 1.3066669038324867\n",
      "Action: [1. 1.], Reward: -0.25595208706675077\n",
      "Action: [-0.51988447  1.        ], Reward: -2.6956466288559455\n",
      "Action: [-0.30060107 -0.23725131], Reward: 10.985642462932649\n",
      "Action: [-0.44817525 -1.        ], Reward: 8.722130490839662\n",
      "Action: [-1. -1.], Reward: -0.09266836369159634\n",
      "Action: [-1. -1.], Reward: 1.983243499904469\n",
      "Action: [-1.         -0.17231065], Reward: 2.4307759642916693\n",
      "Action: [-1.         -0.45449412], Reward: 2.3356823047932913\n",
      "Action: [-1.          0.70410943], Reward: 0.04696030168559744\n",
      "Action: [-1. -1.], Reward: -2.1195951871258676\n",
      "Action: [-1.         -0.18529934], Reward: -1.7533878194363286\n",
      "Action: [-1.        -0.6886869], Reward: 0.514588973466834\n",
      "Action: [-1.          0.67682767], Reward: 0.9655383982542094\n",
      "Action: [-1.        0.509516], Reward: 1.2921250975691805\n",
      "Action: [-1.  1.], Reward: 1.2161437966893065\n",
      "Action: [-1.         -0.60687476], Reward: 1.1534173469849445\n",
      "Action: [-1.        -0.6657312], Reward: 0.8339681351506532\n",
      "Action: [-1. -1.], Reward: -0.027676287372481595\n",
      "Action: [-1.        -0.7173692], Reward: -0.31847589083141814\n",
      "Action: [-1.  1.], Reward: 0.953540098932572\n",
      "Action: [-1.  1.], Reward: 0.22010667309386037\n",
      "Action: [-1.          0.46641567], Reward: -0.12056210906242626\n",
      "Action: [-1.          0.36744374], Reward: 1.2268692249842772\n",
      "Action: [-1.        0.417942], Reward: 0.11316365338879919\n",
      "Action: [-1.  1.], Reward: -0.4813868902958294\n",
      "Action: [-1.  1.], Reward: -0.3071408531496693\n",
      "Action: [-1.         0.5325999], Reward: -0.012905237062602808\n",
      "Action: [-1.          0.48205435], Reward: 0.5636737223106572\n",
      "Action: [-1.         0.8800043], Reward: -0.13238643135353478\n",
      "Action: [-1.  1.], Reward: -0.45544816971128266\n",
      "Action: [-1.          0.24597265], Reward: 0.5642973402495848\n",
      "Action: [-1. -1.], Reward: -0.30626272392212317\n",
      "Action: [-1.          0.31009087], Reward: 0.4012356812909559\n",
      "Action: [-1.          0.02478337], Reward: -0.09136764957115062\n",
      "Action: [-1.         -0.31896412], Reward: 0.029482998366770374\n",
      "Action: [-1.        -0.8053682], Reward: -0.18282300840630328\n",
      "Action: [-1.  1.], Reward: -0.19032104061285324\n",
      "Action: [-1.        -0.9804922], Reward: 0.0868819455682302\n",
      "Action: [-1.  1.], Reward: -0.0493826239269228\n",
      "Action: [-1.          0.38881367], Reward: 0.2866686892463193\n",
      "Action: [-1.  1.], Reward: -0.38016433468575594\n",
      "Action: [-1.        -0.5748595], Reward: 0.09822823946316937\n",
      "Action: [-1.          0.41091558], Reward: 0.16038178161437422\n",
      "Action: [-1.         -0.27367246], Reward: 0.0802405275497815\n",
      "Action: [-1.        -0.3386101], Reward: -0.026886407018920977\n",
      "Action: [-1.         -0.23150623], Reward: 0.021219734538032498\n",
      "Action: [-1.        -0.7648156], Reward: -0.22699167028341613\n",
      "Action: [-1.         -0.43568057], Reward: 0.1565737872002888\n",
      "Action: [-1. -1.], Reward: -0.4738895130159839\n",
      "Action: [-1.         0.0386332], Reward: 0.5086593092849601\n",
      "Action: [-1.          0.78250307], Reward: -0.48929146889150504\n",
      "Action: [-1.          0.26198894], Reward: 0.4570208888274738\n",
      "Action: [-1.  1.], Reward: -0.5455049264806593\n",
      "Action: [-1.          0.19324112], Reward: 0.5034755125094907\n",
      "Action: [-1.         -0.05860236], Reward: -0.15887737749835473\n",
      "Action: [-1.         -0.29168537], Reward: 0.0724646036745007\n",
      "Action: [-1. -1.], Reward: -0.4534820269381121\n",
      "Action: [-1.         -0.74957657], Reward: -0.21582499381560305\n",
      "Action: [-1.  1.], Reward: 0.4976271067022229\n",
      "Action: [-1.  1.], Reward: -0.584798414456025\n",
      "Action: [-0.8357723  1.       ], Reward: -0.6118194664707974\n",
      "Action: [-1.         -0.94344056], Reward: 1.2179719032309504\n",
      "Action: [-1.          0.79310405], Reward: -0.02351713778294396\n",
      "Action: [-1.          0.11594181], Reward: -0.059491120625938976\n",
      "Action: [-1.  1.], Reward: -0.3371402975954989\n",
      "Action: [-1.          0.06080696], Reward: 0.3999440038497468\n",
      "Action: [-1.         0.6219114], Reward: 0.015755332259892327\n",
      "Action: [-1. -1.], Reward: -0.45270279274393377\n",
      "Action: [-1.         0.9335903], Reward: 0.09762277700151295\n",
      "Action: [-1.        -0.9672552], Reward: -0.09321990553316979\n",
      "Action: [-1.         -0.19537657], Reward: 0.325372951462219\n",
      "Action: [-1.          0.06906562], Reward: -0.10887559981829398\n",
      "Action: [-1.          0.32245412], Reward: 0.07363424438056398\n",
      "Action: [-1.  1.], Reward: -0.3456457686665455\n",
      "Action: [-1.          0.66119164], Reward: -0.06769723390105362\n",
      "Action: [-1. -1.], Reward: 0.10070324656084509\n",
      "Action: [-1.         -0.26690277], Reward: 0.2459272453532133\n",
      "Action: [-1.  1.], Reward: -0.42352045017446005\n",
      "Action: [-1.          0.59100693], Reward: -0.17609711648569606\n",
      "Action: [-1.          0.35455668], Reward: 0.5650916699626887\n",
      "Action: [-1.         0.9804046], Reward: -0.4445417254064658\n",
      "Action: [-1.        -0.5480814], Reward: 0.23314606223975645\n",
      "Action: [-1.          0.20460947], Reward: 0.09743136108490802\n",
      "Action: [-1.  1.], Reward: -0.2567165406833183\n",
      "Action: [-1.  1.], Reward: -0.36423374046572543\n",
      "Action: [-1.  1.], Reward: -0.2870836979099447\n",
      "Action: [-1.       -0.549336], Reward: 0.8214074192918751\n",
      "Action: [-1.  1.], Reward: -0.20684079491078108\n",
      "Action: [-1. -1.], Reward: -0.1425644042340106\n",
      "Action: [-1.          0.22827238], Reward: 0.31412999985737144\n",
      "Action: [-1. -1.], Reward: -0.23168491449182824\n",
      "Action: [-1.         -0.31889182], Reward: 0.15357548578941582\n",
      "Action: [-1.         -0.05354093], Reward: -0.052199670291532385\n",
      "Action: [-1. -1.], Reward: -0.12620200670157786\n",
      "Action: [-1.        -0.9990828], Reward: -0.5222406005755038\n",
      "Action: [-0.5801976  0.9202881], Reward: 0.5044409672044023\n",
      "Action: [-1.  1.], Reward: -0.6108834223399373\n",
      "Action: [-1.         -0.70462006], Reward: 0.6160662772587591\n",
      "Action: [-1.          0.04851601], Reward: 0.03487089529949117\n",
      "Action: [-1.  1.], Reward: -0.22116311066997227\n",
      "Action: [-1.         -0.00137798], Reward: 0.22874050961061698\n",
      "Action: [-1.  1.], Reward: -0.4618487248542482\n",
      "Action: [-1.  1.], Reward: -0.29432770204947967\n",
      "Action: [-1.  1.], Reward: -0.25081762741432523\n",
      "Action: [-1.  1.], Reward: -0.672073460685932\n",
      "Action: [-1.  1.], Reward: -0.41988058248124704\n",
      "Action: [-1.        -0.4735986], Reward: 0.5515637810016614\n",
      "Action: [-1.        -0.2525773], Reward: 0.5572209348501911\n",
      "Action: [-1.          0.08342847], Reward: 0.5628649257265792\n",
      "Action: [-1.  1.], Reward: -0.44584334664960745\n",
      "Action: [-1. -1.], Reward: 0.29129001695760626\n",
      "Action: [-1.         -0.54327106], Reward: 0.005105571475724064\n",
      "Action: [-1.  1.], Reward: -0.12528580874208892\n",
      "Action: [-1.          0.76353014], Reward: -0.11961089058366987\n",
      "Action: [-1. -1.], Reward: 0.14728448473556924\n",
      "Action: [-1.  1.], Reward: 0.11549732845306807\n",
      "Action: [-1.         -0.76247704], Reward: -0.13703787115193455\n",
      "Action: [-1.       -0.605892], Reward: -0.03588990001805349\n",
      "Action: [-1.          0.29029033], Reward: 0.28680494349237406\n",
      "Action: [-1.          0.45883638], Reward: -0.0963924375696692\n",
      "Action: [-1.        -0.7484149], Reward: -0.04715747233384576\n",
      "Action: [-1.         0.6317244], Reward: -0.14400857749262372\n",
      "Action: [-1.       -0.915196], Reward: -0.011028439403742069\n",
      "Action: [-1.         0.5371799], Reward: -0.00011849787339578144\n",
      "Action: [-1.         0.5348031], Reward: 0.021485372680806025\n",
      "Action: [-1.  1.], Reward: -0.27062436813058954\n",
      "Action: [-0.6693797   0.07455516], Reward: 0.43973350444746195\n",
      "Action: [-1.        -0.9304106], Reward: -0.5967549971419164\n",
      "Action: [-1.         0.5408391], Reward: 0.4661239225227191\n",
      "Action: [-1.          0.73568755], Reward: -0.3124599032621632\n",
      "Action: [-1.         -0.29404497], Reward: 0.36638494982645575\n",
      "Action: [-1.        -0.0173206], Reward: -0.11371765846080173\n",
      "Action: [-1.  1.], Reward: -0.27454653972494003\n",
      "Action: [-1.          0.46196526], Reward: 0.33747883678836743\n",
      "Action: [-1.          0.19223204], Reward: -0.1035943375255588\n",
      "Action: [-1.  1.], Reward: -0.2879739193192854\n",
      "Action: [-1.          0.33779836], Reward: 0.37351524829535343\n",
      "Action: [-1.  1.], Reward: -0.2704455820299294\n",
      "Action: [-1.         -0.12383224], Reward: 0.19742907248734198\n",
      "Action: [-1.         0.0534188], Reward: -0.057284217376869506\n",
      "Action: [-1.        -0.8655167], Reward: -0.3215837953272046\n",
      "Action: [-1.       -0.275651], Reward: 0.4163487770203922\n",
      "Action: [-1.        -0.3065108], Reward: -0.13938937806890905\n",
      "Action: [-1.          0.10113557], Reward: 0.07264146723313303\n",
      "Action: [-1.          0.09374566], Reward: 0.09257182040519218\n",
      "Action: [-1.        -0.9715695], Reward: -0.4711184408480711\n",
      "Action: [-1.  1.], Reward: 0.020732817267968856\n",
      "Action: [-1.        -0.6874478], Reward: 0.15090026943706542\n",
      "Action: [-1.  1.], Reward: -0.09293601645218239\n",
      "Action: [-1.  1.], Reward: -0.677310784099922\n",
      "Action: [-1. -1.], Reward: 0.594269137451261\n",
      "Action: [-1.         0.6193016], Reward: 0.13927157670968754\n",
      "Action: [-1.         -0.82796645], Reward: -0.12650859872286646\n",
      "Action: [-1. -1.], Reward: -0.41458731196101684\n",
      "Action: [-1.  1.], Reward: 0.3319831451794155\n",
      "Action: [-1.         -0.53942424], Reward: 0.17164178735050314\n",
      "Action: [-1.  1.], Reward: -0.3731571705843624\n",
      "Action: [-1.         -0.52279997], Reward: 0.13977488648367697\n",
      "Action: [-1.          0.42413765], Reward: 0.1830426478210576\n",
      "Action: [-0.833537  1.      ], Reward: -0.2692793942852869\n",
      "Action: [-1.         -0.24795939], Reward: 0.2732932466395841\n",
      "Action: [-1.         0.2204156], Reward: -0.08348864447730442\n",
      "Action: [-1.         -0.27546152], Reward: 0.06998986312917133\n",
      "Action: [0.12888765 1.        ], Reward: -2.4985024963641638\n",
      "Action: [-1.          0.61744016], Reward: -0.08672662454080524\n",
      "Action: [-1.         -0.57346314], Reward: 1.0168867634704732\n",
      "Action: [-1.  1.], Reward: -0.40233236869222355\n",
      "Action: [-1.         -0.12217213], Reward: 0.5555239619928454\n",
      "Action: [-1.          0.89684063], Reward: -0.42001191068444654\n",
      "Action: [-1. -1.], Reward: 1.287397025042199\n",
      "Action: [-1.         0.7008152], Reward: -0.04692543606386202\n",
      "Action: [-1.          0.01545203], Reward: -0.06666824819259709\n",
      "Action: [-1.  1.], Reward: -0.09366014108958495\n",
      "Action: [-1.          0.01905265], Reward: 0.10204351663219668\n",
      "Action: [-1.        -0.7496611], Reward: -0.2973606826339594\n",
      "Action: [-1. -1.], Reward: -0.3156433200942874\n",
      "Action: [-1.          0.75632644], Reward: 0.45454491333744346\n",
      "Action: [-1.  1.], Reward: -0.6240751281837607\n",
      "Action: [-1.         0.9730901], Reward: -0.5673895385903038\n",
      "Action: [-1.         -0.09218533], Reward: 0.5578128358524985\n",
      "Action: [-1.  1.], Reward: -0.21622365163796672\n",
      "Action: [-1. -1.], Reward: 0.6854774028297375\n",
      "Action: [-1.          0.98376423], Reward: -0.1653021074233436\n",
      "Action: [-1.          0.00237779], Reward: 0.2892236392320271\n",
      "Action: [-1.          0.82530886], Reward: -0.14268369822692967\n",
      "Action: [-1.         -0.25116116], Reward: 0.06113085919496619\n",
      "Action: [-1.        -0.5048559], Reward: -0.19946218401848617\n",
      "Action: [-1.        -0.7557651], Reward: -0.22060515008200213\n",
      "Action: [-0.97727835 -1.        ], Reward: -0.5976210451871686\n",
      "Action: [-1.        -0.5531883], Reward: 0.07729188324138896\n",
      "Action: [-1.  1.], Reward: 0.7257477920761588\n",
      "Action: [-1.         0.0705851], Reward: 0.1760757426596733\n",
      "Action: [-1.  1.], Reward: -0.6278990486276104\n",
      "Action: [-1.         -0.36465323], Reward: 0.5641048337860717\n",
      "Action: [-1.  1.], Reward: -0.44053716157601275\n",
      "Action: [-1.        -0.2386236], Reward: 0.4948683976396637\n",
      "Action: [-1.        -0.3605757], Reward: -0.1566034107487262\n",
      "Action: [-1.          0.08521162], Reward: 0.07316339808481764\n",
      "Action: [-1.        -0.6078999], Reward: -0.1762222684221429\n",
      "Action: [-1.         -0.48969606], Reward: 0.16765952016007013\n",
      "Action: [-1.  1.], Reward: -0.62587608114858\n",
      "Action: [-1.        -0.8141099], Reward: 0.5019719581934828\n",
      "Action: [-1.         -0.30522949], Reward: 0.0480258427606568\n",
      "Action: [-1.  1.], Reward: -0.16480702780668352\n",
      "Action: [-1.  1.], Reward: -0.4576382360818225\n",
      "Action: [-1.        -0.8179306], Reward: 0.4551841500403951\n",
      "Action: [-1.         0.2865394], Reward: 0.08449947382342593\n",
      "Action: [-1.          0.03313711], Reward: 0.10740969229776276\n",
      "Action: [-1.  1.], Reward: -0.5745094614615194\n",
      "Action: [-0.44185948 -1.        ], Reward: 0.21319248309834762\n",
      "Action: [-1.         -0.47492278], Reward: 0.2349462092002561\n",
      "Action: [-0.65055823  1.        ], Reward: -0.42676163301639103\n",
      "Action: [-1.  1.], Reward: -0.303117333551733\n",
      "Action: [-1.         0.5070998], Reward: 0.14179935266370144\n",
      "Action: [-1.         0.7005713], Reward: -0.009776278388705532\n",
      "Action: [-0.6694238 -1.       ], Reward: 0.24837944530796577\n",
      "Action: [-1.        -0.8587138], Reward: -0.31251643430222115\n",
      "Action: [-1.         -0.00728628], Reward: 0.565285019967046\n",
      "Action: [-1.          0.58901596], Reward: -0.43254272460975346\n",
      "Action: [-1.         -0.44399408], Reward: 0.37789456760159723\n",
      "Action: [-1.  1.], Reward: -0.3136077827549346\n",
      "Action: [-1.  1.], Reward: -0.26804301873627767\n",
      "Action: [-1.        -0.6176805], Reward: 0.3568631580839774\n",
      "Action: [-1. -1.], Reward: -0.4214717829663577\n",
      "Action: [-1.  1.], Reward: 0.19604993089451675\n",
      "Action: [-1. -1.], Reward: 0.030950886052193455\n",
      "Action: [-1.  1.], Reward: -0.08080729143267248\n",
      "Action: [-1.          0.00156346], Reward: 0.29952668326978404\n",
      "Action: [-1. -1.], Reward: -0.5380632491685204\n",
      "Action: [-1.  1.], Reward: 0.21247627319197207\n",
      "Action: [-1.          0.40635806], Reward: 0.21619146895777241\n",
      "Action: [-1.          0.31445223], Reward: 0.03816698182672873\n",
      "Action: [-1.         -0.23217209], Reward: -0.009752326142697498\n",
      "Action: [-1.  1.], Reward: -0.3077753503354106\n",
      "Action: [-1.         0.5806336], Reward: 0.07195174903317643\n",
      "Action: [-1.          0.70910656], Reward: -0.26372634521445026\n",
      "Action: [-1.         0.6194001], Reward: -0.051490286756469744\n",
      "Action: [-1. -1.], Reward: 0.18495794803605328\n",
      "Action: [-1.          0.17512453], Reward: 0.21172536900386874\n",
      "Action: [-1.  1.], Reward: -0.42950817431525823\n",
      "Action: [-1.          0.31426328], Reward: 0.43906138409829865\n",
      "Action: [-1.          0.63003737], Reward: 0.01194439136554223\n",
      "Action: [-1.         0.9642251], Reward: -0.38024993756237796\n",
      "Action: [-1.          0.12892458], Reward: 0.2794336875840919\n",
      "Action: [-1.  1.], Reward: -0.18735593118821534\n",
      "Action: [-1.  1.], Reward: -0.40817459108573817\n",
      "Action: [-1.       -0.504262], Reward: 0.3953212806040662\n",
      "Action: [-1.         0.5495767], Reward: 0.12423527798553323\n",
      "Action: [-1.          0.70708907], Reward: -0.11831637998470967\n",
      "Action: [-1.         0.8989996], Reward: -0.3231458522684065\n",
      "Action: [-1.          0.13601893], Reward: 0.39714046511118184\n",
      "Action: [-1.        -0.8813887], Reward: -0.37949673513284315\n",
      "Action: [-1.  1.], Reward: 0.10135502005297006\n",
      "Action: [-1.          0.25092435], Reward: 0.24841725289829952\n",
      "Action: [-1.  1.], Reward: -0.40044291888496875\n",
      "Action: [-1. -1.], Reward: 0.00012909251296491786\n",
      "Action: [-1.  1.], Reward: 0.04758063198225784\n",
      "Action: [-1.  1.], Reward: -0.3437865117458141\n",
      "Action: [-1.         0.1696896], Reward: 0.5653671574930321\n",
      "Action: [-1.         0.7275338], Reward: -0.13129937346005305\n",
      "Action: [-1.         -0.64001834], Reward: -0.22194756930936235\n",
      "Action: [-1.         -0.16722557], Reward: 0.2738204261701007\n",
      "Action: [-1.          0.71345866], Reward: -0.28510311330142457\n",
      "Action: [-1.         -0.12807676], Reward: 0.30354631118902375\n",
      "Action: [-1.  1.], Reward: -0.20083234545086867\n",
      "Action: [-1.  1.], Reward: -0.5109167551359064\n",
      "Action: [-1.  1.], Reward: -0.5258270917570094\n",
      "Action: [-1.         -0.29212138], Reward: 0.5596952876026133\n",
      "Action: [-0.46375895  0.28519085], Reward: 0.5655100567785194\n",
      "Action: [-0.6238606  -0.24274747], Reward: -0.15239661267369442\n",
      "Action: [-1.         -0.79424185], Reward: -0.36367714959736885\n",
      "Action: [-1.         0.4414118], Reward: 0.5083720226029413\n",
      "Action: [-1. -1.], Reward: -0.3094725192481025\n",
      "Action: [-1.  1.], Reward: -0.12859729545759066\n",
      "Action: [-1.        -0.3753693], Reward: 0.32825472620319474\n",
      "Action: [-1.  1.], Reward: -0.5002125270127464\n",
      "Action: [-1.          0.03740601], Reward: 0.5070453406556119\n",
      "Action: [-1.          0.30851537], Reward: -0.16079452944675943\n",
      "Action: [-1.        -0.9887983], Reward: -0.32110285264061333\n",
      "Action: [-1.         -0.67019486], Reward: -0.12380809724185199\n",
      "Action: [-1.          0.34199944], Reward: 0.5659483292470053\n",
      "Action: [-1. -1.], Reward: -0.21804194730926937\n",
      "Action: [-0.66261387  0.6499765 ], Reward: -0.141252473489564\n",
      "Action: [-1.          0.15568939], Reward: 0.2454723123596665\n",
      "Action: [-1. -1.], Reward: -0.3858676987522205\n",
      "Action: [-1.         0.3374313], Reward: 0.3956252347766025\n",
      "Action: [-1.  1.], Reward: -0.6198484936114717\n",
      "Action: [-1.          0.02595881], Reward: 0.5657131909729394\n",
      "Action: [-1.          0.42805356], Reward: -0.10298679440124126\n",
      "Action: [-1. -1.], Reward: -0.4443721163625145\n",
      "Action: [-0.67556643  1.        ], Reward: 0.18409100800894798\n",
      "Action: [-1.  1.], Reward: -0.3131328705242076\n",
      "Action: [-1.         -0.09243314], Reward: 0.564725204705077\n",
      "Action: [-0.9648609  -0.70354986], Reward: -0.3015016275904111\n",
      "Action: [-1.  1.], Reward: 0.0998890656512377\n",
      "Action: [-1.         -0.14506748], Reward: 0.21002513608491924\n",
      "Action: [-1.         -0.98718774], Reward: -0.37017903179748496\n",
      "Action: [-1.  1.], Reward: -0.11061267269196293\n",
      "Action: [-1.  1.], Reward: -0.3740017146869714\n",
      "Action: [-1.         -0.46430546], Reward: 0.5635873207807833\n",
      "Action: [-1.  1.], Reward: -0.39241331310439587\n",
      "Action: [-1.         0.4704985], Reward: 0.5653988906087406\n",
      "Action: [-0.91896534 -1.        ], Reward: -0.6650603413789813\n",
      "Action: [-1.  1.], Reward: 0.24124270854743848\n",
      "Action: [-1.  1.], Reward: -0.6191251486534675\n",
      "Action: [-1.          0.32518986], Reward: 0.5617311347428924\n",
      "Action: [-1. -1.], Reward: -0.09300928319375565\n",
      "Action: [-1.          0.15727814], Reward: 0.4335695211356789\n",
      "Action: [-1. -1.], Reward: -0.3386136493131555\n",
      "Action: [-1.        -0.0102893], Reward: 0.24640658598184828\n",
      "Action: [-1.         0.9680926], Reward: -0.4612649782799583\n",
      "Action: [-1.         0.4303217], Reward: 0.4843290160154634\n",
      "Action: [-1.  1.], Reward: -0.3136997930100811\n",
      "Action: [-1.         -0.17932892], Reward: 0.21166507415294689\n",
      "Action: [-1.          0.79192084], Reward: -0.09770561542679332\n",
      "Action: [-1.         0.4035703], Reward: 0.02231902227360294\n",
      "Action: [-1.        -0.9950975], Reward: -0.30922773412368865\n",
      "Action: [-1.          0.05078644], Reward: 0.3884936200067486\n",
      "Action: [-1.          0.29127824], Reward: -0.1304088764605833\n",
      "Action: [-1.          0.41659838], Reward: 0.0730103327690621\n",
      "Action: [-1.          0.31107965], Reward: 0.09017765950213708\n",
      "Action: [-1.         -0.09067544], Reward: -0.01922249477341431\n",
      "Action: [-1.         0.4839434], Reward: 0.007568787915971598\n",
      "Action: [-1.        -0.9421835], Reward: -0.5865683117900911\n",
      "Action: [-1. -1.], Reward: -0.29014549860900973\n",
      "Action: [-1. -1.], Reward: -0.3367369369621531\n",
      "Action: [-1. -1.], Reward: -0.5829179217947604\n",
      "Action: [-1.  1.], Reward: 1.6864279466040089\n",
      "Action: [-1.          0.22560269], Reward: -0.12507771550427904\n",
      "Action: [-0.94102323  1.        ], Reward: -0.3098541960505148\n",
      "Action: [-1.         -0.47266853], Reward: 0.3741694558963111\n",
      "Action: [-1.          0.22448383], Reward: -0.11668802703777637\n",
      "Action: [-1.  1.], Reward: -0.22494276372673092\n",
      "Action: [-1.         -0.02366634], Reward: 0.27917008505770724\n",
      "Action: [-1.  1.], Reward: -0.12868363459485962\n",
      "Action: [-1.         -0.08950283], Reward: 0.03732100485710177\n",
      "Action: [-1.          0.11761181], Reward: -0.004379333760196502\n",
      "Action: [-1.          0.10565616], Reward: 0.09978550373644524\n",
      "Action: [-1.          0.83729935], Reward: -0.37964269096225484\n",
      "Action: [-1.         -0.48168004], Reward: 0.33626717404934325\n",
      "Action: [-0.74815524  0.18406716], Reward: -0.10367444753443422\n",
      "Action: [-1.  1.], Reward: -0.3317967955455454\n",
      "Action: [-1.        -0.6025811], Reward: 0.18257728871764078\n",
      "Action: [-1.         -0.15546584], Reward: 0.18368769524425588\n",
      "Action: [-1.        0.995891], Reward: -0.3833461223946928\n",
      "Action: [-1.          0.34090343], Reward: 0.3904402898067598\n",
      "Action: [-1.  1.], Reward: -0.445518475257505\n",
      "Action: [-1.        -0.3655895], Reward: 0.4236630477493524\n",
      "Action: [-1.         -0.71945685], Reward: -0.39735106169380374\n",
      "Action: [-1.         0.8782457], Reward: 0.11798852704482016\n",
      "Action: [-1.  1.], Reward: -0.6019205659659235\n",
      "Action: [-1.         -0.29817212], Reward: 0.5626911490944355\n",
      "Action: [-1.         0.0097525], Reward: 0.17854809652556725\n",
      "Action: [-1.  1.], Reward: -0.2707904682829454\n",
      "Action: [-1.        -0.5259527], Reward: 0.019424875725400314\n",
      "Action: [-1.  1.], Reward: 0.06483096569242022\n",
      "Action: [-1.        -0.2737398], Reward: 0.14709167050945204\n",
      "Action: [-1.          0.83349156], Reward: -0.29992955481651556\n",
      "Action: [-1.         0.6573991], Reward: -0.08654785950925956\n",
      "Action: [-1.        0.938505], Reward: -0.18551626490281406\n",
      "Action: [-1. -1.], Reward: 0.19056497665252017\n",
      "Action: [-1.         -0.28424624], Reward: 0.26822794916237136\n",
      "Action: [-1. -1.], Reward: -0.6128123929856233\n",
      "Action: [-1.  1.], Reward: 0.3158303536471323\n",
      "Action: [-1.         -0.06319065], Reward: 0.21916145060467862\n",
      "Action: [-0.8600811  0.7794786], Reward: -0.1710957714405175\n",
      "Action: [-1. -1.], Reward: -0.3243226747859047\n",
      "Action: [-1.  1.], Reward: 0.1951327383912134\n",
      "Action: [-1.         -0.85849947], Reward: 0.011092139963406283\n",
      "Action: [-1.          0.33837122], Reward: 0.2061984340031131\n",
      "Action: [-1.  1.], Reward: -0.38281141323479306\n",
      "Action: [-1.          0.62524045], Reward: -0.09011592059138394\n",
      "Action: [-1.         -0.09343159], Reward: 0.4368586668222605\n",
      "Action: [-1.        -0.8358321], Reward: -0.3977047124020504\n",
      "Action: [-1.  1.], Reward: -0.02883414599059428\n",
      "Action: [-1.         -0.35417718], Reward: 0.38357099964177266\n",
      "Action: [-1.         -0.50315046], Reward: -0.26656289553870216\n",
      "Action: [-1.         -0.16484192], Reward: 0.24231129238384597\n",
      "Action: [-1.         -0.00624134], Reward: -0.02343920033253788\n",
      "Action: [-1.          0.21118659], Reward: 0.007082537849953496\n",
      "Action: [-1.  1.], Reward: -0.38839964428590146\n",
      "Action: [-1.  1.], Reward: -0.4802538509449181\n",
      "Action: [-1.        -0.5488116], Reward: 0.727122831758884\n",
      "Action: [-1.          0.01603083], Reward: -0.03070706030032966\n",
      "Action: [-1.         -0.00683387], Reward: 0.08809083697891396\n",
      "Action: [-1.          0.72035986], Reward: -0.21460372767673286\n",
      "Action: [-1.         -0.26895753], Reward: 0.18985428092778456\n",
      "Action: [-1.         -0.13602811], Reward: -0.05733334781222865\n",
      "Action: [-1.  1.], Reward: -0.17841315697956148\n",
      "Action: [-1.         -0.59179896], Reward: -0.051386441326424115\n",
      "Action: [-1.  1.], Reward: -0.021919131694004362\n",
      "Action: [-1.         0.5454835], Reward: 0.0684404673217621\n",
      "Action: [-1.          0.81533766], Reward: -0.20080624191044755\n",
      "Action: [-1.         0.6163886], Reward: 0.053080432949655074\n",
      "Action: [-1.         -0.14911422], Reward: 0.20214223353382366\n",
      "Action: [-1.         -0.46484107], Reward: -0.05926332135108936\n",
      "Action: [-1.         0.6616988], Reward: 0.011050666906958214\n",
      "Action: [-1.          0.58962095], Reward: -0.15029544836187383\n",
      "Action: [-1.  1.], Reward: -0.5237658454579861\n",
      "Action: [-1.         -0.90364814], Reward: 0.5357722363052364\n",
      "Action: [-1.         0.9565573], Reward: -0.217088470652604\n",
      "Action: [-1.          0.31553143], Reward: 0.29637281186201214\n",
      "Action: [-1.  1.], Reward: -0.36378293809046514\n",
      "Action: [-1.  1.], Reward: -0.2101296820126943\n",
      "Action: [-1.          0.02427088], Reward: 0.5654516540649404\n",
      "Action: [-1.  1.], Reward: -0.41933763521775336\n",
      "Action: [-1.         -0.17640173], Reward: 0.3371991574871309\n",
      "Action: [-1.         0.3089649], Reward: -0.10413941439019325\n",
      "Action: [-1.         -0.54641384], Reward: -0.11119726120962148\n",
      "Action: [-1.        0.290433], Reward: 0.21202821499720592\n",
      "Action: [-1.          0.76515377], Reward: -0.36364730848927707\n",
      "Action: [-1.       -0.546217], Reward: 0.2070668441203641\n",
      "Action: [-1.          0.03261513], Reward: 0.07857523392818777\n",
      "Action: [-1.         -0.86471456], Reward: -0.24771113693243207\n",
      "Action: [-1.         -0.36331576], Reward: 0.22929128857371595\n",
      "Action: [-1.         -0.18430193], Reward: -0.07776417377074374\n",
      "Action: [-1. -1.], Reward: -0.114187125694827\n",
      "Action: [-1.  1.], Reward: -0.2557146640090522\n",
      "Action: [-1.  1.], Reward: -0.3717964219062331\n",
      "Action: [-1.         -0.54274017], Reward: 0.6455960519559905\n",
      "Action: [-1.         -0.23682757], Reward: 0.01334846711922566\n",
      "Action: [-1.         -0.03141336], Reward: 0.09490914168381615\n",
      "Action: [-1.         -0.06378864], Reward: 0.02282527583007088\n",
      "Action: [-1.         0.7785529], Reward: -0.3004876833126815\n",
      "Action: [-1.        -0.9278414], Reward: -0.18273183607534121\n",
      "Action: [-1.  1.], Reward: 0.11044108296513791\n",
      "Action: [-1. -1.], Reward: -0.11798803925640364\n",
      "Action: [-1.         -0.06854358], Reward: 0.3434064904450089\n",
      "Action: [-1.        -0.6052775], Reward: -0.049698097113854374\n",
      "Action: [-1.        -0.7066703], Reward: -0.18546431314088813\n",
      "Action: [-1.         0.5877067], Reward: -0.12440578026973931\n",
      "Action: [-1. -1.], Reward: 0.10904253909544195\n",
      "Action: [-1.         -0.64046204], Reward: -0.2043624554752043\n",
      "Action: [-1.         -0.46987286], Reward: 0.3543615699834177\n",
      "Action: [-1.          0.07844141], Reward: -0.11937549165437922\n",
      "Action: [-1.  1.], Reward: -0.23406006998273618\n",
      "Action: [-1.  1.], Reward: -0.6806111428596242\n",
      "Action: [-1.        0.531837], Reward: 0.1392412949303359\n",
      "Action: [-1.         0.3658716], Reward: 0.5628319447374892\n",
      "Action: [-1.         0.8490225], Reward: -0.1058760722415191\n",
      "Action: [-1.         -0.03153987], Reward: 0.2824625930983693\n",
      "Action: [-1.       -0.271181], Reward: -0.08606806118388022\n",
      "Action: [-1.          0.15455416], Reward: 0.083747366378919\n",
      "Action: [-1.          0.32915187], Reward: 0.08272307246067534\n",
      "Action: [-1.         -0.58913606], Reward: -0.1986423381718749\n",
      "Action: [-1.         0.6784325], Reward: -0.11499095195436951\n",
      "Action: [-1.         -0.42394805], Reward: 0.215310546169718\n",
      "Action: [-1.         0.7998896], Reward: -0.1692803273637405\n",
      "Action: [-1.          0.09029531], Reward: 0.0685569445678702\n",
      "Action: [-1.  1.], Reward: -0.27925914810941666\n",
      "Action: [-1.  1.], Reward: -0.6803931710938922\n",
      "Action: [-0.8399805  1.       ], Reward: -0.6709179590541854\n",
      "Action: [-1.         0.6189618], Reward: -0.152847367449684\n",
      "Action: [-1.  1.], Reward: -0.4758415450634945\n",
      "Action: [-1.        -0.7325615], Reward: 1.0678013526488228\n",
      "Action: [-1.         -0.88396716], Reward: 0.9419929075208172\n",
      "Action: [-1.  1.], Reward: -0.16542894295964736\n",
      "Action: [-1.         -0.66722846], Reward: -0.11732355012169216\n",
      "Action: [-1.          0.14524138], Reward: 0.2857065462416344\n",
      "Action: [-1. -1.], Reward: -0.45902407395034106\n",
      "Action: [-1.         0.6226567], Reward: 0.23263023110414033\n",
      "Action: [-1.         -0.47820377], Reward: 0.14448277981817093\n",
      "Action: [-1. -1.], Reward: -0.1994210425258511\n",
      "Action: [-0.99137235  0.4630771 ], Reward: 0.2275306830479238\n",
      "Action: [-0.9863298  0.5002603], Reward: -0.3099387142036605\n",
      "Action: [-0.50412107  0.7516254 ], Reward: -0.016196965143353947\n",
      "Action: [-1.  1.], Reward: -0.2179801437817452\n",
      "Action: [-1.  1.], Reward: -0.46753100853946594\n",
      "Action: [-1.          0.21375252], Reward: 0.5620086521470782\n",
      "Action: [-1.         -0.44064867], Reward: 0.3085043188641663\n",
      "Action: [-1. -1.], Reward: -0.4403801230758677\n",
      "Action: [-1.          0.97303975], Reward: 0.012397944150322217\n",
      "Action: [-1.  1.], Reward: -0.23325370644621898\n",
      "Action: [-1.         -0.23664373], Reward: 0.5650251280816612\n",
      "Action: [-1.       -0.801769], Reward: -0.40231349930811594\n",
      "Action: [-1. -1.], Reward: -0.4069212250414178\n",
      "Action: [-1.        -0.5698886], Reward: -0.03996264498685996\n",
      "Action: [-1.  1.], Reward: 0.5672219631970503\n",
      "Action: [-1.          0.19565906], Reward: 0.17946307973317843\n",
      "Action: [-1. -1.], Reward: -0.4326992948275421\n",
      "Action: [-1.  1.], Reward: 0.14370906695781052\n",
      "Action: [-1.  1.], Reward: -0.4262438001492448\n",
      "Action: [-1.         0.3205852], Reward: 0.5641867724603387\n",
      "Action: [-1.         -0.07494165], Reward: 0.003382979208939929\n",
      "Action: [-1.  1.], Reward: -0.14616967221816488\n",
      "Action: [-1.          0.27498642], Reward: 0.13975615923899376\n",
      "Action: [-1.          0.00760724], Reward: -0.038527752253717296\n",
      "Action: [-1.         0.2381122], Reward: 0.09387425288523588\n",
      "Action: [-1.  1.], Reward: -0.3294965433617001\n",
      "Action: [-1.  1.], Reward: -0.4359996107483084\n",
      "Action: [-1.          0.33064488], Reward: 0.5636864739830898\n",
      "Action: [-1.          0.14551193], Reward: 0.07092159127517661\n",
      "Action: [-1.  1.], Reward: -0.2559028231112783\n",
      "Action: [-1.          0.00663622], Reward: 0.2642867807716076\n",
      "Action: [0.35325193 0.32033044], Reward: -0.4243716478333539\n",
      "Action: [-1.          0.60210514], Reward: -0.1848410866336964\n",
      "Action: [-1.          0.42059636], Reward: 0.4315673742684467\n",
      "Action: [-1.  1.], Reward: -0.37467412069826156\n",
      "Action: [-0.7761649  1.       ], Reward: -0.42581371918025\n",
      "Action: [-1. -1.], Reward: 0.5051095690821381\n",
      "Action: [-1.  1.], Reward: 0.010701845222422152\n",
      "Action: [-1.  1.], Reward: -0.5600640924633822\n",
      "Action: [-1.         -0.39430323], Reward: 0.5640181679676814\n",
      "Action: [-1.  1.], Reward: -0.6513857662587383\n",
      "Action: [-1. -1.], Reward: 0.4038909801474564\n",
      "Action: [-1.          0.88225883], Reward: 0.06776496203125305\n",
      "Action: [-1.          0.39220068], Reward: 0.19223547569347232\n",
      "Action: [-1.        -0.4438703], Reward: -0.05876912647311805\n",
      "Action: [-1.        -0.6786759], Reward: -0.19762874164282052\n",
      "Action: [-1.         0.2602978], Reward: 0.20751745612208872\n",
      "Action: [-1. -1.], Reward: -0.4422899522060024\n",
      "Action: [-1.        -0.7566501], Reward: -0.10963839330165243\n",
      "Action: [-1.          0.75557876], Reward: 0.3129668992817761\n",
      "Action: [-1.         0.6488354], Reward: -0.16785515590424524\n",
      "Action: [-1.  1.], Reward: -0.42274198316391387\n",
      "Action: [-1.          0.47632825], Reward: 0.5634621765562411\n",
      "Action: [-1.  1.], Reward: -0.2767091224446847\n",
      "Action: [-1.  1.], Reward: -0.387075448255074\n",
      "Action: [-1. -1.], Reward: 0.5536378937458484\n",
      "Action: [-1.          0.03590806], Reward: 0.11880404534212374\n",
      "Action: [-1.  1.], Reward: -0.2675239017220139\n",
      "Action: [-1.        -0.7596704], Reward: -0.03481443539860123\n",
      "Action: [-1.         0.1712174], Reward: 0.2942412472643845\n",
      "Action: [-1.         -0.21470486], Reward: -0.011751831931071877\n",
      "Action: [-1.          0.31643638], Reward: 0.003063010083636186\n",
      "Action: [-1.          0.05787875], Reward: 0.047936225635812235\n",
      "Action: [-1.  1.], Reward: -0.6442171389586402\n",
      "Action: [-1.        -0.5036013], Reward: 0.4601169735552768\n",
      "Action: [-1.       -0.531814], Reward: -0.20837791922240123\n",
      "Action: [-1.         -0.87878084], Reward: -0.30394501096182963\n",
      "Action: [-1.        -0.5254103], Reward: 0.06525462628510482\n",
      "Action: [-1.          0.66203654], Reward: 0.27682589251067424\n",
      "Action: [-1.  1.], Reward: -0.2925095206596342\n",
      "Action: [-1.  1.], Reward: -0.6187213403792178\n",
      "Action: [-1.          0.35244793], Reward: 0.5601523563133259\n",
      "Action: [-1.         -0.68743175], Reward: 0.25578633070387125\n",
      "Action: [-1.          0.65718585], Reward: 0.1307046631999373\n",
      "Action: [-1.         0.8756153], Reward: -0.5005716447445616\n",
      "Action: [-1.         -0.14817622], Reward: 0.5652214901961141\n",
      "Action: [-1.          0.74903286], Reward: -0.06289022756588303\n",
      "Action: [-1.          0.41504204], Reward: -0.12283181729632453\n",
      "Action: [-1.          0.48175097], Reward: 0.04875492193191633\n",
      "Action: [-1.        -0.9452239], Reward: -0.2961224087263314\n",
      "Action: [-1.         0.1828479], Reward: 0.3492914170799537\n",
      "Action: [-1.  1.], Reward: -0.4586127584054853\n",
      "Action: [-1.         -0.01893447], Reward: 0.46113624267560027\n",
      "Action: [-1.  1.], Reward: -0.5681736654084257\n",
      "Action: [-1.         -0.01902738], Reward: 0.5381253853853281\n",
      "Action: [-1.        -0.1648881], Reward: -0.1711213435093093\n",
      "Action: [-1. -1.], Reward: -0.45490799208716726\n",
      "Action: [-1.  1.], Reward: 0.1987851812423547\n",
      "Action: [-1. -1.], Reward: 0.044401943414706296\n",
      "Action: [-1. -1.], Reward: -0.5678668533373983\n",
      "Action: [-1.  1.], Reward: 0.5631648766691761\n",
      "Action: [-1.          0.20272806], Reward: 0.16302869569801004\n",
      "Action: [-1.         0.6626654], Reward: -0.14739580188574444\n",
      "Action: [-1.  1.], Reward: -0.2439518223182435\n",
      "Action: [-1.  1.], Reward: -0.5588284149673413\n",
      "Action: [-1.        -0.7505928], Reward: 0.7532159980888414\n",
      "Action: [-1.        -0.5910073], Reward: -0.17213279828557354\n",
      "Action: [-1.         -0.25820208], Reward: 0.2838016668639476\n",
      "Action: [-1.  1.], Reward: -0.5598683780847107\n",
      "Action: [-1.          0.21271764], Reward: 0.5660645656860108\n",
      "Action: [-1. -1.], Reward: -0.46918104865771926\n",
      "Action: [-1.          0.26982346], Reward: 0.42827098637083516\n",
      "Action: [-1.          0.04587775], Reward: -0.10776558441759931\n",
      "Action: [-1.         -0.22098364], Reward: 0.034505159475029856\n",
      "Action: [-1. -1.], Reward: -0.28332247964423796\n",
      "Action: [-1.          0.17278808], Reward: 0.2707649708046098\n",
      "Action: [-1.         -0.01630343], Reward: -0.0915718304487676\n",
      "Action: [-1. -1.], Reward: -0.198187777311406\n",
      "Action: [ 0.15532708 -0.92512363], Reward: -1.830463653320032\n",
      "Action: [-1.  1.], Reward: 1.4456714084461961\n",
      "Action: [-1.          0.75150055], Reward: 0.20475636068881628\n",
      "Action: [-1.          0.86262256], Reward: -0.1011115502280294\n",
      "Action: [-1.  1.], Reward: -0.5013363673267512\n",
      "Action: [-1.          0.42205286], Reward: 0.5631981690521357\n",
      "Action: [-1.        -0.8965826], Reward: -0.11935132352029428\n",
      "Action: [-1.          0.02772549], Reward: 0.29604164270924116\n",
      "Action: [-1.         -0.39572126], Reward: -0.018604132656054873\n",
      "Action: [-1. -1.], Reward: -0.23439445281389545\n",
      "Action: [-1.  1.], Reward: -0.13271000721676593\n",
      "Action: [-1. -1.], Reward: 0.14661216078729858\n",
      "Action: [-1. -1.], Reward: -0.4578227981648595\n",
      "Action: [-1.          0.16489643], Reward: 0.5661947115955641\n",
      "Action: [-1.         0.8352488], Reward: -0.4515522255796345\n",
      "Action: [-1.        -0.5379845], Reward: 0.28678358957430256\n",
      "Action: [-1.  1.], Reward: -0.06049417578619362\n",
      "Action: [-1. -1.], Reward: -0.23831305393507904\n",
      "Action: [-1.         0.6299273], Reward: 0.22115351059328966\n",
      "Action: [-1.          0.21511717], Reward: 0.1278175314788026\n",
      "Action: [-1. -1.], Reward: -0.28417441725148573\n",
      "Action: [-1.         -0.47907168], Reward: 0.2524654853804673\n",
      "Action: [-1. -1.], Reward: -0.13937203782911986\n",
      "Action: [-1.  1.], Reward: -0.37374101143466265\n",
      "Action: [-1.         -0.49732822], Reward: 0.5004072902111645\n",
      "Action: [-1.  1.], Reward: -0.5001165946626809\n",
      "Action: [-1. -1.], Reward: 0.160250361421473\n",
      "Action: [-1.          0.10174702], Reward: 0.20868524876119032\n",
      "Action: [-1.          0.38585356], Reward: 0.050140380407246\n",
      "Action: [-1.         -0.07779522], Reward: -0.01721817553760907\n",
      "Action: [-1.  1.], Reward: -0.6341634361688235\n",
      "Action: [-1.         0.8622657], Reward: -0.41443901768350827\n",
      "Action: [-1.  1.], Reward: -0.4944852277948957\n",
      "Action: [-1.  1.], Reward: -0.6155960965586871\n",
      "Action: [-1.         0.9033808], Reward: -0.22581119469239647\n",
      "Action: [-1. -1.], Reward: 1.4607825060528257\n",
      "Action: [-1.         -0.71035224], Reward: 0.4886250570242622\n",
      "Action: [-1.          0.20031898], Reward: 0.14013621871338167\n",
      "Action: [-1.          0.38549784], Reward: 0.06156450578899353\n",
      "Action: [-1.          0.40308553], Reward: -0.02080860215273006\n",
      "Action: [-1.        0.852247], Reward: -0.2762945378261494\n",
      "Action: [-1.         -0.05065751], Reward: 0.20183419892579124\n",
      "Action: [-1.         -0.09659046], Reward: -0.05987117421272181\n",
      "Action: [-1.        -0.0510359], Reward: 0.08024970412771104\n",
      "Action: [-1.  1.], Reward: -0.26935748508962365\n",
      "Action: [-1.  1.], Reward: -0.6496951801178212\n",
      "Action: [-1.        -0.6918001], Reward: 0.7852780308004345\n",
      "Action: [-1.        -0.0801643], Reward: -0.03500766612416584\n",
      "Action: [-1.         -0.34830278], Reward: 0.08764616557362714\n",
      "Action: [-1.         0.7514051], Reward: -0.195938624595292\n",
      "Action: [-1.         0.4988399], Reward: 0.1782667561449287\n",
      "Action: [-1.          0.14173982], Reward: -0.05396395057623948\n",
      "Action: [-1.  1.], Reward: -0.4417675446640217\n",
      "Action: [-1.  1.], Reward: -0.6576743827531011\n",
      "Action: [-1.        -0.5092636], Reward: 1.112733433626497\n",
      "Action: [-1.        -0.5659789], Reward: -0.4059768598088027\n",
      "Action: [-1.          0.06041358], Reward: 0.3756485124119049\n",
      "Action: [-1.          0.59511197], Reward: -0.27245018575847824\n",
      "Action: [-1.          0.05611889], Reward: 0.24698628712303083\n",
      "Action: [-1.          0.44818437], Reward: -0.04327063583535917\n",
      "Action: [-1.          0.56005186], Reward: -0.12265321617048841\n",
      "Action: [-1.        -0.8603628], Reward: -0.22773943020085113\n",
      "Action: [-1. -1.], Reward: -0.4513379535802382\n",
      "Action: [-1.          0.16510025], Reward: 0.5679476751575159\n",
      "Action: [-1.          0.13712053], Reward: 0.14311669518500736\n",
      "Action: [-1.  1.], Reward: -0.37054651579986486\n",
      "Action: [ 0.24894929 -0.04445975], Reward: -0.8373924799758108\n",
      "Action: [-1.  1.], Reward: -0.5405810299328848\n",
      "Action: [-1.         -0.62650925], Reward: 1.2766117518252345\n",
      "Action: [-1.        -0.7486495], Reward: -0.10177807624390639\n",
      "Action: [-1.  1.], Reward: -0.01793413945618269\n",
      "Action: [-1.  1.], Reward: -0.34580384833436884\n",
      "Action: [-1.         -0.62169063], Reward: 0.42343571033494337\n",
      "Action: [-1.        -0.5737561], Reward: -0.09732780431759522\n",
      "Action: [-1.          0.21508129], Reward: 0.23584430259894518\n",
      "Action: [-1.          0.32764035], Reward: -0.07958058601417761\n",
      "Action: [-1.  1.], Reward: -0.5289034509531063\n",
      "Action: [-1.         -0.34759122], Reward: 0.5642685412767001\n",
      "Action: [-1.          0.56060624], Reward: 0.036120783750656714\n",
      "Action: [-1.         -0.57444966], Reward: -0.38085590573955197\n",
      "Action: [-1.         -0.49314308], Reward: 0.341461971164005\n",
      "Action: [-1. -1.], Reward: -0.41282643246800066\n",
      "Action: [-1. -1.], Reward: -0.4433861821170886\n",
      "Action: [-1.         0.3077399], Reward: 0.5684842704108046\n",
      "Action: [-1.          0.47583663], Reward: 0.1939539498486731\n",
      "Action: [-1.          0.34199387], Reward: -0.06609270186111793\n",
      "Action: [-1.  1.], Reward: -0.2495229640287053\n",
      "Action: [-1.  1.], Reward: -0.4306988016472608\n",
      "Action: [-1.         0.2296541], Reward: 0.563738409471048\n",
      "Action: [-1.          0.50934184], Reward: -0.05836774114233819\n",
      "Action: [-1.  1.], Reward: -0.28135714658310174\n",
      "Action: [-1.         -0.59475243], Reward: 0.19134722581696656\n",
      "Action: [-1.       -0.292471], Reward: 0.1878091471775587\n",
      "Action: [-1.          0.02372904], Reward: 0.06231815816741104\n",
      "Action: [-1.         -0.17950106], Reward: -0.021195146051864455\n",
      "Action: [-1.         -0.32604286], Reward: 0.026648384456345653\n",
      "Action: [-1. -1.], Reward: -0.344259060394992\n",
      "Action: [-1.          0.39136893], Reward: 0.2567590779417248\n",
      "Action: [-1.  1.], Reward: -0.5636988336788213\n",
      "Action: [-1.          0.90761465], Reward: -0.5245604165439561\n",
      "Action: [-1.         -0.41931254], Reward: 0.5604126214608964\n",
      "Action: [-1.         -0.35251588], Reward: 0.5171958588284244\n",
      "Action: [-1.          0.39237678], Reward: -0.1642047728320417\n",
      "Action: [-1.        -0.5785705], Reward: -0.1647238699269876\n",
      "Action: [-1.  1.], Reward: -0.19621471318442715\n",
      "Action: [-1. -1.], Reward: 0.13097175807301412\n",
      "Action: [-1.  1.], Reward: -0.06544237787671278\n",
      "Action: [-1.         0.9912646], Reward: -0.29439001341011667\n",
      "Action: [-1.  1.], Reward: -0.6058066913185491\n",
      "Action: [-1.          0.47944146], Reward: 0.5595160865429278\n",
      "Action: [-1.          0.17416814], Reward: 0.5651529155409314\n",
      "Action: [-1.         0.6270924], Reward: -0.15828174743410522\n",
      "Action: [-1.  1.], Reward: -0.34526402846608906\n",
      "Action: [-1.          0.44256318], Reward: 0.4779665083198328\n",
      "Action: [-1.         -0.00909474], Reward: -0.15098060993353535\n",
      "Action: [-1.          0.04625681], Reward: 0.07408156464719973\n",
      "Action: [-1. -1.], Reward: -0.5034356106407525\n",
      "Action: [-1.         0.7932454], Reward: 0.33271366320833445\n",
      "Action: [-1.          0.47556525], Reward: 0.11377695335572646\n",
      "Action: [-1.         -0.18425445], Reward: 0.10848096618424563\n",
      "Action: [-1.  1.], Reward: -0.2577420574017004\n",
      "Action: [-1.         -0.70218325], Reward: -0.18199644553033031\n",
      "Action: [-1.         0.8213243], Reward: 0.16518508984007135\n",
      "Action: [-1.         -0.02044591], Reward: 0.19105720628985345\n",
      "Action: [-1.         0.8553451], Reward: -0.18511049605061317\n",
      "Action: [-1.         0.3318301], Reward: 0.04774756712500405\n",
      "Action: [-1.          0.33769163], Reward: -0.008781001646786635\n",
      "Action: [-1.        -0.1829619], Reward: 0.08584876812761344\n",
      "Action: [-1.          0.17228734], Reward: 0.03421154485888778\n",
      "Action: [-1.        -0.8719771], Reward: -0.3996504090744574\n",
      "Action: [-1.         0.2838337], Reward: 0.3219531978616992\n",
      "Action: [-1.          0.67378867], Reward: -0.3311662112776245\n",
      "Action: [-1.  1.], Reward: -0.5502260725580725\n",
      "Action: [-1.          0.35399413], Reward: 0.5623562232707329\n",
      "Action: [-1.        -0.3764093], Reward: 0.23528243505753466\n",
      "Action: [-1.         0.5810124], Reward: 0.03437174542283266\n",
      "Action: [-1. -1.], Reward: -0.5968697986108562\n",
      "Action: [-1.         -0.15492469], Reward: 0.5669333277085027\n",
      "Action: [-1.         -0.76251787], Reward: -0.20732952037493377\n",
      "Action: [-1.          0.76610094], Reward: -0.2029026229200521\n",
      "Action: [-1.        -0.4599315], Reward: 0.3402680332172281\n",
      "Action: [-1.        -0.6710114], Reward: -0.32561432602752993\n",
      "Action: [-1.  1.], Reward: -0.048129039552562175\n",
      "Action: [-1.  1.], Reward: -0.20726593560975878\n",
      "Action: [-1.        -0.5721826], Reward: 0.32680800112550523\n",
      "Action: [-1. -1.], Reward: -0.5770514107331912\n",
      "Action: [-1.         -0.19242519], Reward: 0.5670260040626616\n",
      "Action: [-1.  1.], Reward: -0.37396374091812734\n",
      "Action: [-0.21137571  0.8396295 ], Reward: -0.4404442822278213\n",
      "Action: [-1.        -0.8808551], Reward: 0.664248973469682\n",
      "Action: [-1. -1.], Reward: -0.2016526154078224\n",
      "Action: [-1.        -0.9481537], Reward: -0.1689045397367221\n",
      "Action: [-0.02929688 -1.        ], Reward: -0.6260348655204442\n",
      "Action: [-1. -1.], Reward: -0.3697851709904316\n",
      "Action: [-1.        -0.7552533], Reward: -0.20307686258021476\n",
      "Action: [-1.  1.], Reward: 1.3382436556683832\n",
      "Action: [-1.          0.35054094], Reward: 0.3330353700139419\n",
      "Action: [-1. -1.], Reward: -0.18658202374481078\n",
      "Action: [-1.        -0.8765873], Reward: -0.3001659096238023\n",
      "Action: [-1. -1.], Reward: -0.19449157994132335\n",
      "Action: [-1.         0.9971864], Reward: 0.31923583038039366\n",
      "Action: [-1.         -0.48142993], Reward: 0.2077925942164942\n",
      "Action: [-1. -1.], Reward: -0.2485855002389765\n",
      "Action: [-1.         0.3377337], Reward: 0.26925675585916053\n",
      "Action: [-1.         -0.45693326], Reward: -0.09050728677003939\n",
      "Action: [-1.         0.1413503], Reward: 0.05483052973271363\n",
      "Action: [-1.        -0.9570587], Reward: -0.20720998698440554\n",
      "Action: [-1.          0.88077635], Reward: -0.1839269471346867\n",
      "Action: [-1.         0.5785203], Reward: 0.0803357557492184\n",
      "Action: [-1.  1.], Reward: -0.5150578470752538\n",
      "Action: [-1.  1.], Reward: -0.23046135837192658\n",
      "Action: [-1.  1.], Reward: -0.5692335258007908\n",
      "Action: [-1.         -0.52236634], Reward: 1.0959709265426805\n",
      "Action: [-1.        0.586204], Reward: -0.0333340785727237\n",
      "Action: [-1.          0.15509051], Reward: 0.32847226626024906\n",
      "Action: [-1.         -0.33405754], Reward: -0.10134668468696262\n",
      "Action: [-1.  1.], Reward: -0.27688433259608547\n",
      "Action: [-1.         -0.74422413], Reward: 0.035541787868075686\n",
      "Action: [-1.        0.481496], Reward: 0.29492339520296085\n",
      "Action: [-1.         -0.09491728], Reward: -0.005580395919684733\n",
      "Action: [-1.         0.5199673], Reward: -0.19242550811547837\n",
      "Action: [-1.  1.], Reward: -0.4451253696490667\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "env = dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "episode = dataset[110]\n",
    "\n",
    "obs, _ = env.reset(seed=42)\n",
    "for action in episode.actions:\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {rew}\")\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executing the script, the dataset will be saved on your disk. You can display the list of datasets with ``minari list local`` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral cloning with PyTorch\n",
    "Now we can use PyTorch to learn the policy from the offline dataset.\n",
    "Let's define the policy network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the output dimension will be two, as previously mentioned. As for the input dimension, it will be four, corresponding to the observation space of ``CartPole-v1``.\n",
    "Our next step is to load the dataset and set up the training loop. The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "However, since each episode can have a varying length, we need to pad them.\n",
    "To achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        # \"seed\": torch.Tensor([x.seed for x in batch]),\n",
    "        # \"total_timesteps\": torch.Tensor([x.total_timesteps for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to load the data and create the training loop.\n",
    "To begin, let's initialize the DataLoader, neural network, optimizer, and loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m action_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# assert isinstance(observation_space, spaces.Box)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# assert isinstance(action_space, spaces.Discrete)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m PolicyNetwork(np\u001b[38;5;241m.\u001b[39mprod(observation_space\u001b[38;5;241m.\u001b[39mshape), \u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(policy_net\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     12\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "minari_dataset = minari.load_dataset(\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "env = minari_dataset.recover_environment()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "# assert isinstance(observation_space, spaces.Box)\n",
    "# assert isinstance(action_space, spaces.Discrete)\n",
    "\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = minari_dataset[0]\n",
    "print(episode)\n",
    "print(episode.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy loss like a classic classification task, as the action space is discrete.\n",
    "We then train the policy to predict the actions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        a_pred = policy_net(batch['observations'][:, :-1])\n",
    "        a_hat = F.one_hot(batch[\"actions\"]).type(torch.float32)\n",
    "        loss = loss_fn(a_pred, a_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can evaluate if the policy learned from the expert!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "done = False\n",
    "accumulated_rew = 0\n",
    "while not done:\n",
    "    action = policy_net(torch.Tensor(obs)).argmax()\n",
    "    obs, rew, ter, tru, _ = env.step(action.numpy())\n",
    "    done = ter or tru\n",
    "    accumulated_rew += rew\n",
    "\n",
    "env.close()\n",
    "print(\"Accumulated rew: \", accumulated_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually observe that the learned policy aces this simple control task, and we get the maximum reward 500, as the episode is truncated after 500 steps.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
