{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Behavioral cloning with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here how to perform behavioral cloning on a Minari dataset using [PyTorch](https://pytorch.org/).\n",
    "We will start generating the dataset of the expert policy for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, which is a classic control problem.\n",
    "The objective is to balance the pole on the cart, and we receive a reward of +1 for each successful timestep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "For this tutorial you will need the [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) library, which you can install with `pip install rl_zoo3`.\n",
    "Let's then import all the required packages and set the random seed for reproducibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117438e70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import minari\n",
    "from minari import DataCollector\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy training\n",
    "Now we can train the expert policy using RL Baselines3 Zoo.\n",
    "We train a PPO agent on the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: python [-h]\n",
      "              [--algo {a2c,ddpg,dqn,ppo,sac,td3,ars,crossq,qrdqn,tqc,trpo,ppo_lstm}]\n",
      "              [--env ENV] [-tb TENSORBOARD_LOG] [-i TRAINED_AGENT]\n",
      "              [--truncate-last-trajectory TRUNCATE_LAST_TRAJECTORY]\n",
      "              [-n N_TIMESTEPS] [--num-threads NUM_THREADS]\n",
      "              [--log-interval LOG_INTERVAL] [--eval-freq EVAL_FREQ]\n",
      "              [--optimization-log-path OPTIMIZATION_LOG_PATH]\n",
      "              [--eval-episodes EVAL_EPISODES] [--n-eval-envs N_EVAL_ENVS]\n",
      "              [--save-freq SAVE_FREQ] [--save-replay-buffer] [-f LOG_FOLDER]\n",
      "              [--seed SEED] [--vec-env {dummy,subproc}] [--device DEVICE]\n",
      "              [--n-trials N_TRIALS] [--max-total-trials MAX_TOTAL_TRIALS]\n",
      "              [-optimize] [--no-optim-plots] [--n-jobs N_JOBS]\n",
      "              [--sampler {random,tpe,auto}] [--pruner {halving,median,none}]\n",
      "              [--n-startup-trials N_STARTUP_TRIALS]\n",
      "              [--n-evaluations N_EVALUATIONS] [--storage STORAGE]\n",
      "              [--study-name STUDY_NAME] [--trial-id TRIAL_ID]\n",
      "              [--verbose VERBOSE]\n",
      "              [--gym-packages GYM_PACKAGES [GYM_PACKAGES ...]]\n",
      "              [--env-kwargs ENV_KWARGS [ENV_KWARGS ...]]\n",
      "              [--eval-env-kwargs EVAL_ENV_KWARGS [EVAL_ENV_KWARGS ...]]\n",
      "              [-params HYPERPARAMS [HYPERPARAMS ...]] [-conf CONF_FILE]\n",
      "              [-uuid] [--track] [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "              [--wandb-entity WANDB_ENTITY] [-P]\n",
      "              [-tags WANDB_TAGS [WANDB_TAGS ...]]\n",
      "python: error: unrecognized arguments: -m rl_zoo3.record_training\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "sys.argv = [\n",
    "    \"python\", \n",
    "    \"--algo\", \"ppo\",\n",
    "    \"--env\", \"LunarLanderContinuous-v3\",\n",
    "    \"--n-timesteps\", \"1000000\",\n",
    "    \"--track\",\n",
    "    \"--wandb-project-name\", \"FRL\",\n",
    "    \"--wandb-entity\", \"frankcholula\",\n",
    "    \"--tensorboard-log\", \"runs\",\n",
    "    \"--hyperparams\",\n",
    "    \"n_envs:16\",\n",
    "    \"n_steps:1024\",\n",
    "    \"batch_size:64\",\n",
    "    \"n_epochs:4\",\n",
    "    \"gamma:0.999\",\n",
    "    \"gae_lambda:0.98\",\n",
    "    \"ent_coef:0.01\"\n",
    "]\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a new folder named `log` with the expert policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "Now let's generate the dataset using the [DataCollector](https://minari.farama.org/api/data_collector/) wrapper:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:17<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "env = DataCollector(gym.make(\"LunarLander-v3\", continuous=True))\n",
    "path = os.path.abspath('') + '/logs/ppo/LunarLanderContinuous-v3_1/best_model.zip'\n",
    "agent = PPO.load(path)\n",
    "\n",
    "total_episodes = 1_000\n",
    "for i in tqdm(range(total_episodes)):\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    while True:\n",
    "        action, _ = agent.predict(obs)\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PatchedTimeLimit' object has no attribute 'create_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m(\n\u001b[1;32m      2\u001b[0m     dataset_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLanderContinuous-v3/ppo-1000-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     algorithm_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     code_permalink\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/frankcholula/FRL-playground/blob/main/code/behavioral_cloning.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     author\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrank Lu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     author_email\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlu.phrank@gmail.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBehavioral cloning dataset for LunarLanderContinuous-v3 using PPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     eval_env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLanderContinuous-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PatchedTimeLimit' object has no attribute 'create_dataset'"
     ]
    }
   ],
   "source": [
    "dataset = env.create_dataset(\n",
    "    dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\",\n",
    "    algorithm_name=\"ppo\",\n",
    "    code_permalink=\"https://github.com/frankcholula/FRL-playground/blob/main/code/behavioral_cloning.py\",\n",
    "    author=\"Frank Lu\",\n",
    "    author_email=\"lu.phrank@gmail.com\",\n",
    "    description=\"Behavioral cloning dataset for LunarLanderContinuous-v3 using PPO\",\n",
    "    eval_env=\"LunarLanderContinuous-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "env = dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "episode = dataset[105]\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for action in episode.actions:\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executing the script, the dataset will be saved on your disk. You can display the list of datasets with ``minari list local`` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral cloning with PyTorch\n",
    "Now we can use PyTorch to learn the policy from the offline dataset.\n",
    "Let's define the policy network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the output dimension will be two, as previously mentioned. As for the input dimension, it will be four, corresponding to the observation space of ``CartPole-v1``.\n",
    "Our next step is to load the dataset and set up the training loop. The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "However, since each episode can have a varying length, we need to pad them.\n",
    "To achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        # \"seed\": torch.Tensor([x.seed for x in batch]),\n",
    "        # \"total_timesteps\": torch.Tensor([x.total_timesteps for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to load the data and create the training loop.\n",
    "To begin, let's initialize the DataLoader, neural network, optimizer, and loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "minari_dataset = minari.load_dataset(\"CartPole-v1/ppo-1000-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "env = minari_dataset.recover_environment()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "assert isinstance(observation_space, spaces.Box)\n",
    "assert isinstance(action_space, spaces.Discrete)\n",
    "\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = minari_dataset[0]\n",
    "print(episode)\n",
    "print(episode.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy loss like a classic classification task, as the action space is discrete.\n",
    "We then train the policy to predict the actions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        a_pred = policy_net(batch['observations'][:, :-1])\n",
    "        a_hat = F.one_hot(batch[\"actions\"]).type(torch.float32)\n",
    "        loss = loss_fn(a_pred, a_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can evaluate if the policy learned from the expert!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "done = False\n",
    "accumulated_rew = 0\n",
    "while not done:\n",
    "    action = policy_net(torch.Tensor(obs)).argmax()\n",
    "    obs, rew, ter, tru, _ = env.step(action.numpy())\n",
    "    done = ter or tru\n",
    "    accumulated_rew += rew\n",
    "\n",
    "env.close()\n",
    "print(\"Accumulated rew: \", accumulated_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually observe that the learned policy aces this simple control task, and we get the maximum reward 500, as the episode is truncated after 500 steps.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
