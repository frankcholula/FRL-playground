{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Behavioral cloning with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here how to perform behavioral cloning on a Minari dataset using [PyTorch](https://pytorch.org/).\n",
    "We will start generating the dataset of the expert policy for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, which is a classic control problem.\n",
    "The objective is to balance the pole on the cart, and we receive a reward of +1 for each successful timestep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "For this tutorial you will need the [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) library, which you can install with `pip install rl_zoo3`.\n",
    "Let's then import all the required packages and set the random seed for reproducibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12354a9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import minari\n",
    "from minari import DataCollector\n",
    "\n",
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy training\n",
    "Now we can train the expert policy using RL Baselines3 Zoo.\n",
    "We train a PPO agent on the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LunarLanderContinuous-v3 ==========\n",
      "Seed: 1897547941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsufanglu\u001b[0m (\u001b[33mfrankcholula\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/frankcholula/Workspace/school/FRL-playground/wandb/run-20250707_194913-4iht0hhr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/frankcholula/FRL/runs/4iht0hhr' target=\"_blank\">LunarLanderContinuous-v3__ppo__1897547941__1751888953</a></strong> to <a href='https://wandb.ai/frankcholula/FRL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/frankcholula/FRL' target=\"_blank\">https://wandb.ai/frankcholula/FRL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/frankcholula/FRL/runs/4iht0hhr' target=\"_blank\">https://wandb.ai/frankcholula/FRL/runs/4iht0hhr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hyperparameters from: /Users/frankcholula/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/rl_zoo3/hyperparams/ppo.yml\n",
      "Default hyperparameters for environment (ones being tuned will be overridden):\n",
      "OrderedDict([('batch_size', 64),\n",
      "             ('ent_coef', 0.01),\n",
      "             ('gae_lambda', 0.98),\n",
      "             ('gamma', 0.999),\n",
      "             ('n_envs', 16),\n",
      "             ('n_epochs', 4),\n",
      "             ('n_steps', 1024),\n",
      "             ('n_timesteps', 1000000.0),\n",
      "             ('policy', 'MlpPolicy')])\n",
      "Using 16 environments\n",
      "Overwriting n_timesteps with n=1000000\n",
      "Creating test environment\n",
      "Using cpu device\n",
      "Log path: logs/ppo/LunarLanderContinuous-v3_4\n",
      "Logging to runs/LunarLanderContinuous-v3__ppo__1897547941__1751888953/LunarLanderContinuous-v3/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | -268     |\n",
      "| time/              |          |\n",
      "|    fps             | 22817    |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24992, episode_reward=-139.37 +/- 22.20\n",
      "Episode length: 65.40 +/- 9.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 65.4         |\n",
      "|    mean_reward          | -139         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24992        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059353397 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.000166     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+03     |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.73e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 12692    |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -180        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11114       |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004467642 |\n",
      "|    clip_fraction        | 0.0374      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.00248    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.64e+03    |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.24e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49984, episode_reward=-122.97 +/- 57.20\n",
      "Episode length: 82.20 +/- 10.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 82.2         |\n",
      "|    mean_reward          | -123         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052393377 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.000738     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 747          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00487     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 2.19e+03     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -125     |\n",
      "| time/              |          |\n",
      "|    fps             | 10479    |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74976, episode_reward=-68.56 +/- 38.31\n",
      "Episode length: 72.60 +/- 17.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 72.6        |\n",
      "|    mean_reward          | -68.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74976       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009187003 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.00703    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 307         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 781         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | -95.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 10131    |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 112          |\n",
      "|    ep_rew_mean          | -81.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9958         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044391984 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.000592    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 306          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 581          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=99968, episode_reward=-68.40 +/- 48.90\n",
      "Episode length: 88.80 +/- 12.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 88.8         |\n",
      "|    mean_reward          | -68.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065487092 |\n",
      "|    clip_fraction        | 0.0555       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.00221      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 196          |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 445          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | -65.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 9799     |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124960, episode_reward=-74.77 +/- 32.35\n",
      "Episode length: 94.20 +/- 2.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 94.2         |\n",
      "|    mean_reward          | -74.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 124960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062956344 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -0.00176     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 365          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | -48.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 9681     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 118        |\n",
      "|    ep_rew_mean          | -33.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 9638       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509621 |\n",
      "|    clip_fraction        | 0.0588     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.85      |\n",
      "|    explained_variance   | -5.08e-05  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 216        |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 440        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=149952, episode_reward=-98.37 +/- 49.37\n",
      "Episode length: 140.60 +/- 24.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 141        |\n",
      "|    mean_reward          | -98.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 149952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00515866 |\n",
      "|    clip_fraction        | 0.0403     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.83      |\n",
      "|    explained_variance   | -4.89e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 168        |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.00273   |\n",
      "|    std                  | 0.988      |\n",
      "|    value_loss           | 430        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | -21.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 9551     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=174944, episode_reward=-132.47 +/- 53.10\n",
      "Episode length: 165.60 +/- 31.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 166         |\n",
      "|    mean_reward          | -132        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006302977 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 5.19e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 228         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 497         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | -22.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 9474     |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | -11.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9452        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005333285 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | -0.0189     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 329         |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.000809   |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 754         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=199936, episode_reward=52.42 +/- 184.14\n",
      "Episode length: 201.20 +/- 64.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 201         |\n",
      "|    mean_reward          | 52.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003936667 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | -0.00707    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 214         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.000784   |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 650         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | -1.66    |\n",
      "| time/              |          |\n",
      "|    fps             | 9383     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224928, episode_reward=-73.48 +/- 58.47\n",
      "Episode length: 168.20 +/- 35.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 168          |\n",
      "|    mean_reward          | -73.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056592273 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.82        |\n",
      "|    explained_variance   | -2.5e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 275          |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.000255    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 665          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | -12.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 9325     |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 256          |\n",
      "|    ep_rew_mean          | -26.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9316         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043896977 |\n",
      "|    clip_fraction        | 0.0375       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 425          |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | 0.000821     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 779          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=249920, episode_reward=-103.68 +/- 51.10\n",
      "Episode length: 170.00 +/- 16.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 170          |\n",
      "|    mean_reward          | -104         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 249920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047015855 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 2.5e-06      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 653          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000341    |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 928          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 265      |\n",
      "|    ep_rew_mean     | -31.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 9269     |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=274912, episode_reward=-106.50 +/- 20.58\n",
      "Episode length: 144.40 +/- 15.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 144          |\n",
      "|    mean_reward          | -106         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 274912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062305713 |\n",
      "|    clip_fraction        | 0.0524       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | -4.52e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 433          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 789          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 276      |\n",
      "|    ep_rew_mean     | -16.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 9228     |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 307          |\n",
      "|    ep_rew_mean          | -8.56        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9219         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052695544 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 9.51e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 485          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 644          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=299904, episode_reward=-7.00 +/- 146.28\n",
      "Episode length: 215.20 +/- 114.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 215         |\n",
      "|    mean_reward          | -7          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 299904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002993689 |\n",
      "|    clip_fraction        | 0.0141      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0124      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 428         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 386      |\n",
      "|    ep_rew_mean     | -9.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 9152     |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324896, episode_reward=27.34 +/- 151.98\n",
      "Episode length: 253.40 +/- 90.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 253          |\n",
      "|    mean_reward          | 27.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 324896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038739454 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.533        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 48.6         |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 270          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 434      |\n",
      "|    ep_rew_mean     | 0.131    |\n",
      "| time/              |          |\n",
      "|    fps             | 9112     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 423          |\n",
      "|    ep_rew_mean          | 9.51         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9113         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043987734 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.705        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 310          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=349888, episode_reward=183.37 +/- 153.97\n",
      "Episode length: 274.40 +/- 58.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | 183          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 349888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043638106 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.3         |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 475      |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 9072     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=374880, episode_reward=110.59 +/- 129.37\n",
      "Episode length: 274.60 +/- 85.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 111         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 374880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004947194 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 91.5        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.000796   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 189         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 550      |\n",
      "|    ep_rew_mean     | 23.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 9035     |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 576         |\n",
      "|    ep_rew_mean          | 33.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9040        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005482744 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66.2        |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00161    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=399872, episode_reward=-19.95 +/- 155.53\n",
      "Episode length: 286.00 +/- 151.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 286         |\n",
      "|    mean_reward          | -19.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 399872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004499326 |\n",
      "|    clip_fraction        | 0.0422      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.000457   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 98.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 632      |\n",
      "|    ep_rew_mean     | 38.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 9005     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424864, episode_reward=-45.06 +/- 77.15\n",
      "Episode length: 240.40 +/- 85.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 240          |\n",
      "|    mean_reward          | -45.1        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 424864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030884973 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -9.76e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 43.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8982     |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 728          |\n",
      "|    ep_rew_mean          | 49.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8985         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036741178 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 103          |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.000435    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 111          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=449856, episode_reward=45.87 +/- 141.34\n",
      "Episode length: 352.40 +/- 248.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 352          |\n",
      "|    mean_reward          | 45.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 449856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052745547 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.898        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 57.6         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8948     |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474848, episode_reward=139.43 +/- 125.63\n",
      "Episode length: 363.80 +/- 196.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | 139          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 474848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054477304 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.83        |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.4         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.000919    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 53       |\n",
      "| time/              |          |\n",
      "|    fps             | 8918     |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 807          |\n",
      "|    ep_rew_mean          | 60.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8924         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055680433 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.81        |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16.3         |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.000845    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 55.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=499840, episode_reward=182.99 +/- 128.39\n",
      "Episode length: 379.60 +/- 178.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 380         |\n",
      "|    mean_reward          | 183         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 499840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004650808 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.8        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00013    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 72.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 8896     |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 856          |\n",
      "|    ep_rew_mean          | 80           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8900         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033970529 |\n",
      "|    clip_fraction        | 0.0395       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.79        |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 124          |\n",
      "|    policy_gradient_loss | -0.000184    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 63.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=524832, episode_reward=149.99 +/- 73.11\n",
      "Episode length: 289.80 +/- 82.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 290         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 524832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004476536 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 128         |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 88.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 8882     |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549824, episode_reward=230.74 +/- 42.15\n",
      "Episode length: 305.20 +/- 21.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | 231          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 549824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032201307 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 132          |\n",
      "|    policy_gradient_loss | -9.12e-05    |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 99.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 8863     |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 875          |\n",
      "|    ep_rew_mean          | 97.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8870         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031845109 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.12         |\n",
      "|    n_updates            | 136          |\n",
      "|    policy_gradient_loss | -9.87e-05    |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 58.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=574816, episode_reward=226.84 +/- 82.63\n",
      "Episode length: 291.80 +/- 38.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 292          |\n",
      "|    mean_reward          | 227          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 574816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022415062 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000193    |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 143          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 98.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 8853     |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599808, episode_reward=203.62 +/- 113.70\n",
      "Episode length: 275.80 +/- 56.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 276         |\n",
      "|    mean_reward          | 204         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 599808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004937497 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.000986   |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 98.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 8837     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 871         |\n",
      "|    ep_rew_mean          | 103         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8844        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004686108 |\n",
      "|    clip_fraction        | 0.033       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 148         |\n",
      "|    policy_gradient_loss | -0.000274   |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 31.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=624800, episode_reward=174.06 +/- 130.62\n",
      "Episode length: 349.40 +/- 46.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 349          |\n",
      "|    mean_reward          | 174          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 624800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047381558 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.14         |\n",
      "|    n_updates            | 152          |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 10.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 8826     |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=649792, episode_reward=251.68 +/- 24.41\n",
      "Episode length: 331.80 +/- 35.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 649792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004277163 |\n",
      "|    clip_fraction        | 0.0388      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.85        |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.000654   |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 935      |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    fps             | 8810     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 960          |\n",
      "|    ep_rew_mean          | 131          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8818         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051876483 |\n",
      "|    clip_fraction        | 0.0377       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 0.000379     |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 11.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=674784, episode_reward=184.43 +/- 112.09\n",
      "Episode length: 287.20 +/- 25.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 184          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 674784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041833865 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.76        |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.47         |\n",
      "|    n_updates            | 164          |\n",
      "|    policy_gradient_loss | -0.000349    |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 18.3         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 130      |\n",
      "| time/              |          |\n",
      "|    fps             | 8805     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699776, episode_reward=256.73 +/- 30.92\n",
      "Episode length: 303.80 +/- 13.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 304         |\n",
      "|    mean_reward          | 257         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 699776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005421252 |\n",
      "|    clip_fraction        | 0.0583      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    std                  | 0.964       |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 944      |\n",
      "|    ep_rew_mean     | 134      |\n",
      "| time/              |          |\n",
      "|    fps             | 8793     |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 941          |\n",
      "|    ep_rew_mean          | 137          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8799         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032319021 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.75        |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.7         |\n",
      "|    n_updates            | 172          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 31           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=724768, episode_reward=264.80 +/- 7.73\n",
      "Episode length: 316.40 +/- 6.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 724768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006763568 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.73       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.44        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | 0.000219    |\n",
      "|    std                  | 0.943       |\n",
      "|    value_loss           | 32.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 136      |\n",
      "| time/              |          |\n",
      "|    fps             | 8786     |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=749760, episode_reward=263.33 +/- 17.17\n",
      "Episode length: 330.20 +/- 12.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 263         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 749760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003949656 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.01        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000853   |\n",
      "|    std                  | 0.939       |\n",
      "|    value_loss           | 25.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    fps             | 8773     |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 924          |\n",
      "|    ep_rew_mean          | 134          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8780         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044703013 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.1          |\n",
      "|    n_updates            | 184          |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    std                  | 0.938        |\n",
      "|    value_loss           | 26.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=774752, episode_reward=265.10 +/- 15.92\n",
      "Episode length: 324.60 +/- 14.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 265          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 774752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046494505 |\n",
      "|    clip_fraction        | 0.0483       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.54         |\n",
      "|    n_updates            | 188          |\n",
      "|    policy_gradient_loss | 0.000868     |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 6.6          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 140      |\n",
      "| time/              |          |\n",
      "|    fps             | 8768     |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799744, episode_reward=264.73 +/- 10.44\n",
      "Episode length: 331.80 +/- 10.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 265         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 799744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004828026 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.000852   |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 7.79        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 965      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    fps             | 8756     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 965          |\n",
      "|    ep_rew_mean          | 142          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8764         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046796347 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.69        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.6         |\n",
      "|    n_updates            | 196          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.926        |\n",
      "|    value_loss           | 21.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=824736, episode_reward=263.78 +/- 17.51\n",
      "Episode length: 317.00 +/- 11.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | 264          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 824736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059324317 |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    std                  | 0.924        |\n",
      "|    value_loss           | 21.9         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    fps             | 8754     |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=849728, episode_reward=257.90 +/- 26.73\n",
      "Episode length: 331.80 +/- 17.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 332          |\n",
      "|    mean_reward          | 258          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 849728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054033455 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.67        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 204          |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    std                  | 0.919        |\n",
      "|    value_loss           | 6.69         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 8744     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 988          |\n",
      "|    ep_rew_mean          | 147          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8750         |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045309905 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.37         |\n",
      "|    n_updates            | 208          |\n",
      "|    policy_gradient_loss | -0.000227    |\n",
      "|    std                  | 0.916        |\n",
      "|    value_loss           | 8.11         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=874720, episode_reward=254.83 +/- 20.56\n",
      "Episode length: 333.20 +/- 1.72\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 333          |\n",
      "|    mean_reward          | 255          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 874720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045548417 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.49         |\n",
      "|    n_updates            | 212          |\n",
      "|    policy_gradient_loss | 0.000697     |\n",
      "|    std                  | 0.914        |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 145      |\n",
      "| time/              |          |\n",
      "|    fps             | 8740     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=899712, episode_reward=262.63 +/- 26.47\n",
      "Episode length: 325.00 +/- 17.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 325          |\n",
      "|    mean_reward          | 263          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 899712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058511775 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.588        |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 3.94         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 146      |\n",
      "| time/              |          |\n",
      "|    fps             | 8731     |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 976          |\n",
      "|    ep_rew_mean          | 143          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8738         |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038825169 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 6.87e-05     |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 15.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=924704, episode_reward=272.50 +/- 8.36\n",
      "Episode length: 304.20 +/- 16.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 304          |\n",
      "|    mean_reward          | 273          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 924704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038414514 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.000952    |\n",
      "|    std                  | 0.904        |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    fps             | 8715     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949696, episode_reward=267.25 +/- 26.78\n",
      "Episode length: 291.20 +/- 8.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 291          |\n",
      "|    mean_reward          | 267          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 949696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046977648 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.63        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.889        |\n",
      "|    n_updates            | 228          |\n",
      "|    policy_gradient_loss | -0.000313    |\n",
      "|    std                  | 0.9          |\n",
      "|    value_loss           | 5.08         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    fps             | 8702     |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 968          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8708         |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056414334 |\n",
      "|    clip_fraction        | 0.055        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.79         |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | 0.000604     |\n",
      "|    std                  | 0.901        |\n",
      "|    value_loss           | 4.24         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=974688, episode_reward=258.68 +/- 14.88\n",
      "Episode length: 286.80 +/- 13.04\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 287        |\n",
      "|    mean_reward          | 259        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 974688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00437776 |\n",
      "|    clip_fraction        | 0.0324     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.63      |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.4       |\n",
      "|    n_updates            | 236        |\n",
      "|    policy_gradient_loss | 0.000791   |\n",
      "|    std                  | 0.908      |\n",
      "|    value_loss           | 16.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    fps             | 8700     |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 960         |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8704        |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004755489 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.63       |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.07        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.000519    |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=999680, episode_reward=275.20 +/- 16.61\n",
      "Episode length: 264.40 +/- 8.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 264          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 999680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043333382 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.41         |\n",
      "|    n_updates            | 244          |\n",
      "|    policy_gradient_loss | 0.000299     |\n",
      "|    std                  | 0.899        |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 151      |\n",
      "| time/              |          |\n",
      "|    fps             | 8697     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 1015808  |\n",
      "---------------------------------\n",
      "Saving to logs/ppo/LunarLanderContinuous-v3_4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.argv = [\n",
    "    \"python\", \n",
    "    \"--algo\", \"ppo\",\n",
    "    \"--env\", \"LunarLanderContinuous-v3\",\n",
    "    \"--n-timesteps\", \"1000000\",\n",
    "    \"--track\",\n",
    "    \"--wandb-project-name\", \"FRL\",\n",
    "    \"--wandb-entity\", \"frankcholula\",\n",
    "    \"--tensorboard-log\", \"runs\",\n",
    "    \"--hyperparams\",\n",
    "    \"n_envs:16\",\n",
    "    \"n_steps:1024\",\n",
    "    \"batch_size:64\",\n",
    "    \"n_epochs:4\",\n",
    "    \"gamma:0.999\",\n",
    "    \"gae_lambda:0.98\",\n",
    "    \"ent_coef:0.01\"\n",
    "]\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a new folder named `log` with the expert policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "Now let's generate the dataset using the [DataCollector](https://minari.farama.org/api/data_collector/) wrapper:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/frankcholula/Workspace/school/FRL-playground/code/models/ppo-LunarLander-v3/model.zip.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# path = os.path.abspath('') + '/logs/ppo/LunarLanderContinuous-v3_1/best_model.zip'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/code/models/ppo-LunarLander-v3/model.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m total_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1_000\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(total_episodes)):\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:681\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m     get_system_info()\n\u001b[0;32m--> 681\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:403\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[1;32m    377\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[1;32m    378\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:240\u001b[0m, in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:291\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    285\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:272\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    270\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/frankcholula/Workspace/school/FRL-playground/code/models/ppo-LunarLander-v3/model.zip.zip'"
     ]
    }
   ],
   "source": [
    "env = DataCollector(gym.make(\"LunarLander-v3\", continuous=True))\n",
    "path = os.path.abspath('')+ \"/code/models/ppo-LunarLander-v3/model.zip\"\n",
    "agent = PPO.load(path)\n",
    "\n",
    "total_episodes = 1_000\n",
    "for i in tqdm(range(total_episodes)):\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    while True:\n",
    "        action, _ = agent.predict(obs)\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = env.create_dataset(\n",
    "    dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\",\n",
    "    algorithm_name=\"ppo\",\n",
    "    code_permalink=\"https://github.com/frankcholula/FRL-playground/blob/main/code/behavioral_cloning.py\",\n",
    "    author=\"Frank Lu\",\n",
    "    author_email=\"lu.phrank@gmail.com\",\n",
    "    description=\"Behavioral cloning dataset for LunarLanderContinuous-v3 using PPO\",\n",
    "    eval_env=\"LunarLanderContinuous-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodeData(id=110, total_steps=1000, observations=ndarray of shape (1001, 8) and dtype float32, actions=ndarray of shape (1000, 2) and dtype float32, rewards=ndarray of 1000 floats, terminations=ndarray of 1000 bools, truncations=ndarray of 1000 bools, infos=dict with the following keys: [])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "# env = dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "episode = dataset[110]\n",
    "print(episode)\n",
    "\n",
    "# obs, _ = env.reset(seed=42)\n",
    "# for action in episode.actions:\n",
    "#     obs, rew, terminated, truncated, info = env.step(action)\n",
    "#     print(f\"Action: {action}, Reward: {rew}\")\n",
    "#     env.render()\n",
    "#     if terminated or truncated:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executing the script, the dataset will be saved on your disk. You can display the list of datasets with ``minari list local`` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral cloning with PyTorch\n",
    "Now we can use PyTorch to learn the policy from the offline dataset.\n",
    "Let's define the policy network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the output dimension will be two, as previously mentioned. As for the input dimension, it will be four, corresponding to the observation space of ``CartPole-v1``.\n",
    "Our next step is to load the dataset and set up the training loop. The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "However, since each episode can have a varying length, we need to pad them.\n",
    "To achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to load the data and create the training loop.\n",
    "To begin, let's initialize the DataLoader, neural network, optimizer, and loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space dimension: 8, Action space dimension: 2\n"
     ]
    }
   ],
   "source": [
    "minari_dataset = minari.load_dataset(\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "env = minari_dataset.recover_environment()\n",
    "\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "assert isinstance(observation_space, spaces.Box)\n",
    "assert isinstance(action_space, spaces.Box)\n",
    "\n",
    "obs_dim = np.prod(observation_space.shape)\n",
    "action_dim = action_space.shape[0]\n",
    "print(f\"Observation space dimension: {obs_dim}, Action space dimension: {action_dim}\")\n",
    "\n",
    "policy_net = PolicyNetwork(obs_dim, action_dim)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpisodeData(id=0, total_steps=1000, observations=ndarray of shape (1001, 8) and dtype float32, actions=ndarray of shape (1000, 2) and dtype float32, rewards=ndarray of 1000 floats, terminations=ndarray of 1000 bools, truncations=ndarray of 1000 bools, infos=dict with the following keys: [])\n",
      "dict_keys(['id', 'observations', 'actions', 'rewards', 'terminations', 'truncations', 'infos'])\n"
     ]
    }
   ],
   "source": [
    "episode = minari_dataset[0]\n",
    "print(episode)\n",
    "print(episode.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy loss like a classic classification task, as the action space is discrete.\n",
    "We then train the policy to predict the actions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1001, 8]) torch.Size([256, 1000, 2])\n",
      "torch.Size([256, 1001, 8]) torch.Size([256, 1000, 2])\n",
      "torch.Size([256, 1001, 8]) torch.Size([256, 1000, 2])\n",
      "torch.Size([232, 1001, 8]) torch.Size([232, 1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# This is baseline MLP cloning\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        observations = batch[\"observations\"][:, :-1]  # Exclude the last observation\n",
    "        expert_actions = batch[\"actions\"]\n",
    "        predictioned_actions = policy_net(observations)\n",
    "        loss = loss_fn(predictioned_actions, expert_actions)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can evaluate if the policy learned from the expert!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated rew:  275.85113360217093\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"human\")\n",
    "obs, _ = eval_env.reset()\n",
    "done = False\n",
    "accumulated_rew = 0\n",
    "while not done:\n",
    "    obs_tensor = torch.Tensor(obs)\n",
    "    action = policy_net(obs_tensor).detach().numpy()\n",
    "    obs, rew, ter, tru, _ = eval_env.step(action)\n",
    "    done = ter or tru\n",
    "    accumulated_rew += rew\n",
    "\n",
    "env.close()\n",
    "print(\"Accumulated rew: \", accumulated_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually observe that the learned policy aces this simple control task, and we get the maximum reward 500, as the episode is truncated after 500 steps.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
