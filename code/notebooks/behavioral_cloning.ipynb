{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Behavioral cloning with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present here how to perform behavioral cloning on a Minari dataset using [PyTorch](https://pytorch.org/).\n",
    "We will start generating the dataset of the expert policy for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment, which is a classic control problem.\n",
    "The objective is to balance the pole on the cart, and we receive a reward of +1 for each successful timestep.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "For this tutorial you will need the [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) library, which you can install with `pip install rl_zoo3`.\n",
    "Let's then import all the required packages and set the random seed for reproducibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1158d6fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import minari\n",
    "from minari import DataCollector\n",
    "\n",
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy training\n",
    "Now we can train the expert policy using RL Baselines3 Zoo.\n",
    "We train a PPO agent on the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: python [-h]\n",
      "              [--algo {a2c,ddpg,dqn,ppo,sac,td3,ars,crossq,qrdqn,tqc,trpo,ppo_lstm}]\n",
      "              [--env ENV] [-tb TENSORBOARD_LOG] [-i TRAINED_AGENT]\n",
      "              [--truncate-last-trajectory TRUNCATE_LAST_TRAJECTORY]\n",
      "              [-n N_TIMESTEPS] [--num-threads NUM_THREADS]\n",
      "              [--log-interval LOG_INTERVAL] [--eval-freq EVAL_FREQ]\n",
      "              [--optimization-log-path OPTIMIZATION_LOG_PATH]\n",
      "              [--eval-episodes EVAL_EPISODES] [--n-eval-envs N_EVAL_ENVS]\n",
      "              [--save-freq SAVE_FREQ] [--save-replay-buffer] [-f LOG_FOLDER]\n",
      "              [--seed SEED] [--vec-env {dummy,subproc}] [--device DEVICE]\n",
      "              [--n-trials N_TRIALS] [--max-total-trials MAX_TOTAL_TRIALS]\n",
      "              [-optimize] [--no-optim-plots] [--n-jobs N_JOBS]\n",
      "              [--sampler {random,tpe,auto}] [--pruner {halving,median,none}]\n",
      "              [--n-startup-trials N_STARTUP_TRIALS]\n",
      "              [--n-evaluations N_EVALUATIONS] [--storage STORAGE]\n",
      "              [--study-name STUDY_NAME] [--trial-id TRIAL_ID]\n",
      "              [--verbose VERBOSE]\n",
      "              [--gym-packages GYM_PACKAGES [GYM_PACKAGES ...]]\n",
      "              [--env-kwargs ENV_KWARGS [ENV_KWARGS ...]]\n",
      "              [--eval-env-kwargs EVAL_ENV_KWARGS [EVAL_ENV_KWARGS ...]]\n",
      "              [-params HYPERPARAMS [HYPERPARAMS ...]] [-conf CONF_FILE]\n",
      "              [-uuid] [--track] [--wandb-project-name WANDB_PROJECT_NAME]\n",
      "              [--wandb-entity WANDB_ENTITY] [-P]\n",
      "              [-tags WANDB_TAGS [WANDB_TAGS ...]]\n",
      "python: error: unrecognized arguments: -m rl_zoo3.record_training\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from rl_zoo3.train import train\n",
    "\n",
    "sys.argv = [\n",
    "    \"python\", \n",
    "    \"--algo\", \"ppo\",\n",
    "    \"--env\", \"LunarLanderContinuous-v3\",\n",
    "    \"--n-timesteps\", \"1000000\",\n",
    "    \"--track\",\n",
    "    \"--wandb-project-name\", \"FRL\",\n",
    "    \"--wandb-entity\", \"frankcholula\",\n",
    "    \"--tensorboard-log\", \"runs\",\n",
    "    \"--hyperparams\",\n",
    "    \"n_envs:16\",\n",
    "    \"n_steps:1024\",\n",
    "    \"batch_size:64\",\n",
    "    \"n_epochs:4\",\n",
    "    \"gamma:0.999\",\n",
    "    \"gae_lambda:0.98\",\n",
    "    \"ent_coef:0.01\"\n",
    "]\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a new folder named `log` with the expert policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "Now let's generate the dataset using the [DataCollector](https://minari.farama.org/api/data_collector/) wrapper:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:09<00:00,  7.74it/s]\n"
     ]
    }
   ],
   "source": [
    "env = DataCollector(gym.make(\"LunarLander-v3\", continuous=True))\n",
    "# path = os.path.abspath('') + '/logs/ppo/LunarLanderContinuous-v3_1/best_model.zip'\n",
    "path = os.path.abspath('')+ \"/code/models/ppo-LunarLander-v3/model.zip\"\n",
    "agent = PPO.load(path)\n",
    "\n",
    "total_episodes = 1_000\n",
    "for i in tqdm(range(total_episodes)):\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    while True:\n",
    "        action, _ = agent.predict(obs)\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = env.create_dataset(\n",
    "    dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\",\n",
    "    algorithm_name=\"ppo\",\n",
    "    code_permalink=\"https://github.com/frankcholula/FRL-playground/blob/main/code/behavioral_cloning.py\",\n",
    "    author=\"Frank Lu\",\n",
    "    author_email=\"lu.phrank@gmail.com\",\n",
    "    description=\"Behavioral cloning dataset for LunarLanderContinuous-v3 using PPO\",\n",
    "    eval_env=\"LunarLanderContinuous-v3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: [-0.28617015 -0.714767  ], Reward: 1.8161625786636648\n",
      "Action: [ 1.        -0.9964686], Reward: -3.6125707707527464\n",
      "Action: [-0.518642    0.09511064], Reward: 1.628096668149567\n",
      "Action: [1.       0.790869], Reward: -5.888326566104735\n",
      "Action: [-0.49828166 -0.7606091 ], Reward: 1.966223083634825\n",
      "Action: [0.6437612 1.       ], Reward: -1.5019909111680807\n",
      "Action: [-1.         -0.52062786], Reward: 1.7075535512099362\n",
      "Action: [-0.28405666 -0.6373388 ], Reward: 1.607869354638616\n",
      "Action: [-0.5445579   0.22386494], Reward: 1.3381137438466908\n",
      "Action: [0.0086565 1.       ], Reward: -0.5827368475826051\n",
      "Action: [ 1.        -0.5579664], Reward: -0.6881141285209503\n",
      "Action: [1.         0.09120452], Reward: -1.8882943511526264\n",
      "Action: [0.33423138 0.14653477], Reward: -1.5946967239508012\n",
      "Action: [ 1. -1.], Reward: -2.465542745378302\n",
      "Action: [-0.87221855  1.        ], Reward: 0.6496736457252166\n",
      "Action: [ 0.5733504  -0.85164344], Reward: -0.5674295750070075\n",
      "Action: [1. 1.], Reward: -3.0142298045168316\n",
      "Action: [ 0.71044624 -1.        ], Reward: 0.26228617144980837\n",
      "Action: [0.09658122 1.        ], Reward: -2.1217288175548945\n",
      "Action: [0.6626229  0.16020685], Reward: -3.101053846625439\n",
      "Action: [ 0.6206112 -1.       ], Reward: -1.6537796379991268\n",
      "Action: [1.        0.5296507], Reward: -0.937552226393633\n",
      "Action: [1. 1.], Reward: -3.866977205925154\n",
      "Action: [ 1.        -0.7939553], Reward: -3.3841684705518045\n",
      "Action: [1.         0.10264874], Reward: -3.622245094131176\n",
      "Action: [-0.10943276 -0.7773795 ], Reward: 1.6449722214930818\n",
      "Action: [1. 1.], Reward: -1.3078941348152728\n",
      "Action: [-0.6289265  1.       ], Reward: 1.2765369651575054\n",
      "Action: [0.9650984  0.75652575], Reward: -3.850504978064571\n",
      "Action: [ 0.8458842 -1.       ], Reward: 0.027443931625687118\n",
      "Action: [-0.87556684  1.        ], Reward: 1.4771884342804082\n",
      "Action: [ 0.91596496 -0.38690042], Reward: -3.076499506354577\n",
      "Action: [-0.32765922  1.        ], Reward: 1.5607714705484466\n",
      "Action: [0.66308016 0.14693901], Reward: -2.710549639183739\n",
      "Action: [0.43020147 0.7350298 ], Reward: -0.7300870391317584\n",
      "Action: [-0.57172656 -0.10676667], Reward: 0.8829116974808073\n",
      "Action: [-0.63324237 -1.        ], Reward: 1.5823022643126603\n",
      "Action: [ 0.09231782 -1.        ], Reward: -0.06625350703306934\n",
      "Action: [ 0.96209466 -1.        ], Reward: -2.0701210953613653\n",
      "Action: [1.         0.23953687], Reward: -2.369994215232725\n",
      "Action: [ 1.         -0.01831186], Reward: -1.9758826548954176\n",
      "Action: [-1.  1.], Reward: 0.7885261193094959\n",
      "Action: [-0.6740582  -0.55072224], Reward: 1.5634395681345223\n",
      "Action: [1.         0.54165757], Reward: -3.321869011124703\n",
      "Action: [ 1.        -0.4764867], Reward: -4.77120383245948\n",
      "Action: [-0.10131252 -0.3853802 ], Reward: 1.2529279564237754\n",
      "Action: [ 0.21270847 -0.6158923 ], Reward: -0.4709678270607492\n",
      "Action: [ 1.        -0.6251993], Reward: -1.9476985298838896\n",
      "Action: [1.        0.8104791], Reward: -2.916278340844658\n",
      "Action: [0.08219889 0.72297907], Reward: -1.3068238299067094\n",
      "Action: [ 1. -1.], Reward: -4.205250122581576\n",
      "Action: [1.        0.1181782], Reward: -3.122274841892266\n",
      "Action: [-0.7175532  1.       ], Reward: 0.4985074224316588\n",
      "Action: [-0.95247436  1.        ], Reward: 0.34188127473459073\n",
      "Action: [-0.58313096 -1.        ], Reward: 1.6617154195613761\n",
      "Action: [1. 1.], Reward: -2.5160293457294514\n",
      "Action: [1.         0.01965912], Reward: -2.041496542392184\n",
      "Action: [ 1.         -0.37397426], Reward: -2.9676369826147093\n",
      "Action: [ 0.6435777 -1.       ], Reward: -1.8958263067879488\n",
      "Action: [-0.42635655  0.08160339], Reward: 1.081519313394466\n",
      "Action: [ 0.6875845  -0.78748995], Reward: -3.569403469688889\n",
      "Action: [0.41886395 0.66398674], Reward: -2.6512345140569673\n",
      "Action: [ 0.2869145 -0.2508603], Reward: -1.7508262194793929\n",
      "Action: [1.         0.62142545], Reward: -5.936565328839988\n",
      "Action: [0.38450876 0.0364116 ], Reward: -1.9053240551058934\n",
      "Action: [-1.          0.37041652], Reward: 0.8736041337657241\n",
      "Action: [0.54881954 1.        ], Reward: -2.3800888788526877\n",
      "Action: [1.         0.46723008], Reward: -5.9922703011746306\n",
      "Action: [ 0.00823918 -0.10103792], Reward: -0.9268393824333486\n",
      "Action: [ 0.5282097 -1.       ], Reward: -1.1022623975992805\n",
      "Action: [1.         0.08805585], Reward: -3.0648718045479653\n",
      "Action: [ 1.         -0.00589868], Reward: -5.181093451835113\n",
      "Action: [ 1. -1.], Reward: -2.204034273341976\n",
      "Action: [-0.9212001 -1.       ], Reward: 1.972873191632034\n",
      "Action: [ 1.        -0.6131066], Reward: -2.7638286266486007\n",
      "Action: [ 0.15333359 -0.07034315], Reward: -2.349736530403857\n",
      "Action: [1.         0.16280082], Reward: -3.177108039646032\n",
      "Action: [-0.28743744  0.6389622 ], Reward: 0.8968841383560311\n",
      "Action: [ 1.         -0.11869726], Reward: -2.9609314684264403\n",
      "Action: [-1.         0.8169117], Reward: 0.7571679706737132\n",
      "Action: [1.        0.7766983], Reward: -4.698822109004288\n",
      "Action: [-0.51274884 -0.74101186], Reward: 1.4911574262690988\n",
      "Action: [ 0.12934947 -0.37343597], Reward: -2.5043497437433757\n",
      "Action: [0.8193151 0.6163566], Reward: -2.7540816931734886\n",
      "Action: [ 1.         -0.22037338], Reward: -4.496729962904214\n",
      "Action: [-1.         -0.49099505], Reward: 0.9441277625645625\n",
      "Action: [-1.         -0.24902247], Reward: 0.9371597233644593\n",
      "Action: [-0.29753512 -1.        ], Reward: 1.9237707751016455\n",
      "Action: [ 0.5827011  -0.95478386], Reward: -0.6678549609412356\n",
      "Action: [-1.          0.38258886], Reward: 1.3454363528120439\n",
      "Action: [1.         0.64039266], Reward: -4.294609994817022\n",
      "Action: [ 1.        -0.3308553], Reward: -0.8872297363843586\n",
      "Action: [-0.1049327   0.58918464], Reward: 0.4113983917420774\n",
      "Action: [ 0.29079813 -1.        ], Reward: 0.5396326403003923\n",
      "Action: [0.5681838  0.50161695], Reward: -0.1423972826051613\n",
      "Action: [ 0.8489151 -0.8867831], Reward: -0.6045020480773223\n",
      "Action: [-0.8418428 -1.       ], Reward: 2.280960714891451\n",
      "Action: [1.         0.08968639], Reward: -2.5602542954170984\n",
      "Action: [-1.  1.], Reward: 0.2903589104599018\n",
      "Action: [-0.01237065  0.5164653 ], Reward: 0.5654388860298241\n",
      "Action: [1. 1.], Reward: -0.6587788299003297\n",
      "Action: [1.         0.32511863], Reward: -2.974366591754335\n",
      "Action: [1.         0.11180207], Reward: -1.6448904324811224\n",
      "Action: [-0.8727143  1.       ], Reward: -0.46788875061750335\n",
      "Action: [ 1.         -0.49362326], Reward: -2.998807712437542\n",
      "Action: [0.14040689 0.16633564], Reward: -0.42857821943889574\n",
      "Action: [ 1.        -0.9105077], Reward: -0.4787593740733382\n",
      "Action: [-1.  1.], Reward: -0.20783863790418763\n",
      "Action: [-0.5984524 -1.       ], Reward: 1.5274501038526591\n",
      "Action: [1.        0.8924818], Reward: -5.154739662247868\n",
      "Action: [-0.2113333   0.30999368], Reward: 0.5862852132859189\n",
      "Action: [ 0.55247086 -0.47272158], Reward: 0.23660482010338912\n",
      "Action: [ 0.28393447 -0.70525   ], Reward: -1.7005548305968068\n",
      "Action: [1.         0.49096805], Reward: -1.5329300332134495\n",
      "Action: [-0.27454504 -0.00269085], Reward: 0.6788947841349682\n",
      "Action: [ 1.        -0.5732609], Reward: -1.431362544690669\n",
      "Action: [-0.47022498 -1.        ], Reward: 1.8843979100071533\n",
      "Action: [0.08285187 1.        ], Reward: -2.523471748833167\n",
      "Action: [1. 1.], Reward: -1.9951490837881511\n",
      "Action: [1.        0.7938601], Reward: -3.1791283846362237\n",
      "Action: [-1.          0.67377245], Reward: -0.2377016744201387\n",
      "Action: [ 0.00581318 -0.59231013], Reward: -0.05939483075094358\n",
      "Action: [ 1. -1.], Reward: -1.400456516163688\n",
      "Action: [1.        0.8786446], Reward: -1.2477195097902496\n",
      "Action: [-0.32871407 -0.9294471 ], Reward: 1.5210963852707777\n",
      "Action: [-0.05546544 -0.01737216], Reward: 0.5649969998984261\n",
      "Action: [0.73749864 0.41260496], Reward: 0.5288102840236775\n",
      "Action: [-0.35931578 -0.56454086], Reward: 1.203601482037941\n",
      "Action: [ 0.3748371  -0.03907061], Reward: -0.4837705095904198\n",
      "Action: [0.49912226 0.4672778 ], Reward: -0.9167446689015264\n",
      "Action: [ 0.12943311 -0.18435442], Reward: -0.4036983050722142\n",
      "Action: [1.         0.28387457], Reward: -3.2915992700552463\n",
      "Action: [-0.3079158  1.       ], Reward: -0.3113950345956187\n",
      "Action: [ 1. -1.], Reward: 0.898482866496904\n",
      "Action: [-0.4775785  1.       ], Reward: -0.4809029808106675\n",
      "Action: [1.         0.89247787], Reward: -1.3203053512914449\n",
      "Action: [-0.7109431 -0.8988509], Reward: 1.4065899689411208\n",
      "Action: [1.        0.6672783], Reward: -1.1089323854254167\n",
      "Action: [1. 1.], Reward: -2.2719222893296247\n",
      "Action: [0.788912 1.      ], Reward: -5.0264148494478995\n",
      "Action: [0.91425097 0.62521887], Reward: -5.125334657279367\n",
      "Action: [-1.          0.76774466], Reward: -1.055808558723586\n",
      "Action: [-1. -1.], Reward: 0.9315313862108792\n",
      "Action: [-0.7012565 -1.       ], Reward: 0.9611678771386505\n",
      "Action: [-0.6682967  -0.44829947], Reward: 0.05094742018184206\n",
      "Action: [-0.67124367 -0.2636995 ], Reward: 0.00029498193686094965\n",
      "Action: [ 0.7943725  -0.29157048], Reward: -0.4459447643887984\n",
      "Action: [ 0.19772452 -0.1095444 ], Reward: -1.114510944730864\n",
      "Action: [-0.49181122  0.62250847], Reward: -0.7767326821894198\n",
      "Action: [ 0.540152  -0.1461094], Reward: -1.1423492318722424\n",
      "Action: [ 1. -1.], Reward: -1.641778692487792\n",
      "Action: [ 0.8864306 -1.       ], Reward: -2.1320448638410925\n",
      "Action: [ 1. -1.], Reward: -1.7199932610414113\n",
      "Action: [-0.136302   -0.17111649], Reward: 0.45571028685833426\n",
      "Action: [ 0.72220737 -0.05089533], Reward: -2.545174958139569\n",
      "Action: [-0.18692803 -0.4337783 ], Reward: 0.4607237801865267\n",
      "Action: [ 1. -1.], Reward: -0.23379894657587555\n",
      "Action: [1.        0.6372927], Reward: -2.008514549228719\n",
      "Action: [-0.7128966  -0.07814382], Reward: 0.5578723485527917\n",
      "Action: [ 0.49570942 -0.28576678], Reward: -0.932436570254481\n",
      "Action: [-0.91384125  0.7265813 ], Reward: -0.4177426190175356\n",
      "Action: [1.         0.27551243], Reward: -2.4165750163330584\n",
      "Action: [ 1. -1.], Reward: -1.653450717749438\n",
      "Action: [1. 1.], Reward: -3.6304053239736773\n",
      "Action: [ 1.         -0.54421717], Reward: -2.86178069841887\n",
      "Action: [ 0.8707516 -1.       ], Reward: -0.7780479949125311\n",
      "Action: [1. 1.], Reward: -1.7493786758372654\n",
      "Action: [1.        0.8812908], Reward: -2.3059709148694467\n",
      "Action: [ 1. -1.], Reward: -1.8416983912684532\n",
      "Action: [ 0.36904347 -0.15114924], Reward: -1.3344463123413106\n",
      "Action: [ 0.24900174 -1.        ], Reward: -1.3034743752701434\n",
      "Action: [-1.       -0.653145], Reward: 1.5040803393014586\n",
      "Action: [-0.06587853 -1.        ], Reward: 2.2027221768175607\n",
      "Action: [-0.9380724  -0.33406073], Reward: 1.2201682526364266\n",
      "Action: [-0.17315313  1.        ], Reward: 0.09361258022195898\n",
      "Action: [ 0.92298687 -0.06037837], Reward: -2.3130425011253464\n",
      "Action: [ 1.         -0.76162887], Reward: -2.946186956877577\n",
      "Action: [1. 1.], Reward: -3.3688876516995534\n",
      "Action: [-1. -1.], Reward: 0.5162127328497104\n",
      "Action: [ 0.27591512 -0.0104475 ], Reward: -0.7309327792068189\n",
      "Action: [0.9053528 1.       ], Reward: -4.020894141026786\n",
      "Action: [-0.6160757 -0.5299324], Reward: 0.1468726800663262\n",
      "Action: [1.        0.2734512], Reward: -4.10667965511293\n",
      "Action: [0.74962926 1.        ], Reward: -3.5679214094036342\n",
      "Action: [0.9358865 1.       ], Reward: -4.595536869261815\n",
      "Action: [-0.95929927 -0.29956055], Reward: -0.012556548325278527\n",
      "Action: [ 0.639759  -0.6569777], Reward: -1.8081439629407532\n",
      "Action: [0.8139422  0.20799138], Reward: -1.2957852829147214\n",
      "Action: [-0.32275724 -0.05033077], Reward: -0.09223492056150917\n",
      "Action: [ 0.6031676 -1.       ], Reward: -0.24811945747581374\n",
      "Action: [-1.  1.], Reward: -0.9762175928858869\n",
      "Action: [-0.22614223  1.        ], Reward: -0.8910511519391366\n",
      "Action: [0.3722853  0.89067936], Reward: -2.7345335646365583\n",
      "Action: [ 0.04353812 -0.8862357 ], Reward: 0.5530913805095088\n",
      "Action: [-0.7158218 -1.       ], Reward: 0.5532080892650992\n",
      "Action: [0.3393126  0.83362764], Reward: -2.5130112813242294\n",
      "Action: [ 0.32214898 -1.        ], Reward: -1.2859788632422398\n",
      "Action: [1. 1.], Reward: -3.188379417589219\n",
      "Action: [ 1.         -0.45254806], Reward: -2.404467404465424\n",
      "Action: [-0.6007873  1.       ], Reward: -0.8133760763920737\n",
      "Action: [ 1.        -0.9463484], Reward: -1.2730080786402915\n",
      "Action: [ 0.42387047 -1.        ], Reward: -1.3163173033475732\n",
      "Action: [1. 1.], Reward: -0.7889598185813771\n",
      "Action: [-1.         -0.44647765], Reward: -0.1303792924187519\n",
      "Action: [ 1.         -0.00213076], Reward: -2.508082902560022\n",
      "Action: [ 1.         -0.52570915], Reward: -2.1039260979899197\n",
      "Action: [-0.79738045  0.3831486 ], Reward: -0.3225511635837961\n",
      "Action: [-0.29492402  1.        ], Reward: -1.2549002361131716\n",
      "Action: [ 0.90344214 -0.59450924], Reward: -1.9498485226962317\n",
      "Action: [-0.21590716 -0.24891657], Reward: -0.28594317825564985\n",
      "Action: [ 1.        -0.3775922], Reward: -0.9392232733575157\n",
      "Action: [-0.73247254  0.27087498], Reward: -0.3087378326015937\n",
      "Action: [-0.77544093 -0.2762422 ], Reward: -0.34819886120993715\n",
      "Action: [1. 1.], Reward: -0.4566687429703097\n",
      "Action: [ 0.8897239  -0.69221395], Reward: -0.8327109600070252\n",
      "Action: [-1. -1.], Reward: 0.4668246338904123\n",
      "Action: [1. 1.], Reward: -2.0475862105699476\n",
      "Action: [-0.41353846  1.        ], Reward: -1.226690855339341\n",
      "Action: [0.34206426 0.9484735 ], Reward: -0.016598791666774547\n",
      "Action: [ 0.32000095 -1.        ], Reward: 1.6160928873019953\n",
      "Action: [ 1. -1.], Reward: 0.531246681987211\n",
      "Action: [-0.59405756 -0.12099418], Reward: -0.11508595350926498\n",
      "Action: [ 1.         -0.12767471], Reward: -100\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "env = dataset.recover_environment(eval_env = True, render_mode=\"human\")\n",
    "episode = dataset[110]\n",
    "\n",
    "obs, _ = env.reset(seed=42)\n",
    "for action in episode.actions:\n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {rew}\")\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once executing the script, the dataset will be saved on your disk. You can display the list of datasets with ``minari list local`` command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral cloning with PyTorch\n",
    "Now we can use PyTorch to learn the policy from the offline dataset.\n",
    "Let's define the policy network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the output dimension will be two, as previously mentioned. As for the input dimension, it will be four, corresponding to the observation space of ``CartPole-v1``.\n",
    "Our next step is to load the dataset and set up the training loop. The ``MinariDataset`` is compatible with the PyTorch Dataset API, allowing us to load it directly using [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html).\n",
    "However, since each episode can have a varying length, we need to pad them.\n",
    "To achieve this, we can utilize the [collate_fn](https://pytorch.org/docs/stable/data.html#working-with-collate-fn) feature of PyTorch DataLoader. Let's create the ``collate_fn`` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        # \"seed\": torch.Tensor([x.seed for x in batch]),\n",
    "        # \"total_timesteps\": torch.Tensor([x.total_timesteps for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.observations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.actions) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.rewards) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.terminations) for x in batch],\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.as_tensor(x.truncations) for x in batch],\n",
    "            batch_first=True\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to load the data and create the training loop.\n",
    "To begin, let's initialize the DataLoader, neural network, optimizer, and loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m action_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# assert isinstance(observation_space, spaces.Box)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# assert isinstance(action_space, spaces.Discrete)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m PolicyNetwork(np\u001b[38;5;241m.\u001b[39mprod(observation_space\u001b[38;5;241m.\u001b[39mshape), \u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(policy_net\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     12\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "minari_dataset = minari.load_dataset(\"LunarLanderContinuous-v3/ppo-1000-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "env = minari_dataset.recover_environment()\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "# assert isinstance(observation_space, spaces.Box)\n",
    "# assert isinstance(action_space, spaces.Discrete)\n",
    "\n",
    "policy_net = PolicyNetwork(np.prod(observation_space.shape), action_space.n)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = minari_dataset[0]\n",
    "print(episode)\n",
    "print(episode.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cross-entropy loss like a classic classification task, as the action space is discrete.\n",
    "We then train the policy to predict the actions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        a_pred = policy_net(batch['observations'][:, :-1])\n",
    "        a_hat = F.one_hot(batch[\"actions\"]).type(torch.float32)\n",
    "        loss = loss_fn(a_pred, a_hat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can evaluate if the policy learned from the expert!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs, _ = env.reset(seed=42)\n",
    "done = False\n",
    "accumulated_rew = 0\n",
    "while not done:\n",
    "    action = policy_net(torch.Tensor(obs)).argmax()\n",
    "    obs, rew, ter, tru, _ = env.step(action.numpy())\n",
    "    done = ter or tru\n",
    "    accumulated_rew += rew\n",
    "\n",
    "env.close()\n",
    "print(\"Accumulated rew: \", accumulated_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually observe that the learned policy aces this simple control task, and we get the maximum reward 500, as the episode is truncated after 500 steps.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
