{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc11682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# flow_matching\n",
    "from flow_matching.path.scheduler import CondOTScheduler\n",
    "from flow_matching.path import AffineProbPath\n",
    "from flow_matching.solver import Solver, ODESolver\n",
    "from flow_matching.utils import ModelWrapper\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# To avoide meshgrid warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b92ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10beab5b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    print('Using gpu')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using cpu.')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7640966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return torch.sigmoid(x) * x\n",
    "\n",
    "\n",
    "# TODO: need to resolve temporal locality problem maybe with a CNN later.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, time_dim: int = 1, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            Swish(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, t: Tensor) -> Tensor:\n",
    "        sz = x.size()\n",
    "        x = x.reshape(-1, self.input_dim)\n",
    "        t = t.reshape(-1, self.time_dim).float()\n",
    "\n",
    "        t = t.reshape(-1, 1).expand(x.shape[0], 1)\n",
    "        h = torch.cat([x, t], dim=1)\n",
    "        output = self.main(h)\n",
    "\n",
    "        return output.reshape(*sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3416b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    observations = [torch.as_tensor(x.observations) for x in batch]\n",
    "    actions = [torch.as_tensor(x.actions) for x in batch]\n",
    "    rewards = [torch.as_tensor(x.rewards) for x in batch]\n",
    "    terminations = [torch.as_tensor(x.terminations) for x in batch]\n",
    "    truncations = [torch.as_tensor(x.truncations) for x in batch]\n",
    "    episode_lengths = torch.tensor([len(x.actions) for x in batch], dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"id\": torch.Tensor([x.id for x in batch]),\n",
    "        \"observations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            observations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"actions\": torch.nn.utils.rnn.pad_sequence(\n",
    "            actions,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"rewards\": torch.nn.utils.rnn.pad_sequence(\n",
    "            rewards,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"terminations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            terminations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"truncations\": torch.nn.utils.rnn.pad_sequence(\n",
    "            truncations,\n",
    "            batch_first=True\n",
    "        ),\n",
    "        \"episode_lengths\": episode_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6286b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trajectory_chunks(batch, horizon):\n",
    "    \"\"\"\n",
    "    Processes a padded batch to create fixed-size trajectory chunks.\n",
    "    \"\"\"\n",
    "    batch_size = batch['observations'].shape[0]\n",
    "    all_chunks = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get the data for one episode and its true length\n",
    "        obs = batch['observations'][i]      # Shape: (max_len, 8)\n",
    "        act = batch['actions'][i]          # Shape: (max_len-1, 2)\n",
    "        length = batch['episode_lengths'][i]       # Scalar, e.g., 495\n",
    "\n",
    "        # A single episode can produce multiple chunks\n",
    "        # We slide a window of size 'horizon' over the valid part of the episode\n",
    "        for start_idx in range(length - horizon + 1):\n",
    "            end_idx = start_idx + horizon\n",
    "\n",
    "            # Slice the observation and action sequences to get a chunk\n",
    "            obs_chunk = obs[start_idx:end_idx] # Shape: (horizon, 8)\n",
    "            act_chunk = act[start_idx:end_idx] # Shape: (horizon, 2)\n",
    "            \n",
    "            # Combine them into a single (horizon, 10) tensor\n",
    "            chunk = torch.cat([obs_chunk, act_chunk], dim=-1)\n",
    "\n",
    "            # Flatten the chunk to the final 1000-D vector and add to our list\n",
    "            all_chunks.append(chunk.flatten())\n",
    "\n",
    "    if not all_chunks:\n",
    "        return None\n",
    "\n",
    "    return torch.stack(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9751e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47883, 1000])\n",
      "torch.Size([47494, 1000])\n",
      "torch.Size([47626, 1000])\n",
      "torch.Size([43232, 1000])\n"
     ]
    }
   ],
   "source": [
    "# load minari dataset\n",
    "import minari\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-deterministic-v1\")\n",
    "dataloader = DataLoader(minari_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "for batch in dataloader:\n",
    "    processed_chunks = create_trajectory_chunks(batch, 100)\n",
    "    print(processed_chunks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325af361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| Epoch     10 | 4.66 s/epoch | Loss  1.02946 \n",
      "| Epoch     20 | 4.78 s/epoch | Loss  1.00540 \n",
      "| Epoch     30 | 4.71 s/epoch | Loss  0.98555 \n",
      "| Epoch     40 | 4.72 s/epoch | Loss  0.97085 \n",
      "| Epoch     50 | 4.66 s/epoch | Loss  0.96104 \n",
      "| Epoch     60 | 4.95 s/epoch | Loss  0.95201 \n",
      "| Epoch     70 | 4.81 s/epoch | Loss  0.94667 \n",
      "| Epoch     80 | 4.88 s/epoch | Loss  0.94008 \n",
      "| Epoch     90 | 4.76 s/epoch | Loss  0.93547 \n",
      "| Epoch    100 | 4.73 s/epoch | Loss  0.93251 \n",
      "| Epoch    110 | 4.68 s/epoch | Loss  0.92926 \n",
      "| Epoch    120 | 4.82 s/epoch | Loss  0.92563 \n",
      "| Epoch    130 | 4.85 s/epoch | Loss  0.92231 \n",
      "| Epoch    140 | 4.96 s/epoch | Loss  0.92042 \n",
      "| Epoch    150 | 4.82 s/epoch | Loss  0.91801 \n",
      "| Epoch    160 | 4.91 s/epoch | Loss  0.91716 \n",
      "| Epoch    170 | 4.94 s/epoch | Loss  0.91743 \n",
      "| Epoch    180 | 4.89 s/epoch | Loss  0.91245 \n",
      "| Epoch    190 | 4.90 s/epoch | Loss  0.91066 \n",
      "| Epoch    200 | 4.73 s/epoch | Loss  0.90931 \n",
      "| Epoch    210 | 4.62 s/epoch | Loss  0.90830 \n",
      "| Epoch    220 | 4.57 s/epoch | Loss  0.90749 \n",
      "| Epoch    230 | 4.74 s/epoch | Loss  0.90611 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpow(predicted_velocity \u001b[38;5;241m-\u001b[39m path_sample\u001b[38;5;241m.\u001b[39mdx_t, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 4. Backward pass and Optimize\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/school/FRL-playground/.frl/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment()\n",
    "horizon = 100\n",
    "action_dim = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "input_dim = (obs_dim + action_dim) * horizon\n",
    "\n",
    "# Training params\n",
    "lr = 0.001\n",
    "num_epochs = 1000\n",
    "print_every = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "vf = MLP(input_dim=input_dim, time_dim=1, hidden_dim=hidden_dim).to(device)\n",
    "path = AffineProbPath(scheduler=CondOTScheduler())\n",
    "optim = torch.optim.Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # epoch_loss = 0.0\n",
    "    total_epoch_loss = 0.0\n",
    "    total_chunks = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x_1 = create_trajectory_chunks(batch, horizon)\n",
    "        if x_1 is None:\n",
    "            continue\n",
    "        x_1 = x_1.to(device)\n",
    "        x_0 = torch.randn_like(x_1).to(device)\n",
    "        t = torch.rand(x_1.shape[0]).to(device)\n",
    "\n",
    "        # 3. Forward pass and Loss\n",
    "        path_sample = path.sample(t=t, x_0=x_0, x_1=x_1)\n",
    "        predicted_velocity = vf(path_sample.x_t, path_sample.t)\n",
    "        loss = torch.pow(predicted_velocity - path_sample.dx_t, 2).mean()\n",
    "\n",
    "        # 4. Backward pass and Optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_epoch_loss += loss.item()\n",
    "        total_chunks += 1\n",
    "\n",
    "    avg_epoch_loss = total_epoch_loss / total_chunks if total_chunks > 0 else 0\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"| Epoch {epoch+1:6d} | {elapsed:.2f} s/epoch | Loss {avg_epoch_loss:8.5f} \")\n",
    "        start_time = time.time()\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "668e0d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sampling from trained model...\n",
    "\n",
    "class WrappedModel(ModelWrapper):\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor, **extras):\n",
    "        return self.model(x, t)\n",
    "\n",
    "wrapped_vf = WrappedModel(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b22867d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1000])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step size for ode solver\n",
    "step_size = 0.05\n",
    "\n",
    "batch_size = 1  # batch size\n",
    "T = torch.linspace(0,1,10)  # sample times\n",
    "T = T.to(device=device)\n",
    "\n",
    "x_init = torch.randn((batch_size, input_dim), dtype=torch.float32, device=device)\n",
    "solver = ODESolver(velocity_model=wrapped_vf)  # create an ODESolver class\n",
    "sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)\n",
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2d576ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory():\n",
    "    sol = solver.sample(time_grid=T, x_init=x_init, method='midpoint', step_size=step_size, return_intermediates=True)\n",
    "    final_trajectory = sol[-1]\n",
    "    reshaped_trajectory = final_trajectory.reshape(horizon, obs_dim + action_dim)\n",
    "    print(reshaped_trajectory.shape)\n",
    "    observations = reshaped_trajectory[:, :obs_dim]\n",
    "    actions = reshaped_trajectory[:, obs_dim:obs_dim + action_dim]\n",
    "    print(observations.shape, actions.shape)\n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "66c6c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "torch.Size([100, 8]) torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "observations, actions = generate_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042d802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Action: [-1.0113046   0.09929121], Reward: -1.5947804003200758\n",
      "Action: [-0.30197778 -1.6979274 ], Reward: -1.3307942221617555\n",
      "Action: [ 1.6620966 -0.7585534], Reward: 2.2764014849200453\n",
      "Action: [0.43808994 1.5055774 ], Reward: 0.9112602594500072\n",
      "Action: [-1.7186     0.5056048], Reward: -1.8416311895850004\n",
      "Action: [ 2.6137555  -0.49187252], Reward: 1.8873048095115734\n",
      "Action: [ 1.2885908  -0.13608862], Reward: 1.8397424148273955\n",
      "Action: [-0.8797167  1.875761 ], Reward: -2.332897115682981\n",
      "Action: [-1.955152  -1.1314248], Reward: -1.5375710262658504\n",
      "Action: [-0.7718348 -0.4404358], Reward: -1.6744209622165727\n",
      "Action: [1.1052104  0.37521407], Reward: 1.0224584276644009\n",
      "Action: [0.31785864 1.2918353 ], Reward: 1.946604298780154\n",
      "Action: [ 0.07928108 -1.3897114 ], Reward: 0.6531493171053637\n",
      "Action: [-0.820808 -2.848034], Reward: -1.3700059376768923\n",
      "Action: [-1.6282293   0.76094323], Reward: -1.7916290196812394\n",
      "Action: [-0.6522331 -0.7577608], Reward: -1.2587839579503146\n",
      "Action: [-0.21356429  0.6381553 ], Reward: -1.583406195491126\n",
      "Action: [-0.56283027 -1.5041193 ], Reward: -1.18924214026262\n",
      "Action: [-0.15713327  0.31773886], Reward: -1.1923049714664273\n",
      "Action: [ 1.3147845  -0.66343683], Reward: 3.327816084282902\n",
      "Action: [1.2060959 0.9927293], Reward: 3.8992385575360617\n",
      "Action: [-0.3098464   0.20747583], Reward: -1.3337213842146696\n",
      "Action: [ 0.7172957 -2.2193308], Reward: 3.5779510448045144\n",
      "Action: [ 2.3928738 -1.3826584], Reward: 2.7770093549321886\n",
      "Action: [0.683539   0.14613274], Reward: 1.112931204001694\n",
      "Action: [1.5299888 0.6133435], Reward: 0.9609970402774376\n",
      "Action: [-1.7882675 -1.1656657], Reward: -0.797157805678977\n",
      "Action: [0.5004374  0.07018933], Reward: 0.882137895209081\n",
      "Action: [ 0.34240136 -1.2339512 ], Reward: 3.1362604496206656\n",
      "Action: [0.2025177 0.5591168], Reward: 0.8927310780556673\n",
      "Action: [ 0.784643  -0.4675987], Reward: 2.84125393203434\n",
      "Action: [-0.5024271 -0.5108107], Reward: -0.7886927849660412\n",
      "Action: [-0.13000596  0.6210685 ], Reward: -1.0068423210002493\n",
      "Action: [1.1395856 1.3742161], Reward: 2.6478792266973543\n",
      "Action: [-0.8502655  -0.71036774], Reward: -0.8339879983991545\n",
      "Action: [-0.60906523 -0.9805379 ], Reward: -0.5530230739829495\n",
      "Action: [ 0.3173606  -0.01956456], Reward: 0.8365381617379967\n",
      "Action: [-1.0586684 -1.226222 ], Reward: -0.35143626314194765\n",
      "Action: [-1.0875671  -0.61205405], Reward: -0.9181585839126933\n",
      "Action: [1.801491   0.62218994], Reward: 4.085845998043425\n",
      "Action: [-0.6905711 -0.6653595], Reward: -1.5217519153728687\n",
      "Action: [ 0.905992   -0.41286418], Reward: 3.2810480681443948\n",
      "Action: [ 0.87122923 -0.89833444], Reward: 1.2016318605432967\n",
      "Action: [ 1.2003686 -1.0266676], Reward: 1.8259894921746547\n",
      "Action: [ 1.4443386 -2.0839608], Reward: 0.16548494347604334\n",
      "Action: [-1.3632388  0.3859823], Reward: -2.1576311952971707\n",
      "Action: [-0.30112222  1.1738799 ], Reward: -2.0031482436193913\n",
      "Action: [-0.3772626  0.3958234], Reward: -1.873487596488559\n",
      "Action: [-0.5049284 -1.1165107], Reward: -2.0080342238205717\n",
      "Action: [ 0.32950893 -1.7160876 ], Reward: -0.11573335796338596\n",
      "Action: [1.327055  0.7764437], Reward: 2.0217773671286796\n",
      "Action: [-0.5438845  1.4409561], Reward: -1.7864605856104834\n",
      "Action: [ 0.29664996 -1.0803305 ], Reward: 0.12942550162537259\n",
      "Action: [1.0012418 0.9025682], Reward: 0.5068234527011406\n",
      "Action: [0.24257162 0.15408395], Reward: -0.26767975768967744\n",
      "Action: [ 0.6772189 -2.1672933], Reward: 2.191367465715241\n",
      "Action: [ 1.2737131 -0.787193 ], Reward: 1.0603208945746438\n",
      "Action: [-1.0947137  1.0954013], Reward: -2.064849545802416\n",
      "Action: [ 1.2378786 -1.6014717], Reward: 2.0882013335273215\n",
      "Action: [0.5453658 1.5782142], Reward: 2.6209779613604454\n",
      "Action: [0.6109515 1.0017048], Reward: 0.059783552934625284\n",
      "Action: [-0.12751006 -1.8922287 ], Reward: -2.3165752209675916\n",
      "Action: [ 1.3327351 -0.9775008], Reward: 0.539194103729809\n",
      "Action: [2.038622  3.2344482], Reward: 2.2159560197314208\n",
      "Action: [-0.35163125  1.7828991 ], Reward: -1.5491789299342702\n",
      "Action: [ 0.7074604  -0.94999486], Reward: 1.019297967805578\n",
      "Action: [0.24243908 2.0726643 ], Reward: 1.4188410000332692\n",
      "Action: [-0.22399405 -0.18396139], Reward: -1.7757384886162413\n",
      "Action: [2.1154792 0.3576519], Reward: 4.225029529975598\n",
      "Action: [0.5042889  0.26954097], Reward: -0.2660225531381002\n",
      "Action: [1.2889098  0.66031283], Reward: -0.37497063684613635\n",
      "Action: [0.24259216 0.73550195], Reward: 1.1100409201968489\n",
      "Action: [0.14669557 0.3701434 ], Reward: -0.774529904205292\n",
      "Action: [-0.6888987  1.2955892], Reward: -0.645646672866518\n",
      "Action: [1.1793462 1.2569747], Reward: 1.6304368833650766\n",
      "Action: [ 0.15185179 -0.8294676 ], Reward: -0.5090041465811601\n",
      "Action: [ 2.4495518  -0.39455572], Reward: 3.0508634884637447\n",
      "Action: [-0.17150792  0.09116567], Reward: -1.073120897849833\n",
      "Action: [-0.06531869 -0.376251  ], Reward: -1.0223658046606658\n",
      "Action: [ 1.1911031 -0.701796 ], Reward: 0.8134778648445978\n",
      "Action: [-0.5676303 -1.553767 ], Reward: -1.7734700592683612\n",
      "Action: [-0.7984827  0.3317486], Reward: -1.207203112889431\n",
      "Action: [0.82686526 0.5019305 ], Reward: 2.783593680126638\n",
      "Action: [-0.6920077 -0.511009 ], Reward: -1.48356614564021\n",
      "Action: [-0.38453993 -1.4054028 ], Reward: 7.964887857375886\n",
      "Action: [-0.28269708 -0.9528655 ], Reward: 5.91965846161799\n",
      "Action: [ 1.1443949  -0.04339148], Reward: 5.310267163846885\n",
      "Action: [-0.14959794  0.49736834], Reward: -100\n"
     ]
    }
   ],
   "source": [
    "env = minari_dataset.recover_environment(eval_env = True)\n",
    "num_eval_episodes = 100\n",
    "model_rewards = []\n",
    "random_rewards = []\n",
    "for eps in range(num_eval_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    total_rew = 0\n",
    "    for i in range(horizon):\n",
    "        action = actions[i].cpu().numpy()\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "        total_rew += rew\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    model_rewards.append(total_rew)\n",
    "\n",
    "    env.reset()\n",
    "    total_rew_random = 0\n",
    "    for i in range(horizon):\n",
    "        action = env.action_space.sample()\n",
    "        obs, rew, terminated, truncated, info = env.step(action)\n",
    "        total_rew_random += rew\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    random_rewards.append(total_rew_random)\n",
    "    if (eps + 1) % 10 == 0:\n",
    "        print(f\"Finished episode {eps + 1}/{num_eval_episodes} - Model Reward: {total_rew}, Random Reward: {total_rew_random}\")\n",
    "env.close()\n",
    "print(\"Evaluation finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0394d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Random Agent ---\n",
      "Episode finished early.\n",
      "Total reward from random agent: -86.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "horizon = 100\n",
    "minari_dataset = minari.load_dataset(dataset_id=\"LunarLanderContinuous-v3/ppo-1000-deterministic-v1\")\n",
    "env = minari_dataset.recover_environment(eval_env=True, render_mode=\"human\")\n",
    "\n",
    "# --- Random Agent Evaluation ---\n",
    "print(\"\\n--- Running Random Agent ---\")\n",
    "obs, _ = env.reset()\n",
    "total_rew_random = 0\n",
    "for _ in range(horizon):\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    obs, rew, terminated, truncated, info = env.step(action)\n",
    "    total_rew_random += rew\n",
    "    env.render()\n",
    "        \n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished early.\")\n",
    "        break\n",
    "\n",
    "print(f\"Total reward from random agent: {total_rew_random:.2f}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
